{"pages":[{"title":"","text":"google-site-verification: google766841393607963c.html","link":"/google766841393607963c.html"},{"title":"标签","text":"","link":"/Tags/index.html"},{"title":"关于我们","text":"本博客的主人为服务端开发工程师两枚，他们在此留下了成长的脚步和生活的色彩。 从北到南，再到从南到北。生活本不该一眼望穿。 contact me : gsmtoday艾特qq.com. 才开始工作没多久，个人 GitHub 还只是一些在学校遗留的东西，希望有机会可以多多更新。","link":"/about/index.html"},{"title":"Categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"Redis持久化总结","text":"本篇主要关于Redis持久化的总结。 Why we need persistence持久化的作用是为了在Redis关闭或者意外宕机的情况下，能恢复内存中的数据。Redis在日常的使用场景中，有常见的两种用途： 1.当数据库使用（不推荐），此时持久化的作用显而易见； 2.做缓存使用，虽然Redis挂掉不会影响我们的数据本身，但如果Redis挂了再重启，开启了固化功能后，内存里能够快速恢复热数据，不会瞬时将压力压到数据库上，没有一个cache预热的过程。 Pros and cons for rdb and aofRDB优点 RDB文件很紧凑（compact），它保存了 Redis 在某个时间点上的数据集。 这种文件非常适合用于进行备份。 RDB的恢复时间快，原因有两个，一是RDB文件中每一条数据只有一条记录，不会像AOF日志那样可能有一条数据的多次操作记录。所以每条数据只需要写一次就行了。另一个原因是RDB文件的存储格式和Redis数据在内存中的编码格式是一致的，不需要再进行数据编码工作。在CPU消耗上要远小于AOF日志的加载。 RDB缺点 安全性差，虽然 Redis 允许你设置不同的保存点（save point）来控制保存 RDB 文件的频率， 但是， 因为RDB 文件需要保存整个数据集的状态， 所以它并不是一个轻松的操作。 因此你可能会至少 5 分钟才保存一次 RDB 文件。 在这种情况下， 一旦发生故障停机， 你就可能会丢失好几分钟的数据。 执行开销大，每次保存 RDB 的时候，Redis 都要 fork() 出一个子进程，并由子进程来进行实际的持久化工作。 在数据集比较庞大时， fork()可能会非常耗时，造成服务器在某某毫秒内停止处理客户端； 如果数据集非常巨大，并且 CPU 时间非常紧张的话，那么这种停止时间甚至可能会长达整整一秒。 AOF优点 安全性高，可通过fsync策略来控制，默认每一秒执行一次，此时最多丢失2秒内的数据。 持久化协议可读性强，操作灵活度高。比如：如果你不小心执行了 FLUSHALL 命令， 但只要 AOF 文件未被重写， 那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令， 并重启 Redis ， 就可以将数据集恢复到 FLUSHALL 执行之前的状态。 AOF缺点 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。数据恢复的比较慢。 根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高。对于RDB来说出去fork()本身可能造成的问题外，子进程就会处理接下来的所有保存工作，父进程无须执行任何磁盘操作，性能更好。 Online configuration Config Value 说明 auto-aof-rewrite-percentage 200 AOF文件大小翻倍出发rewrite auto-aof-rewrite-min-size 2147483648 AOF文件触发重写的最小size no-appendfsync-on-rewrite yes 在日志重写时，AOF追加只写缓存 appendonly yes 开启AOF appendfsync everysec 每秒刷盘 rdbcompression yes 开启RDB压缩 rdbchecksum yes 开启RDB校验和 save null RDB自动触发未配置 特殊说明：在同时执行bgrewriteaof操作和主进程写aof文件的操作，两者都会操作磁盘，而bgrewriteaof往往会涉及大量磁盘操作，这样就会造成主进程在写aof文件的时候出现阻塞的情形。no-appendfsync-on-rewrite被设置为yes后，在日志重写时，主进程不进行命令追加的刷盘操作，而只是将其放在缓冲区里，避免与重写造成DISK IO上的冲突。如果rewrite的过程中，Redis down掉的话 丢失的数据 就不是之前appendfsync 定下的策略，而是整个 rewrite 过程中的所有数据。","link":"/2018/07/30/Redis持久化总结/"},{"title":"BeanUtils中HashMap触发死循环","text":"一次HashMap多线程不安全的踩坑… 故障现场2019-09-03 下午16点48收到CPU Idle小于20%的报警。 这个时候立刻查看了接口成功率，调用量，延时等指标，发现均为正常： 再看硬件指标：发现只有这台（docker4）从16点37开始CPU idle不断下降，一个小时之内跌到0. 另外查看docker物理机监控，发现物理机CPU 情况正常，排除物理机问题。 与此同时（报警时刻，CPU idle 20%），查看了JVM jstack : 发现：123456789101112131415\"http-nio-8099-exec-72\" #117 daemon prio=5 os_prio=0 tid=0x00007f6398041800 nid=0xdf7 runnable [0x00007f6358cc8000] java.lang.Thread.State: RUNNABLE at java.util.WeakHashMap.get(WeakHashMap.java:403) at ma.glasnost.orika.metadata.TypeKey.getTypeIndex(TypeKey.java:55) at ma.glasnost.orika.metadata.TypeKey.valueOf(TypeKey.java:47) at ma.glasnost.orika.metadata.TypeFactory.intern(TypeFactory.java:421) at ma.glasnost.orika.metadata.TypeFactory.valueOf(TypeFactory.java:69) at ma.glasnost.orika.impl.MapperFacadeImpl.typeOf(MapperFacadeImpl.java:1045) at ma.glasnost.orika.impl.MapperFacadeImpl.resolveMappingStrategy(MapperFacadeImpl.java:154) at ma.glasnost.orika.impl.MapperFacadeImpl.map(MapperFacadeImpl.java:675) at ma.glasnost.orika.impl.MapperFacadeImpl.map(MapperFacadeImpl.java:655) at com.xiaojukeji.sec.upm.common.utils.BeanUtils.mapList(BeanUtils.java:108) at com.xiaojukeji.sec.upm.core.api.controller.coreapi.UserController.userAreaList(UserController.java:155) at com.xiaojukeji.sec.upm.core.api.controller.coreapi.UserController$$FastClassBySpringCGLIB$$bc29b0e0.invoke(&lt;generated&gt;) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) 多次top, 占用CPU的线程ID没有变过；jstack多次发现代码一直在WeakHashMap的get的函数上. 这个时候查看报警时间（16点48）的业务日志，发现无论是访问日志还是error日志，都是正常的。在事后补看最开始CPU Idle发生变化的时间点的业务日志，发现15.37 服务程序发生了由于弹性云均衡热点导致的重启（弹性云记录）。 故障处理当时通知运维摘除了这个docker, 之后漂移重建了这个docker. 事后反思： 还是应该第一时间重启服务. （CPU 飚高 → 重启服务）不能只查看报警时间点业务日志，要根据监控有转折点的时间看业务日志，实际可能看更早的日志。 故障分析故障后排查，发现以下几个条件： 1.CPU idle 下降2.占用CPU的线程id没有变过3.jstack多次发现代码一直在get的函数上4.发现数据结构使用的是weakHashMap可以推断是发生了Java HashMap死循环的问题。 查看占用CPU高的线程堆栈现场，发现是执行在这个业务代码上.1234567891011121314151617181920212223/** * 获取用户的地区列表 * * @param appId * @param userAreaParamDto * @return */@PostMapping(\"/area/list\")public ResponseEntity&lt;AppUserAreaResponseDto&gt; userAreaList(@SessionAttribute(ApiConstants.REQUEST_APPID) Long appId, @RequestBody UserAreaParamDto userAreaParamDto) { if (StringUtils.isEmpty(userAreaParamDto.getUserName())) { throw new ServiceException(ApiExceptionEnums.Area.USERNAME_PARAMS_NOT_EXISTS.getCode(), ApiExceptionEnums.Area.USERNAME_PARAMS_NOT_EXISTS.getMsg()); } Long userId = userManager.getUserIdByName(userAreaParamDto.getUserName()); List&lt;AppUserAreaDto&gt; areas = userManager.getUserAreas(userId, appId); List&lt;AppUserAreaResponseDto&gt; responseDtos = BeanUtils.mapList(AppUserAreaResponseDto.class, areas); // HERE!!! return ResponseEntity.success(responseDtos);} BeanUtils.mapList 是我们的一个继承自org.apache.commons.beanutils.BeanUtils的对象转换的类。跟踪源码发现使用到了外部包orika-core-1.5.0.jar的类ma.glasnost.orika.metadata.TypeKey . type是方法输入的类.class, 因为是个通用方法，类似调用还有12345BeanUtils.mapList(FeatureResponseDto.class, featureDtos)BeanUtils.mapList(FlagResponseDto.class, flagDtos)BeanUtils.mapList(RoleResponseDto.class, roleDtos)BeanUtils.mapList(AreaResponseDto.class, appUserAreaDto)... 而knowTypes 使用了一个多个线程共用的static WeakHashMap (非线程安全类)。 虽然getTypeIndex 方法使用了synchronized 对type加了锁，但是这个锁的粒度只能锁住同一个type, 对于其他type还是可以进入临界区执行knowTypes的get和put，这样就出现HashMap线程不安全的三要素： 并发情况存在线程切换 多个key,两个key hash到同一个槽 触发扩容 从而存在一定几率出现HashMap因为多线程情况出现的死循环，CPU Idle下降的情况。 参考耗子哥的HashMap死循环分析","link":"/2019/09/03/BeanUtils中HashMap触发死循环/"},{"title":"Angkor Wat - 柬埔寨浮光掠影","text":"吴哥窟边的少年。 时间 ： 2016年7月 地点 ： 柬埔寨 - 暹粒 人物 ： with @mikloo","link":"/2016/07/03/angkorWat/"},{"title":"权限控制发展综述","text":"本文试图用不那么学术性的白话解释各种权限管理模型和适合使用场景。 名词解释 主体：进行资源访问的实体。例如用户，服务，进程等。为了易于理解起见，本文将主体简化为用户。 客体：资源。例如系统，数据等，本文将客体简化为资源。 访问控制：访问控制限制主体对客体的访问权限，从而使系统在合法范围内使用；其实就是解决谁可以对什么资源进行什么样的操作的问题。 例如，某个页面只能由公司HR访问，这里主体是公司的HR，客体就是页面, 通过给公司HR关联权限达到限制页面访问的目的。 ACLACL（Access Contrl List）是上个世纪90年代提出的权限控制模型。ACL是一个规则列表, 将客体和主体进行了映射, 规定了客体资源可以被哪些主体进行操作。例如公司的docker白名单（给docker添加了某个IP白名单，这台docker就可以访问这个IP）。另外公司弹性云集群流量隔离都是通过ACL去实现的访问控制。再比如某些业务系统使用ACL定义了用户和业务线的访问控制关系。ACL的优点在于它非常简单，易于实现。但是它的缺点也非常明显： 用户和资源直接关联，在应对大量用户和客体资源的权限管控的时候，访问控制列表就会变得非常繁琐。解决思路也非常简单，用户和资源之间再抽象一层，给资源分一个资源类，用户直接和这个类别关联，这样就清晰很多 – 这个资源类就是角色。 RBACRBAC （Role Based Access Control） 模型也是上个世纪90年代提出的ACL增强版的权限控制模型。RBAC将人和资源解耦，在用户和资源之间增加了一个中间桥梁 – 角色。 角色可以看做是一组操作的集合，不同的角色具有不同的操作集，这些操作集关联到用户身上。一个用户可以拥有多个角色，从而实现权限管理的灵活性。 例如「XX系统」根据用户的岗位分为了“运营角色”，“客服角色”，“电销管理员角色”，“财务角色”等多个角色。再例如「XX系统」根据自身业务内容划分了角色：“问题诊断角色”, “查询角色”等。 ACL✖️RBAC公司拥有多条业务线，例如专车，快车, 安全等等。这样在管理权限时候就会遇到一个问题 – 如何管理不同业务线的同一类操作问题。而这类问题使用RBAC和RBAC✖️ACL都可以解决: RBAC模型：「A系统」针对不同业务线的同一个操作设置了多个角色，例如角色“专车-看板角色”, “快车-看板角色”,”安全-看板角色”等等。 RBAC✖️ACL的模型：「B系统」使用ACL标识定义了业务线资源（设置了标识位：专车，快车, 安全），再设置了一个“看板角色”，用户在申请角色的时候选择自己业务线的标识位，再选择看板角色即可。 可以看出虽然是同一个RBAC模型，不同使用者（业务系统）使用的效果各异。这就好比在权限系统提供基础食材（权限模型），各个厨师（业务系统）对各个基础食材各有发挥，做出来的菜也不一样。同样是土豆，有的厨师做出了土豆丝，有的做出了薯条。每一种使用方式都可以解决业务系统的业务问题，权限系统重点在于提供一个可以让厨师自由选择食材的平台。 RBAC模型的优点显而易见：大大简化了权限管理，降低了管理开销，使得权限管理系统具有更强大的可操作性和可管理性。事实证明，RBAC也是公司目前被业务系统使用最多的权限管理模型。 纵观ACL和RBAC两个模型，这两个模型虽然强大，但是仍然很难覆盖所有权限管理场景。例如管控“预算部门是汽车资产管理中心且操作城市是北京”的用户的某一角色权限，普通的RBAC或者多个ACL✖️RBAC模型管理的复杂度会增加很多。总结来说，复杂的权限管控场景需要可以动态根据一个或者一组属性是否满足某种条件进行权限判断，而这就是ABAC产生的背景。 ABACAttribute Based Access Control (ABAC)是2013提出的模型。不同于RBAC和ACL将用户通过某种方式与资源关联的方式，ABAC是通过动态计算一个或者一组属性是否满足某种条件进行授权判断。ABAC可以实现非常灵活的权限控制，几乎能满足所有类型的需求。 ABAC的属性有四类： 用户属性（用户职位） 操作属性（增删改查） 环境属性 （当前时间） 资源属性 目前我们的ABAC模型是给资源添加各种策略（属性），模型如下图： ABAC模型虽然强大，但是通常这个模型都是最后一个选择。实际使用中往往我们推荐给权限模型使用者都是RBAC&gt;ACL&gt;ABAC. 正是因为ABAC模型很强大，他试图解决所有权限管理场景，就会带来管理策略非常复杂的问题，使用起来也不是很用户友好，实际场景难以运营和落地。 Summary目前我们的系统作为统一权限管理平台，支持ACL,RBAC以及ABAC三种权限模型的管理。","link":"/2019/12/04/access-control-policies/"},{"title":"一种使用自定义注解+切面统一收集审计日志的方式","text":"最近在做一个审计模块，想要实现的是为微服务各个模块提供一个审计日志服务，即各个微服务模块收集日志 + 日志存储在db/elk/hive，然后针对存储的审计日志做展示或者分析的一个服务。可以看出实现一个审计服务的三个关键地方是： 收集日志 存储日志 展示/分析日志 第一个关键地方是收集日志, 本文也想探讨下如何更低耦合的收集日志。 什么是审计日志审计日志记录了系统用户操作了什么，以便对用户行为进行追踪和审计。最典型的审计日志： “张三新增了一个用户李四”； “张三给李四新增了一个管理员权限” “张三新增了一个用户李四”这条日志主语是当前登录的用户”张三”，谓语是动作“新增”，宾语是用户“李四”，还需要记录使用的系统功能“用户管理。” 所以最基本日志需要包含字段： 操作人operator； 操作动作action. 审计模块一般针对“新增”，“修改”和“删除”和“登录”类型的操作做记录； 操作的功能function,例如角色管理，应用管理，用户管理； 操作的主体subject，例如新创建一个用户是李四, 李四就是操作的主体; 日志的创建时间createTime 记录审计日志的方法假如我们系统有三个服务，用户服务，权限服务，角色服务，需要在用户/权限/角色相关操作上记录审计日志。最直观的做法是在每个服务中嵌入审计日志rpc服务。例如： 用户服务 - 新增用户代码： 12345678910public void addUser(UserDTO userDTO) { userService.addUser(userDTO); AuditLog auditLog= new AuditLog(); auditLog.setOperator(getCurrentLoginUser()); auditLog.setAction(\"新增\")； auditLog.setFunction(\"用户管理\"); auditLog.setSubject(userDTO.getUserName()); auditLogSerice.writeLog(auditLog);} 但是这种做法有一个很大的缺陷就是业务代码和审计日志服务高耦合。业务coder需要花费很大的时间去封装日志需要的参数，但是实际上他是不需要关注这些日志相关的事情的。另外业务代码也会被割裂，很难写出clean code 一段代码只做一件事的代码。 我想到的一种优化方式是使用自定义注解+AOP切面生成统一日志。 首先定义一个注解，该注解的目的是只要被该注解@Auditable注解过的方法，都会被切面接收到打印审计日志。12345678910111213141516171819202122232425@Retention(RetentionPolicy.RUNTIME)@Target({ElementType.METHOD, ElementType.TYPE})public @interface Auditable { Action action();// 行为 Function function(); //功能}public enum Action{ ADD(\"增加\"), DELETE(\"删除\"), MODIFY(\"修改\"); private String description; private Action(String description) { this.description = description; } public String getDescription() { return this.description; }}public enum Function{ ...} 再定义一个注解，该注解帮助切面捕获被@Audit注解的方法参数中的操作主体值（例如刚才的张三）12345@Retention(RetentionPolicy.RUNTIME)@Target({ElementType.FIELD, ElementType.TYPE, ElementType.LOCAL_VARIABLE})public @interface AuditingTargetUsername { String value() default \"\";} 用户DTO可以如下定义：123456@Datapublic class UserDTO implements Serializable{ // already use lombok @NotNull @AuditingTargetUsername private String name;} 切面1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Component@Aspectpublic class AuditAspect { @Resource private IAuditLogService auditLogService; @After(value = \"@annotation(auditable)\") @Transactional public void logAuditActivity(JoinPoint jp, Auditable auditable) { String action = auditable.actionType().getDescription(); String function = auditable.function().getFunction(); String valueFilledIntoSubject = extractTargetAudintUserFromAnnotation(jp.getArgs()[0]); AuditLog auditLog = new AuditLog(); auditLog.setOperationFunctionType(function); auditLog.setFunctionType(action); auditLog.setCreatedAt(new Date()); auditLog.setUpdatedAt(new Date()); auditLog.setOpName(getCurrentLoginUser());//获取当前登录用户 auditLog.setContent(getCurrentLoginUser() + actionType + subject + valueFilledIntoSubject);//张三新增了用户 auditLogService.insert(auditLog); } private String extractTargetAudintUserFromAnnotation(Object obj){ // ... return getSubjectValueViaAnnotation(obj); } private String getSubjectValueViaAnnotation(Object obj) { String result = null; try { for (Field f : obj.getClass().getDeclaredFields()) { for (Annotation a : f.getAnnotations()) { if (a.annotationType() == AuditingTargetUsername.class) { f.setAccessible(true); Field annotatedFieldName = obj.getClass().getDeclaredField(f.getName()); annotatedFieldName.setAccessible(true); String annotatedFieldVal = (String) annotatedFieldName.get(obj); result = annotatedFieldVal; } } } } catch (Exception e) { } return result; }} 总结通过上述自定义注解+切面可以实现将具体业务和记录审计日志解耦，提高各自开发人员的效率，代码也更加好维护一些。但是这种方式无法实现某些个性化的日志。我将日志分为两种： 通用日志 日志需要确定的信息都是固定的，例如异常/错误日志，或者一些简单的审计日志场景，例如上例中审计日志只需要“动作”，操作的”功能“，操作的”主体“值，或者登陆用户的ip, 等等固定信息都是可以算为通用日志，利用切面去优化日志实现方式。 个性化日志 在上述场景中，如果想进一步在服务中查询出某些数据反映在审计日志中，这些数据可以理解为动态日志数据，切面是无法拿到的（因为切面是基于反射，只能拿到方法的输入输出参数）。举例子来说就是删除用户场景。前端传入删除用户id = 5：1234567@Auditable(actionType = ActionType.DELETE, function = Function.User)@RequestMapping(\"/delete\")@ResponseBodypublic void delete(int userId){ String userNameDeleted = userService.getUserById(userId);// 需要记录日志 “DELETE USER 张三“ 但是aop无法拿到， userInfoService.delete(userId);} 这时候可以利ThreadLocal记录动态日志数据，有一点不太方便的是要使用双方约定好threadlocal中的字段。","link":"/2018/01/19/auditlog-md/"},{"title":"Google的BeyondCorp安全模型总结","text":"未来要接触一个企业安全项目，先了解了下Google的BeyondCorp安全模型。 传统的企业安全是通过使用防火墙来实现的，然而这个方式暴露出来了很多问题。例如一旦企业的内网边界被突破，攻击者可以在内网为所欲为。而且随着企业开始使用移动和云技术办公，企业的边界安全越来越难以保证。谷歌采用了一个不同的安全模型实现了企业的网络安全—BeyondCorp。BeyondCorp取消了员工访问企业内网的要求，将公司的应用迁移到了互联网。 BeyondCorp是一个零信任模型，它认为公司网络安全的一个重要的大前提是无论内网和外网都是不安全的。与传统的边界安全模型不同，BeyondCorp不是以用户的物理登录地点或者用户使用的网络作为访问服务的判定标准(无论员工是在家or在公司or咖啡店)，其访问策略是建立在设备信息(device)、状态(state)和关联用户(associated user)的基础上，更偏向用户行为和设备状态的分析。企业资源的所有访问都是完全验证，充分授权，并根据设备状态和用户凭证完全加密。BeyondCorp针对公司不同资源实现了不同粒度的访问控制。这样做的结果就实现了所有谷歌员工可以在任何网络下工作，根本不需要VPN. 架构 Architecture Trust Tiers 信任级别. 信任级别据访控敏感度分为不同的层级。员工的设备和资源都被赋予了不同的信任级别，只有高于资源最小信任级别的员工设备才能访问资源，这样确保员工能够以最小权限履行职责，这样既减少了运维开支，又提高了设备可用性。 Resources 资源是一系列接入访问控制的应用，服务或者基础设施。资源包括任何来自在线知识库，财政数据库，或者链路层链接，实验室网络的数据。每个资源与一个最小信任层关联。 Trust Inferer 信任推断系统。信任推断系统持续分析和标注设备的状态。信任推断系统设置被设备访问的最大信任层级和设置设备使用的局域网。这些数据记录在设备清单服务中。 Device Inventory Service 设备清单服务组件是系统的核心。在设备的生命周期中，该组件持续收集，处理，发布设备的更新。清单是基于清单访问控制的必备基础。员工的设备（例如手机，笔记本电脑）以及他们设备的动态环境（设备的操作系统）都要登记到设备清单服务上。设备清单服务就像一个持续更新的管道一样，管道的另一端是来自数据源的大量数据。数据源包括：活动目录、Puppet软件自动化配置和部署工具和Simian冗余代码检查工具；各种配置和企业资产管理系统；漏洞扫描器，证书管理以及ARP地址解析表等网络基设施元素。beyondcorp从15个数据源获取了上亿级数据，每天处理三百万数据，数据总量达到80T。可以利用员工历史数据做一些数据分析达到安全审计和调查目的。 设备清单服务还依赖设备清单数据库，BeyondCorp设备清单数据库集群通过一个元清单数据库规范多个来源的设备信息，并向下游组件提供这些信息。被企业管理的设备需要在设备清单数据库上有唯一的id。给设备赋唯一id的方法可以给每个设备安装一个设备证书。这样也表明此设备为“managed device”,即被公司管理的设备。 Access Control Engine 位于访问代理服务器内部的访控引擎是一个策略实施中心。访问控制引擎对每次访问企业应用的请求进行服务级的认证授权。授权取决于访问的用户，用户属于哪个组，用户使用的设备证书，设备清单数据库的纪录。例如，谷歌Bug跟踪系统只限制于全职工程师的工程设备访问。财务应用程序只限制于在会计组的全职和兼职职员的管理非工程设备。访问控制引擎还可以通过不同的方式来限制访问应用程序的部分功能。例如，在我们的Bug跟踪系统，更新或搜索比查看一个Bug记录有着更严格的访问控制。 Access Policy 访问政策是资源，信任层等等的程序表示。 Gateways 网关 数据分析前文说到，BeyondCorp访问策略是建立在对访问者的设备信息(device)、状态(state)和关联用户(associated user)的用户行为和设备状态的分析基础上。这就涉及到了对数据的收集和分析。数据来源为15个上文提到的数据源，BeyondCorp把数据分为两类：observed data 和 prescribed data. observed data数据为通过所编写的程序生成观测数据。 它包括： 上一次在设备上执行的安全分析及安全分析的结果 活动目录上一次的同步政策和时间戳 操作系统版本和补丁等级 任何安装的软件 Prescribed data数据为手动维护的数据，它包括： 设备的所属人 访问设备的用户和群体 使用的DNS和DHCP 直接访问的DNS Data Processing数据处理步骤为： 将输入数据转换为一个统一的数据类型 统计数据相关性. 来自不同数据源的数据需要被统一成为这个设备的唯一的数据记录。 当输入的数据被融合为一个统一的数据记录后，Trusted Inferer被通知去触发重评估。 网络建设 - 内网和外网几乎没有区别 不再相信内网-公司内部网络建设:. BeyondCorp将公司内部访问和远程访问等同，并在Google大楼内部建立了一个无特权网络（unprivileged network）. 所有谷歌大楼内办公的设备都连接这个无特权网络。对于有线和无线接入，谷歌使用RADIUS（远端用户拨入验证服务）服务器将设备分配到合适的网络。 所有谷歌企业应用都对外解析：所有谷歌的企业应用都通过面向互联网的访问代理服务器暴露给了外部和内部用户，并强制对通信进行加密访问。访问代理服务器被配置为每个应用程序提供通用的功能，如全球可达性，负载均衡，访问控制检查，应用健康检查，和拒绝服务保护。代理服务器在访问控制检查完成之后再将请求发送到后端应用。 从任意网络访问企业应用当一个工程师使用笔记本电脑访问公司代码评审url: codereview.corp.goole.com, 在beyondcorp会发生： 访问请求被重定向到访问代理服务器。笔记本提供自己的设备证书。 访问代理服务器识别不出此用户，将请求重定向到SSO系统。 工程师提供他或她的主要凭证和次要凭证给SSO系统进行验证，通过验证后，分配一个token，并被重定向到访问代理服务器 代理服务器为codereview.corp.goole.com执行特定的授权检查。授权检查是基于每个请求的： 该用户是否隶属于工程师组 用户是否拥有一个足够的信任级别 设备为公司的管理设备 设备拥有一个足够的信任级别 当以上1-4条件满足后，请求才会传递到coderreview后端。 参考 http://www.tomsitpro.com/articles/google-beyondcorp-future-network-security,1-3229.html Ward R, Beyer B. Beyondcorp: A new approach to enterprise security[J]. login, 2014, 39: 5-11. Osborn B, McWilliams J, Beyer B, et al. BeyondCorp: Design to Deployment at Google[J]. 2016. https://cloud.google.com/beyondcorp/ https://www.scaleft.com/product/web-access/#works https://duo.com/blog/beyondcorp-for-the-rest-of-us","link":"/2017/12/18/beyondcorp-md/"},{"title":"程序员修炼知道--从小工到专家","text":"这是一本关于方法论的书，这是一本关于如何成为注重实效的程序员(方便起见，我称他为好的程序员)的书；这是一本很大的书，也是一本很小的书，这本书就是《程序员修炼之道 从小工到专家》，而这就是我作为刚入行的程序员读完此书后，对它的看法。 修炼是浪漫的程序员们借用武侠玄幻来表达自我能力提升的一种形象化表达方式。看过武侠的朋友都知道，在纷争动乱的江湖背景下，无论是华山论剑，睥睨天下的自我实现，还是路见不平，拔刀相助的侠肝义胆，都需要以强大的实力为支撑，不然剑还没拔，就已经成为路边亡魂，带着虚妄的江湖梦找马克思报道去了。在程序的江湖里(仿佛比武侠的世界更动荡，因为技术的更迭太快了)也是一样，没有足够的实力，也只能哀叹：程序是一袭华美的袍，爬满了bug，让你把头不停的挠。 关于程序员的修炼，借用江湖术语，还有更近一步的描述：内外兼修。内指的是内功，从编程的角度看，我愿意把他理解为逻辑思维方式以及抽象能力；外指的是招式，在程序的世界里，就是涉及到语言，技术和架构等关于代码的一切。仔细想想，这种类比是有一定的相似性可言：内功强大的人，招式上大多手到擒来，技术一般也具有触类旁通的特性，底子好的程序员接触上手一项新知识，也不会高呼：噫吁嚱，危乎高哉！蜀道难，难于上青天；但终究还是有些区别，我们没有一本内功秘籍可以练，程序员的内力还是需要长期耳濡目染，慢慢的悟，投入的有效时间够了，自然水到渠成。所以，肉眼可见的修炼大都集中在外力上。 虽然都是外力，但也应该有层级之分。由于工作需要以及个人兴趣，对Java服务端开发的技能与技术有持续关注。以下引用网上的一张关于Java服务端开发的基础技能图谱： 在博文作者罗列的知识体系下，对Java核心概念(对象模型和接口设计)知识的掌握，需要有底层JVM，操作系统等知识的辅助，而上层的网络应用框架也少不了对Java语言和TCP/IP协议等的辅助。其实这不难理解，我们知识积累的方式就像是盖房子，地基挖的越深，房子架构越合理，房子就能越盖越高，越牢固。 回到武侠世界里来，本书所提及的修炼仍然是外力上的持续进步，但并不是从上述知识图谱的角度来向读者介绍，成为一个好的程序员应该学习哪些具体招式，我觉得它更像是一个拳谱或者剑谱的总纲，是作者在软件开发领域多年奋斗经验，炼化成的知识结晶，可谓字字珠玑。它不讲解知识点，只给出如何向一个好的程序员迈进最中肯的建议，我把它们理解为好程序员的基本素养。 无论工作领域与研究方向的差异，程序员最直观的生产力就是代码。好程序员的素养在编码之前，编码过程中，编码之后具体体现如下（纯属个人总结）： 充分掌握需求：用户是产品的审判员，挖掘他们最深沉的需求，确保质量成为需求的一部分，不过分偏执，知道该何时止步。 手有利器：一个熟练高效操作的编辑器，进行文本和数据处理的shell命令，版本控制系统 不留破窗，不惧变化：不让破窗理论坏了自己的代码；懂得解耦，保持功能模块的正交性；不留重复；充分抽象，利用元数据的优势 无情测试：早测试，多测试，自动测试，保证你手里出去的代码是可靠的 知识资产储备：保持学习，扩展视野 承担责任：勇于承认错误；这是我的代码，我的品牌 前面说过，这是一本总纲，但凡在招式上每有所成，回头来看总会有新的体会。我想这也是为什们不少业界大师与偶像们，对此书盛赞频频或奉为圭臬的原因吧。作为入门学徒，虽然有疑惑不解之处，不能完全掌握其紧要，初次读来，结合自身经验，但亦有所初悟。 最后，以江湖规矩与自己立个盟约：青山不改，绿水长流，两年之后，重读此书，愿领悟更深，不见不散。","link":"/2016/11/06/book-programtic-programer/"},{"title":"关于MySQL的count","text":"感谢丁奇，让我们对MySQL的count了解的更清楚了。 count(*), count(1),count(主键id) 以及count(字段)哪个快？原则： server 层要什么就给什么； InnoDB只给必要的值； 优化器只优化了count（*）的语义为”取行数“。 count(*), count(1),count(主键id) 都表示返回满足条件的结果集的总行数；而count(字段)则表示返回满足条件的数据行里面，参数“字段”不为NULL的总个数。 count(主键id)：InnoDB引擎会遍历整张表，把每一行的id值都取出来，返回给server层。server层拿到id后，判断是不可能为空，就按行累加。 count(1): InnoDB引擎会遍历整张表，但不取值。server层对于返回的每一行，放一个数字1进去，判断不可能为空，按行累加。可以看出count(1)执行要比count(主键id)快。因为从引擎返回id会涉及到解析数据行，以及拷贝字段操作。 count(字段): 如果这个“字段”是定义为not null, 一行行地从记录里面读出这个字段，判断不能为null, 按行累加。 如果这个“字段”定义为云讯null, 那么执行的时候，判断到有可能是null, 还要把取值取出来再判断下，不是null才累加。 count(*):并不会把全部字段取出来返回给server层，而是做了专门优化，不取值。count(*)肯定不是null, 所以操作只是：遍历全表，按行累加。 结论是：按照效率排序的话，1count(字段) &lt; count(主键id) &lt; count(1) &lt; ~= count(*) 建议尽量使用count(*)。 count(*) 实现方式不同的引擎中，count(*)有不同的实现方式。 MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候回直接返回这个数，效率很高。 而InnoDB引擎因为要支持事务的多版本并发控制MVCC，执行count(*)的时候，需要把数据一行行读出来，然后累积计数。 优化取数业务如果业务使用了InnoDB引擎，业务上还需要频繁显式记录总数，例如一个页面要经常显示交易系统的操作记录总数。应该如何优化？ – 只能自己计数，找地方把操作记录表的行数存起来。 使用缓存使用缓存会出现逻辑上不一致的问题。例如：会话A是一个插入交易记录的逻辑，往数据表里插入一行R，然后Redis计数加1；会话B就是查询页面显示时需要的数据。在上图的时序里，T3时刻会话B查询的时候，会显示出新插入的R这个记录，但是Redis的技术还没加1.这时候会出现数据不一致。 – 两个不同的存储构成的系统，不支持分布式事务，无法拿到精确一致的视图。 使用DB保存计数把计数直接放到数据库里面单独的一张计数表，利用事务是可以实现一致的优化取数业务的。虽然会话B的读操作仍然是在T3执行的，但是因为这时候新事物还没有提及，所以计数值加1这个操作对会话B还不可见。 因此会话B看到的结果里，查计数和最近100条记录看到的寄过，逻辑上就是一致的。 参考MySQL实战45讲","link":"/2019/02/09/count/"},{"title":"一次线程池设置过大踩坑记录","text":"踩坑中成长。 背景我们的业务（权限业务）上有一个需求：给权限点批量绑定角色集合，绑定成功后需要更新每个角色对应的用户-权限点缓存。抽象下问题即为：A实体（权限点）与B实体（角色）集合有关联关系，B实体与C实体（用户）集合有关联关系。而更新用户权限点缓存量比较大，理论上为$|权限点| * |角色| * |用户|$次更新操作。其中一个权限点对应的角色数可能达到100+， 角色关联的用户数可能达到1000+。 为了更快返回，我开了一个线程池操作更新用户权限点缓存1ExecutorService threadPool = Executors.newFixedThreadPool(100); 但是上线后，很快出现了CPU Idle掉底的情况。 反思因为我们服务环境是4core 8G docker机器。线程池里面有100个线程，而更新用户-权限点的操作计算量大，为典型的计算密集型任务。CPU的每个core都被占满，而100个线程会导致大量的线程在较少的CPU和内存资源上发生竞争，频繁的线程上下文切换也会带来额外的性能开销，基于上述原因导致了CPU Idle掉底。 结论线程CPU时间所占比例越高，需要越少线程。线程等待时间所占比例越高，需要越多线程。 线程池设置大小应该取决于给线程池处理什么样的任务以及机器的CPU core数量。任务类型不同，线程池大小的设置方式也是不同的。任务一般可分为：计算密集型，IO密集型，混合型。 计算密集型任务尽量使用较小的线程池，一般为CPU核数+1（参考Java并发实战）。因为CPU密集型任务使得CPU使用率很高，若开过多的线程数，只能增加上下文切换的次数，因此会带来额外的开销。注意，这里推荐值是指机器集中做该任务并且只有一个线程池的情况下的推荐值。而一般WEB服务会同时处理很多个请求，因此任务的线程池需要再相应调低一些。 IO密集型任务可以使用稍大的线程池，一般为2*CPU核心数。IO密集型任务CPU使用率并不高，因此可以让CPU在等待IO的时候去处理别的任务，充分利用CPU时间。-混合型任务可以将任务分成IO密集型和CPU密集型任务，然后分别用不同的线程池去处理。 只要分完之后两个任务的执行时间相差不大，那么就会比串行执行来的高效。 因为如果划分之后两个任务执行时间相差甚远，那么先执行完的任务就要等后执行完的任务，最终的时间仍然取决于后执行完的任务，而且还要加上任务拆分与合并的开销，得不偿失。 引申一个系统最快的部分是CPU，所以决定一个系统吞吐量上限的是CPU。增强CPU处理能力，可以提高系统吞吐量上限。但是根据短板效应，真实系统吞吐量应该取决于系统短板，例如网络延迟、IO。因此为了提高系统真实吞吐量，应该： 尽量提高短板操作的并行化比率，例如多线程下载技术； 增强短板能力，比如NIO替代IO。 为什么Redis 单线程就性能高呢？ Redis是典型的单线程模型，它非常高效，基本操作能达到10w/s。从单线程角度看，部分原因在于： 多线程会存在线程上下文切换开销，单线程没有此种开销； 锁 更本质的原因为：Redis基本都是内存操作（很少IO操作），这种情况下单线程可以高效地利用CPU。而多线程使用场景一般是：存在相当比例的IO和网络操作。 参考 Java并发编程实战 8.2节 http://ifeve.com/how-to-calculate-threadpool-size/","link":"/2019/02/14/cpu-problem/"},{"title":"分布式锁实现浅谈(Redis实现方式)","text":"和我厂老师傅练功夫系列。另，本文范围只是基于Redis实现。 分布式锁想解决什么问题Martin Kleppmann 认为一般我们使用分布式锁有两个场景： 效率： 使用分布式锁可以避免不同节点重复相同的工作，这些工作会浪费资源，比如用户付了钱之后可能不同节点会发出多封短信；再比如定时任务多个节点执行多次，实际上只需要任务执行一次。 正确性：加分布式锁同样可以避免破坏正确性的发生，如果两个节点在同一条数据上操作，比如多个节点机器对同一个订单操作不同的流程有可能会导致该笔订单最后状态出现错误，造成损失。 分布式锁特点 1.互斥性：和本地锁一样，互斥性是最基本的特性。但是分布式锁需要保证在不同节点的不同线程的互斥。 2.可重入性： 同一个节点的同一个线程获取了锁之后，那么也可以再次获得这个锁（用的少）。 3.不会发生死锁：即使有一个客户端在持有锁期间崩溃而没有主动解锁，也能保证后续客户端能加锁。 4.解铃还须系铃人：加锁和解锁必须是同一个客户端，客户端自己不能把别人家的锁给解了。 单机Redis分布式锁实现分布式锁本质上要实现的目标就是Redis里面占一个“坑”，当别的进程也要来占坑时，发现那里已经有一根“大萝卜”了，就只好放弃或者稍后再试。 占坑一般使用setnx,只允许一个客户端占坑。先来先占，用完了，再调用del指令。 最基本操作 distributed-lock 加锁1.0 版本123setnx lock-key true // 加锁do the job // 执行任务del lock-key // 解锁 1.0版本有个问题，如果执行逻辑到中间出现异常了，可能会导致del指令没有被调用，这样就会陷入死锁，锁永远得不到释放从而发生死锁。所以需要在拿到锁之后加上一个过期时间，比如5s, 这样即使中间出现异常也可以保证5s之后锁会自动释放。于是 distributed-lock 加锁2.0 版本：1234setnx lock-key true // 加锁expire lock-key 5 // 加上过期时间避免死锁do the job // 执行任务del lock-key // 解锁 但是以上逻辑还是有问题，如果setnx和expire之间服务器进程突然挂掉了，可能是因为机器掉电或者人为导致，就会导致expire得不到执行，也会造成死锁。 这种问题的根源就在于setnx和expire是两条指令而不是原子指令。如果这两条指令可以一起执行就不会出现问题。为了解决这个问题，Redis 2.8版本中，作者加入了set指令的扩展参数，使得expire和setnx可以一起执行，彻底解决了分布式锁的乱象。 distributed-lock 加锁3.0 版本：12set lock-key true ex 5 nx //5s超时del lock-key 3.0版本还有一个问题：如果锁被错误的释放（如超时），或被错误抢占，或因redis问题等导致锁丢失，无法很快感知到。例如：1号线程任务超时，这时候锁过期了，第二个线程重新持有了这把锁， 但是紧接着第一个线程执行完了业务逻辑，就把锁给释放了，第三个线程就会在第二个线程逻辑执行完之间拿到了锁。— 这就要求自己的锁只能自己解。distributed-lock 加锁4.0 版本：12345set lock-key random_value ex 5 nx //5s超时// do stheval &quot;if redis.call(&apos;get&apos;,KEYS[1]) == ARGV[1] then return redis.call(&apos;del&apos;,KEYS[1]) else return 0 end&quot;//方案4在3的基础上，增加对 value 的检查，只解除自己加的锁。//此方案 redis 原生命令不支持，为保证原子性，需要通过lua脚本实现:。 此方案更严谨：即使因为某些异常导致锁被错误的抢占，也能部分保证锁的正确释放。并且在释放锁时能检测到锁是否被错误抢占、错误释放，从而进行特殊处理。 推荐使用此方式。 注意事项1.锁的过期时间锁的过期时间是一个比较重要的变量： 锁的过期时间不能太短，否则在任务执行完成前就自动释放了锁，导致资源暴露在锁保护之外。锁的过期时间不能太长，否则会导致意外死锁后长时间的等待。除非人为接入处理。因此建议是根据任务内容，合理衡量锁的过期时间，将锁的过期时间设置为任务内容的几倍即可。如果实在无法确定而又要求比较严格，可以采用定期 setex/expire 更新锁的过期时间实现。具体可以参考Reddision的看门狗机制，如果加锁的业务没有执行完，就会给锁的过期时间续期一段时间。 2.重试如果拿不到锁，建议根据任务性质、业务形式进行轮询等待。等待次数需要参考任务执行时间。 3.基于故障转移实现的缺陷–master挂掉单点失败问题。如果Redis挂了怎么办？如果只增加一个slave节点解决是行不通的。Redis的主从同步通常是异步的。当正好一个节点挂掉的时候，多个客户端同时取到了锁 在这种场景（主从结构）中存在明显的竞态: 客户端A从master获取到锁 在master将锁同步到slave之前，master宕掉了。 slave节点被晋级为master节点 客户端B取得了同一个资源被客户端A已经获取到的另外一个锁。安全失效！ 多机Redis分布式锁实现RedLockRedLock主要解决Redis没有总是可用的保障，解决failover问题。加锁的时候，它会向过半节点发送 set(key, value, nx=True, ex=xxx)指令，只要过半节点set成功，那就认为加锁成功。释放锁时，需要向所有节点发送del 指令。不过Redlock算法还需要考虑出错重试、时钟漂移等很多细节问题，同时因为RedLock需要向多个节点进行读写，意味着相比单实例Redis性能会下降一些。 具体实现在Redis分布式环境中，我们假设有N个Redis master。这些节点完全独立，不存在主从复制或者其他集群协调机制。redlock确保在每（N)个实例上使用此方法获取和释放锁。假设有5个不会同时都宕掉的Redis master节点。 为了取到锁，客户端应该执行以下操作: 获取当前Unix时间，以毫秒为单位。 依次尝试从N个实例，使用相同的key和随机值获取锁。在步骤2，当向Redis设置锁时,客户端应该设置一个网络连接和响应超时时间，这个超时时间应该小于锁的失效时间。例如你的锁自动失效时间为10秒，则超时时间应该在5-50毫秒之间。这样可以避免服务器端Redis已经挂掉的情况下，客户端还在死死地等待响应结果。如果服务器端没有在规定时间内响应，客户端应该尽快尝试另外一个Redis实例。 客户端使用当前时间减去开始获取锁时间（步骤1记录的时间）就得到获取锁使用的时间。当且仅当从大多数（这里是3个节点）的Redis节点都取到锁，并且使用的时间小于锁失效时间时，锁才算获取成功。 如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间（步骤3计算的结果）。 如果因为某些原因，获取锁失败（没有在至少N/2+1个Redis实例取到锁或者取锁时间已经超过了有效时间），客户端应该在所有的Redis实例上进行解锁（即便某些Redis实例根本就没有加锁成功）。 这个方案的缺点就是太重，通常不被推荐。如果很在乎高可用性，希望挂了一台Redis完全不受影响，那么应该考虑redlock. Jedis 实现Java实现：12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718private static final String SET_IF_NOT_EXIST = \"NX\";private static final String SET_WITH_EXPIRE_TIME = \"PX\";private static final String LOCK_SUCCESS = \"OK\";public static boolean tryGetDistributedLock(Jedis jedis, String lockKey, String requestId, int expireTime) { /** * arg1 : key * arg2: value * arg3: nxxx, NX 代表set if not exist, 即当key 不存在的时候，进行set; 若key已经存在，不做任何操作 * arg4: expx, PX 代表加个过期的设置 * arg5: timeout 过期时间 */ String result = jedis.set(lockKey, requestId, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, expireTime); if (LOCK_SUCCESS.equals(result)) { return true; } return false;} 再看解锁。distributed-lock 解锁1.0 版本如下：1del distributed-lock // 解锁 这是最简单的解锁方法，但是容易出现问题：不预先判断锁的拥有者而直接解锁，会导致任何客户端都可以随时进行解锁，即使这把锁不是它的 – 违背了解铃还需系铃人。 distributed-lock 解锁2.0 版本：12345lockId = get distributed-lock if(lockId.equals(requestId)) { // 若在此时，这把锁突然不是这个客户端的，则会误解锁。 del distributed-lock} 但是2.0版本也不是完美的，如代码注释，问题在于如果调用jedis.del()方法的时候，这把锁已经不属于当前客户端的时候会解除他人加的锁。那么是否真的有这种场景？答案是肯定的，比如客户端A加锁，一段时间之后客户端A解锁，在执行jedis.del()之前，锁突然过期了，此时客户端B尝试加锁成功，然后客户端A再执行del()方法，则将客户端B的锁给解除了。 此博客提供了一种功能3.0版本方法，通过Lua实现原子性解锁。 distributed-lock 解锁3.0 版本：123456789101112131415161718/** * 释放分布式锁 * @param jedis Redis客户端 * @param lockKey 锁 * @param requestId 请求标识 * @return 是否释放成功 */public static boolean releaseDistributedLock(Jedis jedis, String lockKey, String requestId) { String script = \"if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end\"; Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId)); if (RELEASE_SUCCESS.equals(result)) { return true; } return false;} Spring实现到这里，基本上一个完整的Java Redis分布式锁已经实现了。但是在生产环境发现上述代码并不实用。因为我们的生产环境采用了Spring的template实现redis 客户端的封装，Spring封装的template并不精细，并没有distributed-lock 加锁3.0 版本：set distributed-lock true ex 5 nx的setnx+expire的原子性实现。所以只能用下面的时间戳方式缓解：1234567891011// 每个进程一个随机数，用来实现“解铃还须系铃人”功能。public class SoaTask extends BasicTask { public static final String TASKKEY = &quot;:task&quot;; public static final Integer EXPIRE = 15; public static volatile String code = &quot;default&quot;; @PostConstruct public void init() { code = UUID.randomUUID().toString(); }} 12345678910111213141516171819202122232425262728293031protected boolean lock() { boolean lockRes = codisCacheTemplate.setIfAbsent(SoaTask.TASKKEY+getTimestamp(), SoaTask.code); if (lockRes) { // 任务一天执行一次 codisCacheTemplate.expire(SoaTask.TASKKEY+getTimestamp(), 86400); } return lockRes; } protected boolean unlock() { if (codisCacheTemplate.get(SoaTask.TASKKEY+getTimestamp()).equals(SoaTask.code)) { codisCacheTemplate.del(SoaTask.TASKKEY+getTimestamp()); } return true; } protected void doJob() { try { if (lock()) { //do job } }finally { unlock(); } } private String getTimestamp() { SimpleDateFormat formatter = new SimpleDateFormat(PATTERN_YYYYMMDD); String formatStr =formatter.format(new Date()); return formatStr; } 时间戳+随机数做分布式锁redis key的方式本质上是通过时间戳做一个超时方案，但时间戳的粒度需要结合分布式任务执行时间来操作。例如我们的分布式定时任务是1天执行一次，那么我就把时间戳设置为YYYYMMDD格式的，这样第二天的分布式锁的key即使不释放，key也是新的，新任务是可以执行的，以此避免了死锁。另外加上随机数保证了自己的锁只能自己解。 ReddisonRedisson是Java语言编写的基于Redis的客户端。分布式锁的实现具有借鉴意义。为了解决“加锁线程在没有解锁之前崩溃而出现死锁“的问题，不同于Redis中通过设置超时时间来处理。Reddison采用了新的处理方式：Redisson内部提供了一个监控锁的看门狗，它的作用是在Redis实例被关闭前不断延长锁的有效期。跟Zookeeper类似，Redisson也提供了这几种分布式锁：可重入锁，公平锁，联锁，红锁，读写锁等。 参考 https://wudashan.cn/2017/10/23/Redis-Distributed-Lock-Implement/ https://mp.weixin.qq.com/s/eU_2lh1slxv3H0v3gFk37Q https://mp.weixin.qq.com/s/doYn9riDh4AdpTyT4OgCwA Reddision 基于Redis的分布式锁到底安全吗","link":"/2019/06/21/distributed-lock/"},{"title":"责任链模式实践","text":"最近参与的项目开发了大量RPC接口，并且需要针对所有RPC接口开发接入公司方法监控的埋点代码。开发RPC方法的监控埋点代码有两种方式： 1、在每个RPC方法体内添加埋点代码。 这是最简单直观的开发方式，但是会造成大量重复冗余的代码。假设项目有m个RPC类，每个类有n个方法，就要开发m*n个监控埋点代码，而监控埋点代码除了方法监控key之外没有任何不同的。显然这种方式并不优雅，耦合度很高。 2、使用责任链模式处理所有RPC的调用请求。 责任链模式用一个调用链组织所有过滤器类，每一个调用请求从过滤器类间依次传递，每个过滤器可以选择是否对调用请求执行自己的操作。因此可以实现一个监控请求的过滤器，该过滤器专门实现针对所有RPC方法调用的监控埋点。 这种方法实现只在代码集中的一处(过滤器类)动态添加监控，而不是在每个RPC接口都添加一段重复的监控埋点代码。提高了代码复用性, 统一添加了方法监控。 123456789101112131415161718192021222324252627/** *为方法监控添加埋点代码的过滤器类 */public class InvokeExceptionFilter extends AbstractFilter{ @Override public ResponseMessage invoke(RequestMessage request) { ResponseMessage responseMessage = MessageBuilder.buildResponse(request); String requestClass = request.getClassName(); String requestMethod = request.getMethodName(); // 调用方法名 String clazzNameStr = StringUtils.substring(requestClass,requestClass.lastIndexOf(\".\")); //RPC interface名称 String alias = request.getInvocationBody().getAlias();// 用于区分业务 String methodWatcherKey = clazzNameStr+\"-Alias:+\"+alias+\"::\"+requestMethod; registerWatcher(methodWatcherKey)// 开始方法监控 try { // 调用链自动往下层执行 request.getInvocationBody().getAlias() responseMessage = this.getNext().invoke(request); }catch (Exception e){ watcherCalculate(methodWatcherKey) // 统计方法可用率 }finally { registerInfoEnd(methodWatcherKey); // 此方法监控结束 } return responseMessage; }} 责任链模式初探 Chain of Responsibility gives more than one object an opportunity to handle a request by linking receiving objects together. by GoF 责任链模式是一种行为模式。责任链允许多个类参与处理一个请求。多个互相独立的类(过滤器)组成一个调用链，请求从调用链的第一个过滤器传递给后一个过滤器，直到这个请求满足某个过滤器的处理条件被处理，至此调用链执行完毕。 使用场景解耦请求发送者和请求接收者，让多个过滤器有机会去处理请求。具体分为以下三个使用场景： 1、多个对象都可以处理一个请求，请求的处理者不需要是一个专门的对象。 2、多个对象都可以处理一个请求，请求的处理者在运行时决定。 3、请求没有被处理也是可接受的。 使用范例除了前文提到的统一监控埋点代码，还有windows系统处理鼠标或键盘产生的事件使用到了责任链模式。另外，异常统一处理也可以使用此模式。Servlet 的过滤器就是根据责任链模式设计的。 如何使用责任链模式责任链模式由以下两个角色组成 抽象过滤器角色 抽象过滤器定义了 - 一个处理请求的接口 - 下一个过滤器的指针 - 获取和设置下一个过滤器的接口 具体过滤器角色 具体过滤器接到请求后，针对请求判断是否满足自己处理条件，可以选择将请求处理掉，或者将请求传给下一个过滤器。 12345678910111213141516171819202122232425262728public interface Filter { ResponseMessage invoke(RequestMessage var1);}/** *抽象过滤器角色 */public abstract class AbstractFilter implements Filter { private Filter next; //下一个过滤器 abstract public ResponseMessage invoke(RequestMessage request); //调用RPC服务 /** * get下一层过滤器 */ protected Filter getNext() { return next; } /** * Sets next下一层过滤器 */ protected void setNext(Filter next) { this.next = next; } ...} 有可能引起的问题责任链模式也有缺点，这与if…else…语句的缺点是一样的，那就是在找到正确的处理类之前，所有的判定条件都要被执行一遍，当责任链比较长时，性能问题是一个值得考虑的问题。 参考资料 https://dzone.com/articles/design-patterns-uncovered-chain-of-responsibility http://blog.csdn.net/eson_15/article/details/52126811","link":"/2017/10/22/filter-chain-pattern/"},{"title":"为什么MySQL会抖一下 - 关于刷脏页磁盘","text":"一条SQL语句，正常执行时候特别快，有时候会突然变得特别慢，而且很难复现，它不只是随机而且持续时间很短。 看上去像数据库抖了一下 – 原因就是MySQL在刷脏页到磁盘。当内存数据页和磁盘数据页内容不一致的时候，这个数据页被称为“脏页”。内存数据写入磁盘后，内存和磁盘的数据页的内容就一致了，称为“干净页”。 不论脏页还是干净页，都存在内存里。 触发数据库的刷脏页时机 InnoDB的redo log写满了。这时候系统会停止所有更新操作去刷盘。这种情况应该尽量避免，因为出现这种情况的时候，整个系统就不能再接受更新，所有更新呗堵住。 内存不足。当需要新的内存页，内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要将脏页写到磁盘。 InnoDB用缓冲池buffer pool来管理内存，缓冲池中的内存页有三种状态： 还没使用 使用了并且是干净页 使用了并且是脏页InnoDB的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；当如果是脏页，就必须先讲脏页刷盘，变成干净页后才能复用。所以刷脏页是常态。 但是出现以下情况会明显影响性能： 一个查询要淘汰的脏页个数太多，就会导致查询的响应事件明显变长。 日志写满，更新全部堵住，写性能跌为0，这种对于敏感业务来说，是不能接受的。 系统空闲时候。 MySQL正常关闭时候。 刷盘策略InnoDB 需要有控制脏页比例的机制，来尽量避免性能的影响。 设置InnoDB所在主机的IO能力，这样InnoDB可以获知全力刷脏页可以多快。1innodb_io_capacity = IOPS case: 如果出现MySQL写入速度很慢，TPS很低，而数据库主机IO压力并不大，很可能就是这个原因。 InnoDB控制引擎按照“全力”的百分比刷脏页 - 刷盘速度。 参考https://time.geekbang.org/column/article/71806","link":"/2019/02/08/flush/"},{"title":"Java8函数式接口","text":"Java8引入了“行为参数化”的理念。为了实现行为参数化，java8提出函数式接口和Lambda表达式。本文首先会讲讲什么是函数式接口，然后会讲到java8预定义的四种核心函数式接口，以及使用这四种接口处理问题的demo。同时，本文还会使用大量的用Stream处理集合数据的例子。 函数式接口 functional interfaces函数式接口： 函数式接口是一种特殊的SAM类型(Single Abstract Method), 即只定义一个抽象方法的接口。 使用@FunctionalInterface标注一个接口即表示该接口是一个函数式接口，如果你用@FunctionalInterface定义了一个接口，而它却不是函数式接口的话，编译器将返回一个提示原因的错误。 Lambda表达式是函数式接口的实例。以函数式接口为参数的方法，可以在调用时使用Lambda表达式作为参数。 之所以java8设计函数式接口，主要目的是因为：函数式接口的抽象方法可以使用Lambda表达式作为输入参数。本质上讲，函数式接口和Lambda表达式将行为参数化成为了可能。 四种核心接口Java8预定义了大量的函数式接口，这样客户端可以直接使用。这些预定义的函数式接口定义在java.util.function下，通常分为以下四种。 函数式接口 参数及返回类型 用途 Consumer void accept(T t) 消费型接口，接受一个参数，没有返回值 Supplier T get() 供给型接口，不接受参数，有一个返回值 Function R apply(T t) 功能型接口，接受一个参数，处理后返回一个值 Predicate Boolean test(T t) 断言性接口，接受一个参数，返回判断结果boolean Consumer顾名思义，Consumer接口使用场景为当一个对象需要被“消费”掉的时候。即这个对象作为方法的输入参数被执行某些操作，而且方法不做任何返回。打印操作就是一个典型的消费操作。 Consumer 源码：1234@FunctionalInterfacepublic interface Consumer&lt;T&gt; { void accept(T t);} 使用consumer执行打印的demo123456789101112131415public class ConsumerExample{ public static void main(String args[]){ Consumer&lt;Integer&gt; consumer= i-&gt; System.out.print(\" \"+i); List&lt;Integer&gt; integerList=Arrays.asList(new Integer(1), new Integer(2), new Integer(3), new Integer(4), new Integer(5), new Integer(6)); printList(integerList, consumer); } public static void printList(List&lt;Integer&gt; listOfIntegers, Consumer&lt;Integer&gt; consumer){ for(Integer integer:listOfIntegers){ consumer.accept(integer); } }} 另外，Consumer的doc提到一句， Consumer is expected to operate via side-effects. Consumer接口可以执行带有副作用的操作，即Consumer的操作可能会更改输入参数的内部状态。实践中，我们可以使用Consumer来更改对象内部状态。 例如Stream中使用率很高的forEach方法。forEach方法是java8新引入的内部遍历(有关内部遍历和外部遍历的区别，可以参见link). 举个例子：根据获取到的系统所有模块列表，创建一个map，map的key是模块id, value是模块对象。 12345678910default Map&lt;Integer, Module&gt; getModules() { Map&lt;Integer, Module&gt; moduleMap = new HashMap&lt;&gt;(); List&lt;? extends Module&gt; moduleMapList = getAllModulePrototype(); // 获取系统所有模块列表 if (CollectionUtils.isNotEmpty(moduleMapList)) { moduleMapList.forEach(module -&gt; moduleMap.put(module.getId(), module)); } return moduleMap; } Supplier当某个场景不需要输入但是需要输出的时候，就可以用到Supplier。 Consumer 源码：12345678@FunctionalInterfacepublic interface Supplier&lt;T&gt; { /** * Gets a result. * @return a result */ T get();} 对于Supplier，可以理解为利用它生产一个新的对象。例如通过实现Supplier接口，可以自己来控制流的生成(generater方法)。 12345678910111213141516//生成num个整数,并存入集合public List&lt;Integer&gt; getNumList(int num, Supplier&lt;Integer&gt; sup) { List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; num; i++) { Integer n = sup.get(); list.add(n); } return list;}public static void main(String[] args) { //10个100以内的随机数 List&lt;Integer&gt; numList = getNumList(10, () -&gt; (int) (Math.random() * 100)); for (Integer num : numList) { System.out.println(num); }} FunctionFunction接口主要用于映射场景. A类型的对象作为输入参数被执行Lambda表达式操作，最后转换为B类型的对象返回。 1234@FunctionalInterfacepublic interface Function&lt;T, R&gt; { R apply(T t);} Function接口被用在Stream的map方法的输入参数，map方法把input Stream的每一个元素，映射成output Stream的另外一个元素。 Stream的map方法:1&lt;R&gt; Stream&lt;R&gt; map(Function&lt;? super T, ? extends R&gt; mapper); 举例，将列表的数字转换成平方数12List&lt;Integer&gt; nums = Arrays.asList(1,2,3,4);List&lt;Integer&gt; squareNums = nums.stream().map(n -&gt;n*n).collect(Collectors.toList); 再比如：获取Person对象的姓名123456789101112List&lt;Person&gt; persons = Arrays.asList( new Person(\"gsm\", 26), new Person(\"nx\", 24), ); String name = persons.stream() .filter(x -&gt; \"gsm\".equals(x.getName())) .map(Person::getName) //convert stream to String .findAny() .orElse(\"\"); System.out.println(\"name : \" + name); 1输出：name : gsm PredicatePredicate的使用场景为：一个对象需要被评估是否满足某条件，并且返回一个boolean型作为评估结果。 Predicate源码：123456package java.util.function;import java.util.Objects;@FunctionalInterfacepublic interface Predicate&lt;T&gt; { boolean test(T t);} 使用demo：自己定义一个以Predicate为参数的的方法。123456789101112public int sumAll(List&lt;Integer&gt; numbers, Predicate&lt;Integer&gt; p) { int total = 0; for (int number : numbers) { if (p.test(number)) { total += number; } } return total;}sumAll(numbers, n -&gt; true); // 求数字列表的总和sumAll(numbers, n -&gt; n % 2 == 0); //求数字列表的偶数总和sumAll(numbers, n -&gt; n &gt; 3);//求数字列表大于3的数字总和 再比如Stream中使用率很高的filter方法。filter方法返回一个由满足predicate条件的元素组成的列表。 1Stream&lt;T&gt; filter(Predicate&lt;? super T&gt; predicate); 使用filter示例,找出名称为自定义模块2的模块。 1234List&lt;Module&gt; moduleList = {new Module(\"自定义模块1\"), new Module(\"自定义模块2\",new Module(\"自定义模块3\")};List&lt;Module&gt; result = richModuleList.stream().filter( richModule -&gt; richModule.getName().equals(\"自定义模块2\")).collect(Collectors.toList());","link":"/2017/10/17/functional-interface/"},{"title":"Git子模块实践","text":"应用背景Git 子模块的使用场景是多个项目都使用了一个公共的项目，为了节省开发成本并且减少出错几率，我们想要实现：每个使用公共项目的外部项目如果更改了这个公共项目，这些更新都可以同步到其他使用了这个公共项目的项目中。Git提供了「子模块」这个工具。 举个例子，我们的浏览项目是组建化活动装修中的一个lua项目，具体业务涉及到某A国的店铺／活动浏览业务，以及某B国活动／店铺浏览业务，以后还会有更多扩展业务。某A国的店铺／活动浏览业务（我们这里称deploy_mall和deploy_act）部署工具支持lua脚本的直接运行，因此某A国项目代码库只需要包括： lua业务核心代码 — esale文件夹 部署脚本（nginx.conf相关） 但是某B国项目（我们这里称为sale-web)部署工具不支持lua脚本的运行，因此需要自己搭建一个web shell(tomcat)来启动，我们只能把lua代码放在 webapp/WEB-INFO/esale 文件夹下。因此代码库包括： web.xml 的src目录 部署脚本（nginx.conf相关） webapp/WEB-INFO/esale文件夹的lua核心代码 可见，sale-web和esale两个工程虽然代码结构不同，但是核心都是一个esale文件夹下的lua浏览工程。因此，我们需要实现任何一个项目，例如sale-web对lua代码做了更新，deploy_mall工程或者deploy_sale可以同步拉到更新，从而“实现维护一套代码，组建化支持多个项目”这个目的。 子模块原理一个Git子模块就是一个标准的Git仓库。不同的是，这个Git仓库被包含在另外一个或者多个外部项目的Git仓库中（有点内部类的意思）。子模块和普通的Git仓库没什么区别，可以对他进行pull,commit,fetch,push等。 那Git 是如何保持多个外部项目仓库和他的Git子模块实现同步更新的呢？答案在于当我们在外部仓库中创建了一个Git子模块，会在外部仓库中同时创建一个 .gitmodule， .gitmodule 里面记录了对子模块的git 引用路径和子模块代码库的地址，即标识了哪个目录是子模块，这个子模块的代码地址。 事实上，外部项目并没有保存子模块的代码，只是保存了一个指向子模块的引用。所以如果我们git clone一个外部项目时候，会发现clone下来外部项目的子项目文件夹是空的。(a) 可以通过将 “–recurse-submodules” 参数加在 “git clone” 上，从而让 Git 知道，当克隆完成的时候要去初始化所有的子模块。(b) 如果仅仅只是简单地使用了 “git clone” 命令，并没有附带任何参数，你就需要在完成之后通过 “git submodule update –init –recursive” 命令来初始化那些子模块 实践搭建子模块Step 1: 创建子模块的git 代码库。我们给这个项目起名 esaleStep 2: 分别在两个使用到 sub-esale的项目（esale和sale-web）中添加子模块： 对于sale-web 使用“git submodule add http://你的git地址” 会发现生成了一个esale文件夹，代表git子模块。在外部项目里会发现生成了记录子模块引用的.gitmodules文件。 使用子模块的某个分支和一个普通的Git仓库不同的是，子模块永远指向一个特定的提交，而不是分支。这是因为一个分支的内容可以在任何时间通过新的提交来改变。所以指向一个特定的提交版本就能始终保证代码的正确。所以通常外部项目具体使用子项目哪次提交来运行，我们称之为「签出一个版本」。 如果我们想让外部项目使用子模块的一个特定分支： Step 0: 进入到子模块查看 git log —oneline —decorate 查看历史提交和分支记录 Step 1: git checkout 特定分支（也可以git checkout versionnum） Step 2: 在外部项目中查看 git submodule status 子模块当前哪个版本被签出了。正是我们checkout到新分支的版本 Step 3: 为了让这个改动生效，需要将它commit到库里 克隆一个带有子模块的项目克隆一个带有子模块的项目。将得到了包含子项目的目录，但里面没有文件：esale 目录虽然存在，但是是空的。需要运行两个命令： 12git submodule init 初始化你的本地配置文件git submodule update 从那个项目拉取所有数据并检出你上层项目里所列的合适的提交。 对子模块做更新修改esale子模块当修改esale子模块内容并提交后，在sale-web外部工程进入esale目录拉取代码。可以同步收到更新。git pull orgin master 切换到，一个没有submodule的分支时，会首先需要把submodule目录移动走。当我们切换回来时，由于lua目录被移走需要手动移回来或者执行上述命令重新初始化一次。最后执行一次同步。 参考https://www.git-tower.com/learn/git/ebook/cn/command-line/advanced-topics/submodules https://git-scm.com/book/zh/v2/Git-%E5%B7%A5%E5%85%B7-%E5%AD%90%E6%A8%A1%E5%9D%97","link":"/2017/11/28/git-submodule/"},{"title":"Java内部类详解","text":"WhatSun公司在JDK1.1以后的版本中引入了内部类的概念：一个类可以定义在另一个类之中。这个嵌套着另一个类的类叫做”外部类（outer class）”, 被嵌套的类叫做内部类（inner class）. 内部类示例： 123456789101112131415161718192021222324252627public class OuterClass { private String name ; private int id; public String getName() { return name; } public void setName(String name) { this.name = name; } public int getId () { return id ; } public void setId (int id ) { this. id = id ; } class InnerClass{ public InnerClass(){ name = \"testName\"; age = 1; } }} 可以看出当引入内部类后，外部类的结构变为了： 外部类 成员变量 成员函数 内部类 匿名内部类没有类名的内部类就是匿名内部类（anonymous inner class）.我们通常同时声明和实例化一个匿名内部类。 语法：123456FatherClass anonymousInnerClassObject = new FatherClass() { public void myMethod() { ........ ........ } }; 在定义一个匿名内部类之前，需要先定义出这个匿名内部类的父类。123abstract class FatherClass { public abstract void myMethod();} 匿名内部类相当于这个父类的实现。 123456789101112public class Outer_class { { public static void main(String args[]) { FatherClass inner = new FatherClass() { public void myMethod() { System.out.println(&quot;This is an example of anonymous inner class&quot;); } }; inner.myMethod(); }} Note : 我们给匿名内部类传递参数的时候，若该形参在内部类中需要被使用，那么该形参必须要为final。也就是说：当所在的方法的形参需要被内部类里面使用时，该形参必须为final。为什么必须是final,具体请参考 简单来说是因为：内部类并不是直接调用方法传递的参数，而是利用自身的构造器对传入的参数进行备份，自己内部方法调用的实际上时自己的属性而不是外部方法传递进来的参数。改了外部方法参数，并不会影响内部参数。为了保持参数的一致性，就规定使用final来避免形参的不改变。 Why为什么要使用内部类？ 多重继承 传递行为 隐藏接口的实现细节 多重继承多重继承指的是一个类可以同时从多于一个的父类那里继承行为和特征，然而我们知道Java为了保证数据安全，它只允许单继承。如果父类为抽象类或者具体类，那么我就仅能通过内部类来实现多重继承了。 12345678910111213141516171819202122232425public class Son { /** * 内部类继承Father类 */ class Father_1 extends Father{ public int strong(){ return super.strong() + 1; } } class Mother_1 extends Mother{ public int kind(){ return super.kind() - 2; } } public int getStrong(){ return new Father_1().strong(); } public int getKind(){ return new Mother_1().kind(); }} 传递行为Jdk1.8 之前，Java语言发展这么多年来一直是“重对象，轻函数”的设计理念。函数对于Java这种依赖于对象存在的语言似乎不那么重要，因此Java语言设计者们在设计时也不那么考虑函数。比如Java里无法将函数作为函数的输入参数传递， 只能借助匿名内部类这种方式 例如线程池处理代码： 12345678threadPoolTaskExecutor.execute(new Runnable() { @Override public void run() { ...//代码逻辑 } }); } 隐藏接口的实现细节内部类还可以实现代码的隐藏，将一个类放在另一个类之间，隐藏对一个接口的实现细节。内部类提供了更好的封装，除了该外围类，其他类都不能访问。 解惑 嵌套类和内部类的区别？ 嵌套类 非静态嵌套类， which is 内部类. 内部类是嵌套类的一种。 静态嵌套类， 已经很少用了。 为什么内部类很特殊【1】？ 内部类的实例可以获取外部类成员变量的消息，即使是私有成员变量也是可以的。 内部类和静态嵌套类的区别？ 内部类的实例可以获取外部类成员变量的消息，即使是私有成员变量也是可以的。而静态嵌套类不能。 内部类不能有 static 数据和 static属性。因为静态域是不能访问非静态的方法和成员变量，如果inner class是有静态的,就违背了【1】 什么时候使用内部类? 知道这个问题十分必要，因为错误的场合使用内部类会导致代码难以理解和维护。首先是可以多重继承。内部类可以继承一个与外部类无关的类，保证了内部类的独立性，正是基于这一点，多重继承才会成为可能。 其次，一般情况由于面向对象设计，一个类具有专有的功能，但是如果这个类还需要另外一个类的信息与之交互，比如类方法的参数希望是一种行为，那么就可以使用内部类。 参考chenssy’s blog","link":"/2017/09/07/inner-class/"},{"title":"一种进程间通信的方案与实现","text":"背景在外化重构过程中，Python包的功能迁移到了Java服务端实现，而部分三方依赖，用Java语言重写开销过大，所以需要某种方式通过Java程序调用本地Python进程，执行用户请求并完成响应。 前期的实现方式为基于apache的exec包，每次用户请求，调起一个Python进程，完成用户请求。在压测过程中发现，当发压机生成的请求过多之后，会引发Linux操作系统的OOM机制，主动kill掉一些python进程甚至杀死Java的服务进程，QPS过低且存在服务崩溃的风险。 改造思路本次改造主要解决的问题在于提高系统吞吐量，并规避Linux OOM的问题，保护Java的服务进程。首先为每个请求创建一个Python进程的本身开销就很大，其次当遇到一些需要处理图片等内存占用较高的Python进程，请求量大了之后就极容易触发Linux的OOM kill机制。 所以本次改造的着手点在于： 固定Python进程数目，采用基于内存映射的方式，完成Java进程和Python进程间的通信 引入请求的排队和超时机制，在流量突发的情况下，使得服务程序也能够平滑稳定的执行用户请求 实现要点 Spring的异步请求；基于DeferredResult将用户请求异步化，避免因为执行某些耗时较长的python方法阻塞Spring执行用户请求的核心线程 用户请求的排队，reject以及负载均衡机制 基于内存映射的进程间通信，需要解决请求响应的匹配识别，通信消息的编解码方式以及映射文件的无锁化访问 Python侧程序的改造 请求处理模型初始方案以每个独立的python进程为核心，构造一套闭环的执行Loop。 其中，每个用户请求会首先放入一个Blocking queue中，等待处理，如果队列满了，请求被直接拒绝。Pthread负责从直接关联的queue里取请求，通过mapped file通知python进程处理用户请求，python读取并执行后通过另外一个mapped file将结果发送给接收线程Cthread，由Cthread完成包装后，返回给用户。此处，之所以创建独立的Cthread来处理结果，是由于针对python执行结果的可能会比较耗时，比如会写图片到OSS这种操作，有利于提高吞吐量。 在第一版的方案实现中存在一个问题就是：同一个Loop的Pthread需要等到Cthread回写结果完成后，才能继续处理下一个请求，（二者依赖信号量同步）其实在Cthread从mapped file取出处理结果后，Pthread就可以继续处理下一个请求，而无需等待Cthread的业务逻辑处理。所以，把Cthread的功能进一步拆解为两个部分：读mapped file 和 响应结果处理，并将读mapped file的逻辑上推至Pthread中，以线程池的方式执行响应结果处理，进一步提高并发程度，并且减少了一份 mmap file的使用。 改进方案： 请求与响应的读写都由pthread控制， 减少mapped file文件的数量。引入线程池，独立执行响应的处理，做到更强的异步化处理。 压测结果以ybc_table为例，改造前10thread， 20s的压测结果如下 123456789101112131415161718`&gt; ---- Global Information -------------------------------------------------------- &gt; request count 14140 (OK=424 KO=13716 ) &gt; min response time 1 (OK=53 KO=1 ) &gt; max response time 426 (OK=426 KO=100 ) &gt; mean response time 13 (OK=238 KO=7 ) &gt; std deviation 42 (OK=62 KO=10 ) &gt; response time 50th percentile 6 (OK=250 KO=6 ) &gt; response time 75th percentile 8 (OK=274 KO=8 ) &gt; response time 95th percentile 50 (OK=332 KO=18 ) &gt; response time 99th percentile 260 (OK=385 KO=57 ) &gt; mean requests/sec 673.333 (OK=20.19 KO=653.143) ---- Response Time Distribution ------------------------------------------------ &gt; t &lt; 800 ms 424 ( 3%) &gt; 800 ms &lt; t &lt; 1200 ms 0 ( 0%) &gt; t &gt; 1200 ms 0 ( 0%) &gt; failed 13716 ( 97%) ---- Errors -------------------------------------------------------------------- &gt; status.find.in(200,304,201,202,203,204,205,206,207,208,209), b 13716 (100.0%) ut actually found 500 改造后，10个独立的python进程，100thread， 60s的压测结果如下，吞吐量和响应时间都有显著提升 123456789101112131415161718---- Global Information -------------------------------------------------------- &gt; request count 53235 (OK=53232 KO=3 ) &gt; min response time 1 (OK=1 KO=3674 ) &gt; max response time 4434 (OK=3913 KO=4434 ) &gt; mean response time 112 (OK=112 KO=3954 ) &gt; std deviation 211 (OK=209 KO=340 ) &gt; response time 50th percentile 13 (OK=13 KO=3756 ) &gt; response time 75th percentile 153 (OK=153 KO=4095 ) &gt; response time 95th percentile 481 (OK=481 KO=4366 ) &gt; response time 99th percentile 771 (OK=769 KO=4420 ) &gt; mean requests/sec 858.629 (OK=858.581 KO=0.048 ) ---- Response Time Distribution ------------------------------------------------ &gt; t &lt; 800 ms 52761 ( 99%) &gt; 800 ms &lt; t &lt; 1200 ms 324 ( 1%) &gt; t &gt; 1200 ms 147 ( 0%) &gt; failed 3 ( 0%) ---- Errors -------------------------------------------------------------------- &gt; status.find.in(200,304,201,202,203,204,205,206,207,208,209),3 (100.0%) ut actually found 500","link":"/2020/01/07/ipc/"},{"title":"Japan-霓虹归来","text":"之所以这次旅行的目的地选择了日本，实在也是一个不得已的选择。无奈世界大山大河虽美好，但不再和平。思来想去只有亚洲还算相对岁月静好，霓虹国又尚未去过，一行人对日本文化也很感兴趣，于是筹划前往。这次一行九天，大阪进东京出，在大阪体验拉面文化，迷失在京都千年神社，邂逅奈良神鹿，筑地市场品尝最新鲜的海鲜，在六本木之丘的52层高的大楼鸟瞰东京夜景，鸭川跑道上驻足希望偶遇村上春树…一直认为旅行占生活中很重要一部分，和老友相聚，接触陌生的人和景色去放空自己。虽无法从庸俗的日常彻底抽离，不过也算离“诗和远方”又近了一步。 飞机抵达大阪当天晚上，欣欣然用Google地图搜到附近一家拉面，名字有趣 – “无铁砲”，发现时间虽已过晚上九点，拉面店门口依然有人排队，谷歌搜索店名原来这家是远近驰名的名店，顿时觉得赚到。点了份豚骨拉面，拉面汤特别浓郁，配上店家上来的小菜和冰水，简直一解旅途劳顿。 第二天来到著名的USJ–日本环球影城，哈利波利项目城堡的逼真程度瞬间将我们带到了记忆中的霍格沃茨。一行四人在USJ分成了两队，一队鹏哥和岛胖君专门体验各种刺激项目，另一队是我和金哥，大白鲨的游船已经到了能容忍的刺激极限…最有趣的莫过于我们在逆行过山车项目下面朝着他们加油打气，看着他们一路吼叫划过。 大阪的夜市，随便找一家品尝深夜食堂。 蟹道乐在日本是一家蛮有名的螃蟹料理店，同样也是一家让我们感慨“哇！原来螃蟹肉是这个味道！”的店。蟹肉刺身精致鲜美极了，这次回去终于也可以说“姐是吃过生螃蟹的人了”。 雨中的大阪城，海贼王海军总部的原型。 京都是日本历史最悠久的城市之一，在这里最吸引游客的就要属各个神社了。伏见稻荷大社是有1300年历史的神社，神社里面伫立着大大小小一千多个鸟居，一眼望去还是蛮震撼的。 经历千年洗礼仍十分整洁的京都的街道。 在京都那天，恰逢中秋，居住的民宿是六楼顶楼，远眺京都远方，遥寄故乡。 奈良是著名的鹿的城市，在这个城市你说不准究竟是鹿和人类哪个才是这里的主人。无论是街道边，公园里还是神社内，你都可以看到或懒洋洋睡觉的鹿儿，或跟你鞠躬乞要鹿仙贝的鹿们。 筑地市场是东京最大的海鲜交易市场，这里凌晨三点会开始金枪鱼拍卖。大大小小的料理店遍布在筑地市场，料理所用的海鲜都是当天捕捞到的最新鲜食材所做。 东京街头。手办店的黑武士们。日本的交通。 时间 ： 2017年10月 地点 ： 日本 - 大阪，京都，奈良，东京 人物 ： with @mikloo@鹏哥@小东尼","link":"/2017/12/19/japan/"},{"title":"JVM调优系列之处理内存报警","text":"服务又收到报警了系列。某日服务收到内存报警：123456789状态:P3故障名称:mem.used.percent大于85%指标:mem.used.percent主机:sec-upm-data-analysis-sf-42851-0.docker.py节点:hnc-v.upm-data-analysis.upm.sec.sec.didi.com当前值:92.01说明:happen(mem.used.percent,#60,60) &gt;90故障时间:2020-01-10 16:49:20详情: http://monitor.xiaojukeji.com/#/odin/alarm/event/373081480 一个服务机器（4Core 4G）内存达到了92%。看了下近几天的监控，机器内存持续升高。现场监控图如下： 本能反应看下top:发现4G docker, 我们的java服务已经占据了2.321G, 再加上docker里还跑着监控服务和日志服务，很容易服务达到3G+的内存。 再看JVM堆内存使用分布：1jmap -heap 970 这里发现一个很奇怪的地方MaxHeapSize 竟然近30G，而docker的内存只是4G，说明JVM对堆的内存并没有进行实际限制。我们知道Xmx 默认值是物理内存的1/4（宿主机128G），由此怀疑我们的Java服务很可能没有配置XMX。排查代码，果然如此。 遂增加配置：1-Xmx2g -Xms2g 其他JVM文章： 频繁Full GC引起的服务雪崩 JVM调试工具入门","link":"/2020/01/10/jvm-xms/"},{"title":"一次Kafka消费不到数据的踩坑记录","text":"突然就..kafka消费不到数据了…问题从发生到验证再到解决比较曲折复杂，下文呈现的是事后梳理过的故障解决全貌。 故障大概在昨天下午2点多，kafka突然消费不到数据。表现为： java服务不再打印消费日志，业务不再有新数据的处理； java服务重启后立刻消费几批数据，但是很快就会不再消费（拉到的数据size 为0），异常如下截图： This member will leave the group because consumer poll timeout has expired. This means the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time processing messages. You can address this either by increasing max.poll.interval.ms or by reducing the maximum size of batches returned in poll() with max.poll.records. 当时被kafka-version properties：null问题迷惑，以为是客户端使用的公司kafka-client有问题，改用kafka原生client后，不再有kafka-version properties：null问题，但是仍然消费不到数据。 解决问题的解决是源自厂有一老（如有一宝）的同事的一句提醒，方才从怀疑kafka-client，转变方向到解决poll超时问题。 Kafka max.poll.interval.ms: 它表示最大的poll数据间隔. 如果超过这个间隔没有发起pool请求，但heartbeat仍旧在发，就认为该consumer处于 livelock状态。就会将该consumer退出consumer group。Consumer Coordinator会让消费者离开消费组，并处罚新一轮的rebalance. 所以为了不使Consumer 自己被退出，Consumer 应该不停的发起poll(timeout)操作。而这个动作 KafkaConsumer Client是不会帮我们做的，这就需要自己在程序中不停的调用poll方法了。 通过查wiki公司默认配置：max.poll.interval.ms = 300000(默认间隔时间为300s)max.poll.records = 500","link":"/2019/11/24/kafka-problem/"},{"title":"庖丁解牛-多服务器并发场景下乐观锁的实际应用","text":"庖丁解牛-多服务器并发场景下乐观锁的实际应用 笔者面临的一个业务场景为：项目中多条业务线的实现都需要创建一个实例ID(instanceID)，实例ID的值为系统当前的实例ID最大值+1。这样就会面临着在同一时间下有多个用户更新实例ID，造成并发冲突的问题。 笔者首先想到的解决方法是使用关键字synchronized来解决。但是组内同事提出光有synchronized是不够的，因为synchronized只能解决单台机器的JVM多线程并发问题，无法解决线上多台服务器分布式导致的并发问题。 同事进一步提出方案，可以尝试使用乐观锁解决,使用一个Sequence表记录系统中实例ID的最大值(INSTANCE_SEQUENCE_ID).当前机器创建实例ID时: 先读取数据库中INSTANCE_SEQUENCE_ID为sequenceId； 接着进行业务处理 最后更新Sequence表的INSTANCE_SEQUENCE_ID值为新创建的实例ID值，即+1。 Step3 更新操作实际为compare and set乐观锁控制：123&lt;update id=&quot;updateSequence&quot; parameterType=&quot;Sequence&quot;&gt; UPDATE &lt;include refid=&quot;tableName&quot;/&gt; SET value=#{maxId} WHERE value= #{sequenceId} and sequence_key =#{sequenceKey}&lt;/update&gt; 比较数据库中INSTANCE_SEQUENCE_ID字段数值是否与此次操作获取的sequenceId一致，如一致则表明未有其他机器进行操作，不会丢失更新，可以进行更新。如果不一致，说明其他机器创建了实例ID，则不允许本台机器进行更新。需要重试，再进行一遍1-2-3操作. 这种方案已经可以解决单台机器多线程并发冲突和多服务并发冲突了，但是存在性能缺陷：每次创建一个页面实例ID，都需要查询/更新数据库多次，这样效率大打折扣，在线上生产环境可能不允许这种方案的实现。于是我们继续阅读项目老代码，寻求解决之策。果然老代码使用了一个方案提高了性能。 一言以蔽之，该方案赋予每台机器一个范围scope = 100, 每台机器在范围内进行sequeceId自增，不用查询/更新数据库。当超出范围，即自增加后的sequeceId为101时，再进行step1-2-3. sequence java bean:1234567891011121314151617181920212223242526272829303132public class Sequence { /** * sequenceId变动范围 */ private int scope = 100; /** * 当前sequenceID */ private Long currentSequenceId = null; /** * 开始的sequenceID */ private Long startSequenceId = null; private Long endSequenceId = null; public DbSequence(){ } public DbSequence(int scope){ this.scope = scope; } /** * @param startId */ public synchronized void setSequenceId(Long startId){ currentSequenceId = startId; startSequenceId = startId; endSequenceId = startSequence }} 算法流程如图: 总结同一台机器的并发冲突可以使用synchronize。乐观锁解决多台机器并发冲突，用scope 100降低冲突概率","link":"/2017/07/23/multithread/"},{"title":"Netty 是怎么做内存管理--内存池","text":"内存管理的主要目的是合理分配内存，减少内存碎片，及时回收资源，提高内存使用效率。（任何一个组件管理内存的目的都是这个）。 从Netty层面来说，操作系统分配内存容易有碎页并且比较耗时，一次性申请足够空间，自己管理更高效。Netty内存管理其实质就是先分配一块大内存，然后在内存的分配和回收过程中，使用一些数据结构记录内存使用状态，如果有新的分配请求，根据这些状态信息寻找最合适的位置并更新内存结构。释放内存时候：同步修改数据结构. Netty 是怎么做内存管理–内存池 Netty is a NIO client server framework which enables quick and easy development of network applications such as protocol servers and clients. It greatly simplifies and streamlines network programming such as TCP and UDP socket server. 个人以为Netty之所以优秀的关键词可以概括为以下两点： 方便：良好的封装与接口设计，对多种通信协议的支持，使得上手简单，快速 高效（性能）：异步的、事件驱动的线程模型和高效的内存管理机制 作为服务端的开发人员，有必要深入学习一下其内部原理。话不多说，先从内存池开始。 Netty内存池的实现参考了jemalloc的原理，关于jemalloc的介绍可以参考:jemalloc或者自行google。 宏观上来说，Netty对内存的管理分为两个层面。在为线程分配内存的过程中，会首先查找线程私有缓存（默认为线程开启缓存，可配置关闭），当私有缓存不满足需求时，会在内存池中查找合适的内存空间，提供给线程。： 线程私有缓存 Cache 线程私有缓存因为可以避免多线程请求内存时的竞争，所以效率很高，但是也存在一些缺陷：最大缓存容量小，每个线程默认32k；使用不当可能会造成内存泄漏. 全局的内存 Arena 全局共享的内存池支持堆内存和堆外内存（Direct Buffer）的申请和回收，其内存管理的粒度有以下几个单位： Chunk：Netty向操作系统申请内存是以Chunk为单位申请的，内存分配也是基于Chunk。Chunk是Page为单元的集合，默认16M。 Page: 内存管理的最小单元，默认8K SubPage: Page内内存分配单元。 Netty逻辑上将内存大小分为了tiny, small, normal, huge 几个单位。申请内存大于Chunk size 为huge，此时不在内存池中管理，由JVM负责处理；当Client申请的内存大于一个Page的大小（normal）, 在Chunk内进行分配; 对tiny&amp;small大小的内存，在一个Page页内进行分配。针对上述几种粒度的内存块的管理，其实现上包含以下几个组件（类）： PoolArena：内存分配中心 PoolChunk：负责Chunk内的内存分配 PoolSubpage：负责Page内的内存分配 用一幅图概括内存池的布局与管理如下： 按照由繁到简的顺序（个人观点），上述组件的实现原理剖析可以依次参见： Netty 是怎么做内存管理-PoolArena Netty 是怎么做内存管理--PoolChunk Netty 是怎么做内存管理--PoolSubPage 最后，对线程私有缓存的管理解析及内存释放的相关内容，请阅读Netty内存管理-线程私有缓存和Netty内存管理-内存释放。","link":"/2017/09/03/netty-memory-pool-md/"},{"title":"一条order by排序语句执行过程","text":"MySQL做排序是一个成本比较高的操作。本文整理了MySQL是怎么做order by排序的。假设一个表:123456789CREATE TABLE `t` ( `id` int(11) NOT NULL, `city` varchar(16) NOT NULL, `name` varchar(16) NOT NULL, `age` int(11) NOT NULL, `addr` varchar(128) DEFAULT NULL, PRIMARY KEY (`id`), KEY `city` (`city`)) ENGINE=InnoDB; 想要查询城市是“杭州”的所有人名字，并且按照姓名排序返回前1000个人的姓名、年龄。业务语句:1select city,name,age from t where city='杭州' order by name limit 1000 ; 无序数据MySQL会给每个线程分配一块内存用于排序，称为sort_buffer。city 索引的示意图： 查询字段不多 - 全字段排序语句执行流程： 初始化sort_buffer,确定放入name, city, age 这三个字段； 从索引city找到第一个满足city = “杭州”条件的主键id, 也就是图中ID_X; 到主键id索引去取出整行，取name, city, age三个字段的值，存入sort_buffer中； 从索引city取下一个记录的主键id; 重复3，4直到city的值不满足查询条件为止，对应的主键id也就是途中ID_Y； 对sort_buffer中的数据按照字段name做快速排序； 按照排序结果取前1000行返回给客户端。 按name排序可以在内存中完成，也可能需要使用外部排序，这取决于排序所需要的内存和参数sort_buffer_size。sort_buffer_size即为MySQL为排序开辟的内存sort buffer的大小。 如果要排序的数据小于sort_buffer_size, 排序就在内存中完成； 如果排序数据量太大，内存放不下，就得利用磁盘的临时文件进行辅助外部排序。外部排序一般使用归并排序算法。MySQL将需要排序的文件分成X份，每一份单独排序后存在这些临时文件中。然后把这X个有序文件再合并成一个有序的大文件。另外MySQL 5.6以后进行了优化，对于limit n, n&lt;sort_buffer_size的排序会使用优先队列排序（堆排序），不需要临时文件。 查询字段多 - row id 排序如果查询要返回的字段很多的话，那么sort_buffer里面要放的字段太多，这样内存里能够同时放下的行数很少。如果要分成很多个临时文件，排序的性能会很差。 当查询字段多，MySQL为了避免分成多个临时文件排序性能差，会放弃使用全字段排序，选择row id排序：语句执行流程： 初始化sort_buffer, 确定放入两个字段，即name和id; 从索引city找到第一个满足city=“杭州”条件的主键id, 也就是图中的ID_X; 到主键id索引取整行，取name, id 这两个字段，存入sort_buffer中； 从索引city取下一个记录的主键id; 重复3，4直到city的值不满足查询条件为止，对应的主键id也就是途中ID_Y； 对sort_buffer中的数据按照字段name做快速排序； 遍历排序结果，取前1000行，并按照id值回表中取出city、name和age三个字段返回给客户端。 对比全字段排序，row id排序多了回表。这种排序方式只有MySQL查询字段多导致内存不足才会使用。 有序数据排序 - 并不需要成本高地去排序如果增加覆盖索引1alter table t add index city_user(city, name, age); 执行流程： 从索引（city, name, age）找到第一个满足city=’杭州’条件的记录，取出其中city, name和age这三个字段的值，作为结果集的一部分直接返回； 从索引 (city,name,age) 取下一个记录，同样取出这三个字段的值，作为结果集的一部分直接返回； 重复执行步骤 2，直到查到第 1000 条记录，或者是不满足 city=’杭州’条件时循环结束。 可以看出extra字段没有using filesort–不需要排序。也使用到了覆盖索引，性能会快很多。 深分页问题解决方法随机排序 从一个单词表随机选出三个单词。 12345mysql&gt; CREATE TABLE `words` ( `id` int(11) NOT NULL AUTO_INCREMENT, `word` varchar(64) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB; order by rand()1mysql&gt; select word from words order by rand() limit 3; tmp_table_size这个配置限制了内存临时表的大小，默认值是16M。如果临时表大小超过了tmp_table_size, 那么内存临时表就会转成磁盘临时表。 使用内存临时表内存临时表排序使用rowid排序方法。 使用磁盘临时表当使用磁盘临时表的时候，对应的就是一个没有显示索引的InnoDB表的排序过程。 无论使用哪种类型的临时表，order by rand()这种写法都会让计算过程非常复杂 - 需要大量的扫描行数，整个表排序、 一种更好的方案 取得这个表的主键id的最大值M和最小值N； 用随机函数生成一个最大值和最小值之间的数 X = （M-N）* rand() + N; 取不小于X的第一个ID的行 参考极客时间","link":"/2019/02/09/order-by-mysql/"},{"title":"徒步奶子沟","text":"是的，名字很..awkward。不过重点是徒步了40km，过程很酸爽，过后可以吹牛。 时间： 2017年5月 地点： 四川","link":"/2017/05/28/post/"},{"title":"Netty 是怎么做内存管理--PoolSubPage","text":"当Netty分配内存大小小于page时候，Netty提供PoolSubpage把chunk的一个page节点8k内存划分成更小的内存段，通过对每个内存段的标记与清理标记进行内存的分配和释放。 初始化PoolSubPage在页内进行内存分配，用位图记录内存分配的情况，位图标记为0表示未分配，标记为1表示已分配。123456789101112PoolSubpage(PoolSubpage&lt;T&gt; head, PoolChunk&lt;T&gt; chunk, int memoryMapIdx, int runOffset, int pageSize, elemSize) { this.chunk = chunk; this.memoryMapIdx = memoryMapIdx; this.runOffset = runOffset; this.pageSize = pageSize; bitmap = new long[pageSize &gt;&gt;&gt; 10]; // pageSize / 16 / 64 init(head, elemSize);} 可以发现，以默认的PageSize=8192byte为例，位图bitmap的大小被初始化为8:1long bitmap[] = new long[pageSize &gt;&gt;&gt; 10] 简单说明一下，在Page中subpage以16字节为最小单位划分内存段，而一个long类型的变量有64位，所以最多使用PageSize/16/64=8个long型的变量就可以表示所有内存段。 假设我们以elemSize=72字节为单位，在页内进行内存段的划分，那最多将有maxNumElem=PageSize/elemSize=113个内存段。（elemSize一旦确定就不会改变， 页面中内存段都是大小一致的）那么这113个内存段就要占用位图中113个位置，那需要多少个bitmap元素呢？ 1234bitmapLength = maxNumElems &gt;&gt;&gt; 6;if ((maxNumElems &amp; 63) != 0) { bitmapLength ++;} 计算也比较简单，除64取整，如果存在不能整除的部分，结果再加1。当maxNumElems=113时，就需要2个数组元素来描述内存段的状态。假如内存段更大到elemSize=4096，maxNumElems只有2时，就需要1个数据元素就可以描述着两个内存段。整个计算过程都基于位操作实现，效率更高。 分配流程PoolSubPage分配内存段的过程就是在位图中找到第一个未被使用的内存段，返回一个描述其内存位置偏移量的整数句柄，用于定位。 代码如下：1234567891011private int findNextAvail() { final long[] bitmap = this.bitmap; final int bitmapLength = this.bitmapLength; for (int i = 0; i &lt; bitmapLength; i ++) { long bits = bitmap[i]; if (~bits != 0) { //当前数组元素上有未分配的内存(marked as zero) return findNextAvail0(i, bits); } } return -1;} 12345678910111213141516171819202122/** * i ：空闲内存在位图数组中的下标 * bits : 数组元素表示的位图详情 * return ：位图索引 */private int findNextAvail0(int i, long bits) { final int maxNumElems = this.maxNumElems; final int baseVal = i &lt;&lt; 6; //高位用来记录分配的内存在位图数组中的下标位置 for (int j = 0; j &lt; 64; j ++) { if ((bits &amp; 1) == 0) { //当前位置表示的内存未分配 int val = baseVal | j; //低6位用来记录空闲内存在long型元素二进制表示中占据的位置 if (val &lt; maxNumElems) { return val; } else { break; } } bits &gt;&gt;&gt;= 1; //右移，尝试下一位 } return -1;} 算法首先在位图数组bitmap中开始遍历，如果当前数组元素表示的内存空间上有空闲内存段(即数组元素的二进制位上有0)，则进一步在此数组元素中查找空闲内存段在二进制位上的位置。通过在二进制位上循环移位遍历，访问到0则构造内存偏移量并返回。整形的内存偏移量，低6位用来表示空闲内存在long型元素的二进制位表示中占据的位置，高位用来记录该数组元素的下标。 以下图的bitmap为例，算法首先在bitmap[0]上没有发现空闲内存，则进一步访问bitmap[1]。为了找到空闲内存在bitmap[1]中的位置，依次遍历，最终在位置2（j=2）上 找到目标内存。构建位图索引，baseVal = 1 &lt;&lt; 6, val = baseVal | j = 01000010。 1234567891011final int bitmapIdx = getNextAvail();int q = bitmapIdx &gt;&gt;&gt; 6; //取数组元素的位置，即上述的baseValint r = bitmapIdx &amp; 63; //相当于模64，计算得到上述算法流程中的变量jassert (bitmap[q] &gt;&gt;&gt; r &amp; 1) == 0;bitmap[q] |= 1L &lt;&lt; r; //位图中相应的位置置1if (-- numAvail == 0) { removeFromPool();}return toHandle(bitmapIdx); 123private long toHandle(int bitmapIdx) { return 0x4000000000000000L | (long) bitmapIdx &lt;&lt; 32 | memoryMapIdx;} 当然，分配完成之后需要将位图中的位置置1，防止被再次分配。详细的过程已在代码中做了注释，不再详述。最终返回给Client的偏移量句柄，还需要做一次变化（toHandle），其结构也比较明显，句柄共占据long型的低48位，其中低32位记录当前内存页在PoolChunk的平衡二叉树中的节点编号，中间16位（低6位记录在位图long型元素的二进制位置，低3位记录在位图数组中的位置）。 上层调用在 Netty 是怎么做内存管理-PoolChunk 一文中提到过，当用户请求的内存空间小于一个页面的内存大小时，会调用allocateSubpage在页面内进行内存分配。 看一下allocateSubpage的实现：1234567891011121314151617181920212223242526272829303132333435363738/** * Create/ initialize a new PoolSubpage of normCapacity * Any PoolSubpage created/ initialized here is added to subpage pool in the PoolArena that owns this PoolChunk * * @param normCapacity normalized capacity * @return index in memoryMap */private long allocateSubpage(int normCapacity) { // Obtain the head of the PoolSubPage pool that is owned by the PoolArena and synchronize on it. // This is need as we may add it back and so alter the linked-list structure. PoolSubpage&lt;T&gt; head = arena.findSubpagePoolHead(normCapacity); synchronized (head) { int d = maxOrder; // subpages are only be allocated from pages i.e., leaves int id = allocateNode(d); if (id &lt; 0) { return id; } final PoolSubpage&lt;T&gt;[] subpages = this.subpages; final int pageSize = this.pageSize; freeBytes -= pageSize; int subpageIdx = subpageIdx(id); PoolSubpage&lt;T&gt; subpage = subpages[subpageIdx]; if (subpage == null) { subpage = new PoolSubpage&lt;T&gt;(head, this, id, runOffset(id), pageSize, normCapacity); subpages[subpageIdx] = subpage; } else { subpage.init(head, normCapacity); } return subpage.allocate(); }}private int subpageIdx(int memoryMapIdx) { return memoryMapIdx ^ maxSubpageAllocs; // remove highest set bit, to get offset} 首先根据在arena中找到normCapacity大小的内存空间应该在arena维持的PoolSubpage列表中的那一个节点上分配 (参见 Netty 是怎么做内存管理-PoolArena对内存结构的分析)，然后以d = maxorder, 在PoolChunk的完全二叉树中，寻找一个空闲的叶子节点，用于此次的内存分配。 在创建PoolChunk的是否会默认创建一个PoolSubpage的数组subpages=new PoolSubpage[1 &lt;&lt; maxorder], 用来记录叶子节点被用作PoolSubpage的分配情况。在PoolChunk找到一个空闲的叶子节点时，首先调用subpageIdx，计算该叶子节点在PoolChunk完全二叉树最底层的相对位置。(完全二叉树最底层的第一页叶子节点编号为2maxorder, 所以任意叶子节点相对首个叶子节点的相对位置，可以通过上述代码中的异或运算，把高位的0抹掉，只保留低位的值即为相对位置) 如果subpages当前位置没有记录，则分配生产一个新的PoolSubpage对象，否则直接初始化当前PoolSubpage对象，并插入head的后。 最后调用allocate()就是执行前文所描述的页面内分配内存的执行流程。","link":"/2017/08/27/poolsubpage/"},{"title":"工程中遇到的排序算法-基础篇","text":"总结下经典的排序算法和工程中遇到的排序。 1. 基于比较的排序算法时间复杂度$O(n^2)$ 的 排序算法冒泡排序: 时间复杂度$O(n^2)$，空间复杂度：原地排序，稳定的排序算法。 快速排序: 时间复杂度$O(n^2)$，空间复杂度：原地排序，稳定的排序算法。 选择排序: 时间复杂度$O(n^2)$，空间复杂度：原地排序，不稳定的排序算法。 上述三种排序算法因为时间复杂度比较高，$O(n^2)$。因此只适合小规模数据的排序。 时间复杂度$O(nlogn)$的排序算法对于大规模数据进行排序，应该选择时间复杂度$O(nlogn)$的排序算法。 归并排序: 时间复杂度$O(nlogn)$，空间复杂度On, 稳定的排序算法。算法思想用到了分治思想。 缺点：实战中使用归并并不多，这是因为归并不是原地排序算法。粗略举例来说，100MB数据，除了数据本身占用的内存之外，排序算法那还需要额外占用100MB的内存，空间耗费翻倍。 快速排序: 时间复杂度 $O(nlogn)$，原地排序，不稳定的排序算法。 谈谈快排的优化，快排可能导致性能差的几点： 快排性能脆弱点1：分区哨兵点选择，哨兵点选择不对可能时间复杂度退化为$O(n^2)$。 快速排序性能关键点在于 pivot分区哨兵点选择。如果数据原来就是有序，每次pivot分区哨兵点都选择最后一个数据，那么快排时间复杂度退化为$O(n^2)$。 最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多。常用的分区点选择算法： 三数取中法。从区间的首、尾、中间分别取一个数，然后对比大小，取这三个数的中间值作为分区点。 随机法。随机选择一个元素作为分区点，从概率的角度来看这种要好一些。 快排性能脆弱点2：快排依赖了递归，警惕堆栈溢出。解决办法： 限制递归深度，一旦超过设定的阈值，就停止递归； 在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有了系统栈大小的现在。 堆排序 2. 非基于比较排序时间复杂度$O(n)$桶排序计数排序基数排序","link":"/2020/01/05/sort-base/"},{"title":"工程中遇到的排序算法-实践篇","text":"总结下经典的排序算法和工程中遇到的排序。 开发语言的基础排序使用场景之C语言的qsort()C语言的qsort() : 当数据小的时候使用归并排序； 当数据大的时候使用快速排序。并且选择分区点的方法就是三数取中法。qsort()自己实现了一个堆，手动模拟递归，避免递归太深导致的堆栈溢出。 并且当要排序的区间元素个数&lt;=4的时候，qsort()就退化为插入排序，不再使用递归做快排。(对于小规模数据的排序，O(n2) 的排序算法并不一定比 $O(nlogn)$排序算法执行的时间长。对于小数据量的排序，就选择比较简单、不需要递归的插入排序算法。) 插入排序使用了哨兵做优化，少了一次判断。 使用场景之Java的Arrays.sort()Java的Arrays.sort()主要使用了双轴快速排序（by Vladimir Yaroslavskiy, Jon Bentley, and Joshua Bloch)，算法时间复杂度$O(nlogn)$。算法性能强于单哨兵的快排。但是并非单纯使用了一种算法就解决了所有排序问题，而是根据具体数组的情况采用合适的算法实现。 算法原理： 待排序元素length &lt; 47， 使用插入排序； 待排序元素length &lt; 286，使用双轴快速排序； 待排序元素length &gt; 286, 检测数组的连续升序和连续降序性好不好，如果好的话就选择TimSort算法（（一种改进的归并排序算法），不好的话使用双轴快速排序。 可以看出整体思路与c语言的qsort()大致相同。 MySQL的排序一句话概括： 当内存放得下待排序数据（待排序数据小于sort_buffer_size时候），使用快速排序； 当内存放不下，就使用归并排序算法外部排序临时文件。 如果是Top-K场景并且没有超过K个数据大小没超过sort_buffer_size还会使用优先队列排序算法，利用堆来线性时间取Top-K优化性能, 并且不需要临时文件。 具体还会根据select 的字段长度，避免内存装不下，分为了全字段排序和row id排序。rowid排序可以一次排序更多行，但是要回表取数据。 MySQL做排序是一个成本比较高的操作，涉及到内存是否可以放下待排序数据，回表等问题。因此，更多时候可以利用覆盖索引技巧避免执行排序。 Redis的ZSET排序借助了跳表的数据结构实现了排序和$O(logn)$查找，跳表维持数据的动态插入和删除时间复杂度也是$O(logn)$。 之所以选择跳表这个数据结构。是因为Redis ZSET核心操作主要有下面这几个： 插入/删除/查找一个数据 按照区间查找数据。 对于按照区间查找数据这个操作，跳表可以做到$O(logn)$的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了。这样做相比于红黑树会非常高效。另外，Redis之所以使用跳表实现有序结合，还有其他原因，比如跳表容易实现，更加灵活。 高性能定时器定时器需要判断每个任务是否到达任务执行时间，如果每过1s就扫描一遍任务列表的做法比较低效。 任务的约定执行时间距离当前时间可能还有很久，前面很多次扫描都是徒劳的； 每次都要扫描整个任务列表，如果任务列表很大的话，势必会比较耗时。 Java的定时任务@Scheduled 使用堆组织任务，不必每秒都轮询一次。用优先级队列来解决。按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部（小顶堆的堆顶）存储的是最先执行的任务。这样，定时起就不需要每隔1s就扫描一次任务列表里，它拿任务队首任务的执行时间点，与当前时间点相减，得到时间间隔T。 这个时间间隔T 就是从当前时间开始，需要等待多久，才会有第一个任务需要被执行。这样定时器就可以设定在Ts 之后，再来执行任务。从当前时间点到（T-1）秒这段时间里，定时器都不需要做任何事情。当 T 秒时间过去之后，定时器取优先级队列中队首的任务执行。然后再计算新的队首任务的执行时间点与当前时间点的差值，把这个值作为定时器执行下一个任务需要等待的时间。这样，定时器既不用间隔 1 秒就轮询一次，也不用遍历整个任务列表，性能也就提高了。 接口延时99分位使用堆排序还可以解决求接口延时99分位值。 99分位概念：如果有 100 个接口访问请求，每个接口请求的响应时间都不同，比如 55 毫秒、100 毫秒、23 毫秒等，我们把这 100 个接口的响应时间按照从小到大排列，排在第 99 的那个数据就是 99% 响应时间，也叫 99 百分位响应时间。总结一下，如果有 n 个数据，将数据从小到大排列之后，99 百分位数大约就是第 n99% 个数据，同类，80 百分位数大约就是第 n80% 个数据。 求99分位延时：维护两个堆，一个大顶堆，一个小顶堆。假设当前总数据个数是n, 大顶堆中保存n 99%个数据，小顶堆中保存n1%个数据。 大顶堆堆顶就是要找的99%响应时间。每次插入一个数据的时候，我们要判断这个数据跟大顶堆和小顶堆堆顶数据的大小关系，然后决定插入到哪个堆中。如果这个新插入的数据比大顶堆的堆顶数据小，那就插入大顶堆；如果这个新插入的数据比小顶堆的堆顶数据大，那就插入小顶堆。 但是，为了保持大顶堆中的数据占 99%，小顶堆中的数据占 1%，在每次新插入数据之后，我们都要重新计算，这个时候大顶堆和小顶堆中的数据个数，是否还符合 99:1 这个比例。如果不符合，我们就将一个堆中的数据移动到另一个堆，直到满足这个比例。移动的方法类似前面求中位数的方法。 通过这样的方法，每次插入数据，可能会涉及几个数据的堆化操作，所以时间复杂度是 O(logn)。每次求 99% 响应时间的时候，直接返回大顶堆中的堆顶数据即可，时间复杂度是 O(1)。","link":"/2019/12/02/sort/"},{"title":"西班牙 - 下","text":"行程的后半段从美丽的科尔多瓦开始。 科尔多瓦Day7 科尔多瓦。如果说塞维利亚像成都的话，科尔多瓦这个小城应该像大理。这里阳光充沛，家家有花。图2就是我们住的airbnb民宿，整个院子被花环绕。据说五月是科尔多瓦的庭院节，那时到处盛开着五彩缤纷的鲜花。不禁羡慕这里的居民，如此明媚的阳光，天然适合侍弄花草。另外，这里还是一个跨文化融合的地方，罗马，穆斯林，犹太文化都在这里长期灿烂过，因此清晨逛大清真寺和市中心的罗马神庙有一种重回历史书的穿越感。对了，这里还有一座古罗马桥始建于1世纪，是权游瓦兰提斯长桥的取景地。 塞哥维亚Day8 塞哥维亚是马德里旁边的一个山上的小镇，虽然只与马德里相距半小时高铁车程，却寒冷许多。下了通往市区的公交车，首先映入眼帘的就是两千年前的古罗马水槽，震撼极了。扶着水槽墙壁行走，仿佛亲手触摸两千年的历史。离开古罗马水槽往城中央走，会看见被称为贵妇的欧洲最后一个哥特教堂，中午有乐队在教堂前面的广场演出，大家举着酒杯高喊bravo。这里最著名的美食要属烤乳猪了配红酒，我会觉得乳猪有一点点腻，完全交给男朋友享用，他觉得外焦里嫩，香嫩可口。anyway，慢悠悠的听着音乐，吃着欧式dish,真舒服呀。 马德里Day9 马德里并没有不思议。倒是丽池公园这满地板栗哟……这里的市民为啥不捡走呢，板栗鸡，烤板栗，炒板栗，水煮板栗…… 时间： 2018年10月 地点： 西班牙","link":"/2018/10/09/spain2/"},{"title":"西班牙 - 上","text":"一想到能在南欧的海边吹着来自地中海的暖风，徜徉在中世纪的教堂里听着教士们唱经，在古罗马的城墙想象十字军东征，再亲眼看一场巴萨的球赛。这种感觉真是奇妙极了。 巴塞罗那北京出发，阿姆斯特丹机场转机到巴塞罗那。荷兰皇家航空的飞机三明治真美味。 巴塞罗那首日，机场大巴到了马德里广场附近。下了车定定神，就听见此起彼伏的喧嚣声，炮声，还有头顶喧嚣的直升机声音。定睛一看，周围还有荷枪实弹的警察，十字路口还设置了路障！第一反应是遇到了枪战？但是周边人还在从容的行走。还没摸清楚情况的两人瞬间懵懂了。跟着人流一看，竟然是碰到了加泰罗尼亚独立游行。放松地叹了一口气，资本主义玩的真大。 午餐选择的是Tapas。一碟菜配块面包就是西班牙有名的Tapas, 无论是专门吃Tapas的餐馆，还是路边小饭馆都提供着样式丰富的Tapas。 要是再配上一杯冰啤酒，再美味不过了。 巴塞罗那的三个灵魂，诺坎普，高迪还有教堂。虽然不是足球热血粉丝，去诺坎普现场感受下氛围，亲眼欣赏梅西的球技还是不错的。闻名不如一见，果然巴塞罗那主场优势燃爆，地铁坐到最后车厢里满满都是穿着巴萨球衣的球迷。每当巴萨有精彩配合所有人都会起身鼓掌，对方守门员持球太久就会立刻遭到嘘声一片。铁粉们全场打鼓唱歌，梅西出场后更是全场高呼Messi。不愧是巴萨！ Day3-Day4 开始了巴塞罗那的第二个灵魂 - 高迪建筑之旅。 乱入一个加泰罗尼亚艺术博物馆，建在山上的它虽然不是高迪的手笔，但是傍晚在这里享受着夕阳，俯瞰巴塞罗那全景，真是惬意极了。 赫罗纳Day 2 来到了赫罗纳（Girona）。这个小镇距离巴塞罗那高铁一个多小时车程，是一个很安静充满中世纪历史感的小镇，不过它更吸引我的地方是它是权力的游戏的取景地。提前在印象笔记中准备了好多取景地打卡，激动的一路拍照对比。 路遇一家米其林三星的甜品店，这个雪糕别提多黑暗料理了… 塞维利亚Day5 - Day6 塞维利亚。类比于中国的城市，我认为这里像是成都。天气炎热，人民热情似火，美食美女多多。这里有Tapas，弗拉明戈和历史悠久的主教堂！城市并不大，却有无数如迷宫般狭窄的小巷，这来自于古时候犹太人的设计，可以遮挡炎炎烈日，实用性与趣味性兼具。记忆深刻的是在记忆房子观看弗拉明戈表演，歌者音喉辽远，舞蹈充满了生命力与张力，吉他表演更加刷新了我对吉他演奏的认知。两天虽然浮光掠影，但是却让我像热爱程度一样喜欢上了这个城市。 时间： 2018年10月 地点： 西班牙","link":"/2018/10/09/spain/"},{"title":"Taiwan - 台湾环岛","text":"「台北-花莲-绿岛-垦丁-台南-台中-台北」 题图为台北车站。 环岛台湾，从成都搭乘飞机而来，一路上坐过台湾干净的台铁和高铁，特意品尝了具有几十年传统的台铁便当；也体验过相当于大陆地铁的台北捷运，办了一张很萌很台湾的悠游卡，假装当地人挤进行色匆匆的捷运人流。在台湾东部还乘坐了要等半个小时之久的公共巴士，等公交的时候“聆听”当地老伯讲政治……在路上，通过不同的搭乘方式，感受台湾的方方面面。 Airbnb上预定的南港住宿，房东非常热情周到。她说经常来广州，会因为台湾口音被认出是台湾人。“我的口音有那么明显么？” 成群的机车党 中正纪念堂，慕名来看每隔一个小时都有的卫兵换班仪式。纪念堂大广场里特别热闹，有一群练街舞的学生，领着孩子玩耍的家长，还有各种文艺宣传活动。 巴洛克式建筑-总统府。101大楼，声名在外，我却认为远没有总统府和中正纪念堂的建筑美好。 站在猫空上眺望台北远景，101大楼直入天际，映入眼帘。 台北的车流不息 肥前屋的秋刀鱼，据说是老板是日本人，能做出特别地道的鳗鱼饭。的确非常美味，不枉我11点就来排队。 地铁旁随便就可以买到便宜，新鲜且美味的寿司 花莲的清水断崖美如画。 太鲁阁的美景宛如唐宋时期的山水画。假装在问道。 七星潭边作画的画家 绿岛是台湾的离岛，只能选乘船和飞机两种交通方式。飞机不好预定且价格高昂，我们选择的乘船。乘船对于晕一切机动车的我来说绝对不是一个美好的体验…好在绿岛的景色美极，也值得了。 到垦丁这一天刚下过雨，天气反而舒爽，骑着机车慕名去后壁湖吃生鱼片。 鹅銮鼻公园，已经接近台湾最南点了 夜宿海生馆，在海豚馆下面打地铺，和海豚一起入睡，别样的旅行住宿体验。 李安的故乡–台南 上过台湾国宴担仔面，虽然我更喜欢吃担担面 孔庙旁小店的美食 台中-东海大学，贝聿铭的设计 时间： 2016年4月 地点： 台湾","link":"/2016/06/14/taiwan/"},{"title":"关于MySQL的临时表和临时文件","text":"本文为学习摘抄。 临时表临时表可以分为磁盘临时表和内存临时表，而临时文件只会存在于磁盘上，不会在内存上。临时表当tmp_table_size小于一定值时候都是内存表 – 临时表的内存形态有Memory 引擎和Temptable引擎，主要区别是对字符类型(varchar, blob，text类型)的存储方式，前者不管实际字符多少，都是用定长的空间存储，后者会用变长的空间存储，这样提高了内存中的存储效率，有更多的数据可以放在内存中处理而不是转换成磁盘临时表。Memory引擎从早期的5.6就可以使用，Temptable是8.0引入的新的引擎。 当超过tmp_table_size – 内存临时表就转成磁盘临时表，磁盘临时表有三种形态。一种是MyISAM表，一种是InnoDB临时表，另外一种是Temptable的文件map表。其中最后一种方式，是8.0提供的。 目前有两种情况会用到临时表： 用户显示创建临时表，这种直接创建磁盘文件。 1create temporary table 优化器隐式创建临时表这种临时表，是数据库为了辅助某些复杂的SQL的执行而创建的辅助表，是否需要临时表，一般都是由优化器决定。与用户显式创建的临时表直接创建磁盘文件不同，如果需要优化器觉得SQL需要临时表辅助，会先使用内存临时表，如果超过配置的内存(min(tmp_table_size, max_heap_table_siz))，就会转化成磁盘临时表，这种磁盘临时表就类似用户显式创建的，引擎类型我们是InnoDB。一般稍微复杂一点的查询，包括且不限于order by, group by, distinct等，都会用到这种隐式创建的临时表。用户可以通过explain命令，在Extra列中，看是否有Using temporary这样的字样，如果有，就肯定要用临时表。 临时文件临时文件更多的呗使用在缓存数据，排序数据的场景。一般情况下，被缓存或者排序的数据，首先放在内存，如果内存不足（超过sort_buffer_size），才会使用磁盘临时文件的方式。 另外explain是看不出是否使用了临时文件，只能通过查看 OPTIMIZER_TRACE 的结果来确认的，你可以从 number_of_tmp_files 中看到是否使用了临时文件。 参考taobao数据库内核月报","link":"/2019/12/03/temp-file-table/"},{"title":"MySQL 临时表","text":"关于MySQL临时表和临时文件。 概述MySQL每个线程都维护了自己的临时表链表。这样每次session内操作表的时候，先遍历链表，检查是否有该名字的临时表，如果有则优先操作临时表，如果没有再操作普通表。临时表有两种，用户自己创建的-用户临时表以及系统创建的，内部临时表。 临时表特点： 一个临时表只能被创建它的session访问，对其他线程不可见; 临时表可以与普通表同名; session A内有同名的临时表和普通表的时候，show create 语句，以及增删改查语句访问的是临时表; show tables命令不显示临时表; session结束的时候回自动删除临时表; 用户临时表由于不用担心线程之间的重名冲突，临时表经常会被用在复杂查询的优化过程中。 1.优化使用场景-分库分表分区key的选择依据：减少跨库和跨表查询。例如，如果大部分语句都会包含f的等值条件，那么就用f做分区键。这样在proxy这一层解析完SQL语句之后，就嗯嗯更确定这条语句路由到哪个分表做查询。1select v from ht where f=N; 这时，可以通过分表规则，比如N%1024来确认需要的数据被放到哪个分表上了。1select v from ht where k&gt;=M order by t_modified desc limit 100; 这时候，由于查询条件里面没有用到分区字段f，只能到所有分区中去查询满足条件的所有行，然后统一做order by操作。操作思路：把各个分库拿到的数据，汇总到一个MySQL实例中的一个表中，然后在这个汇总实例上做逻辑操作。 在汇总库创建一个临时表 temp_ht, 表里面包含三个字段 v,k,t_modified; 在各个分库上执行； 1select v,k,t_modified from ht_x where k&gt;=M order by t_modified desc limit 100; 把分库执行的结果插入到temp_ht表中； 执行1select v from temp_ht order by t_modified desc limit 100; 内部临时表内部临时表是系统在执行过程中创建的，先用内存临时表，如果内存不够用了，就使用磁盘临时表。tmp_table_size这个配置限制了内存临时表的大小。默认值16M。如果临时表大小超过了tmp_table_size,那么内存临时表就会转成磁盘临时表。当使用磁盘临时表的是，对应的就是一个没有显式索引的InnoDB的排序过程。 MySQL什么时候会使用 如果语句执行过程可以一边读数据，一边直接得到结果，是不需要额外内存的，否则就需要额外的内存，来暂存中间结果； join_buffer是无序数组，sort_buffer是有序数组，临时表是二位表结构；如果执行逻辑需要二维表特性，就会优先考虑使用临时表。例如union需要用唯一索引约束，group by还需要用到另外一个字段来存累积计数。 1. Union 执行过程内存临时表起到了暂存数据作用，计算过程用上了临时表主键id的唯一性约束，实现了Union语义。 2. Group by例如：1select id%10 as m, count(*) as c from t1 group by m; 1.创建内存临时表，表里有两个字段 m 和 c，主键是 m；2.扫描表 t1 的索引 a，依次取出叶子节点上的 id 值，计算 id%10 的结果，记为 x；2.1如果临时表中没有主键为 x 的行，就插入一个记录 (x,1);2.2如果表中有主键为 x 的行，就将 x 这一行的 c 值加 1；3.遍历完成后，再根据m排序，将得到的结果集返回给客户端。group by的弊端是：如果数据量很大，group by 又用到了临时表，很可能临时内存表不够用，要使用到磁盘临时表，这样性能就很差了。 Group by 优化方法-索引之所以group by 需要临时表，是因为数据无序（id%10）需要暂存。如果数据都有序，依次+1, 就不需要临时表了。因此解决方法是可以给m%10增加个列，并添加该列索引，保证数据有序。或者：MySQL 5.7的generated column机制，用来实现列数据的关联更新。1alter table t1 add column z int generated always as(id % 100), add index(z); group by优化方法 – 直接排序如果不适合创建索引，就只能直接用磁盘临时表存储优化，避免先存储到内存临时表再转到磁盘临时表。在group by语句中加入SQL_BIG_RESULT提示，告诉优化器直接使用磁盘临时表。1select SQL_BIG_RESULT id%100 as m, count(*) as c from t1 group by m; MySQL优化器判断，磁盘临时存储是B+树存储，存储效率不如数组高。因此在数据量大的时候，磁盘临时存储使用数组存。因此执行流程： 初始化sort_buffer， 确定放入一个整形字段，记为m; 扫描表 t1 的索引 a，依次取出里面的 id 值, 将 id%100 的值存入 sort_buffer 中； 扫描完成后，对 sort_buffer 的字段 m 做排序（如果 sort_buffer 内存不够用，就会利用磁盘临时文件辅助排序）； 排序完成后，就得到了一个有序数组。 根据有序数组，得到数组里面的不同值，以及每个值的出现次数。 3. insert select1insert into t(c,d) (select c+1, d from t force index(c) order by c desc limit 1); 执行过程：1、创建临时表，表里有两个字段c,d2、按照索引c扫描t, 依次取c=4,3,2,1，然后回表，读到c和d的值写入临时表。这时row_examined=4;3、由于语义里面有 limit 1，所以只取了临时表的第一行，再插入到表 t 中。这时，Rows_examined 的值加 1，变成了 5。 这个语句不好的地方是：这个语句会导致在表 t 上做全表扫描，并且会给索引 c 上的所有间隙都加上共享的 next-key lock。所以，这个语句执行期间，其他事务不能在这个表上插入数据。 使用临时表的原因是这类一边遍历数据，一边更新数据的情况，如果读出来的数据直接写回原表，就有可能在遍历过程中，读到刚刚插入的记录。新插入的记录如果参与计算逻辑，就跟语义不符。 优化办法：先 insert into 到临时表 temp_t，这样就只需要扫描一行；然后再从表 temp_t 里面取出这行数据插入表 t1。1234create temporary table temp_t(c int,d int) engine=memory;insert into temp_t (select c+1, d from t force index(c) order by c desc limit 1);insert into t select * from temp_t;drop table temp_t;","link":"/2019/02/22/temp-table/"},{"title":"Spring项目加载慢问题","text":"最近遇到一个诡异的问题，项目启动异常缓慢(每次重启tomcat都要6-7分钟)，这对于开发来说简直是灾难式体验，所以决心修正此问题。 首先google搜索解决tomcat启动慢的问题，按照大多数人反映的计算熵SecureRandom的方法修改，发现无效。于是只好阅读tomcat启动日志，发现每次项目启动，时间都卡在这里org.apache.catalina.core.ApplicationContext.log Initializing Spring root WebApplicationContext。这时候搜索结果就与计算熵SecureRandom无关了，而是spring启动的问题，摘取排查问题步骤如下，果然解决。 检查spring是否运行在debug模式下，是跳转到2 否则跳转到3 查看spring在run模式下是否运行依旧缓慢 是跳转到3，否则跳转到4 检验是否spring bean加载了多次（quartz加载很有可能导致部分bean被是实例两次） 是跳转到; 否则跳转到 请将代码中所有代码断点禁用掉或者全部删除重新进入到debug模式下查看加载速度是否变快 . 发现debug模式下可能导致应用启动速度大幅度变慢。将breakpoint删除后我的应用从 171021ms+43824 ms=====》13021ms+2950ms 我使用Step4就解决了项目启动慢的问题，原来是打了太多断点，影响了加载速度…","link":"/2017/08/23/tomcat-slow-start/"},{"title":"自贡","text":"自贡位于川东南，以出产井盐和百姓擅长食辣而闻名，盐和辣椒的相逢碰撞出一道道热闹美味的菜肴，自然吸引一批老饕前往觅食。 鲜锅兔由兔肉，仔姜，二荆条等食材烹饪而成，属于重口味鲜辣川菜，也是一道自贡特有的辣到令人望而生畏的盐帮菜。老姜小米辣用油爆炒，二荆条切小段，仔姜切细丝，入锅和切小块的兔肉一起煮，出锅后的兔肉极鲜嫩，尖椒的辣味在姜丝的催化下已经完全渗透到兔肉里头，最后撒上一把香菜，大功告成。 作为来自成都的食客，自然也是吃过此类的鲜锅兔的。但是我认为，鲜锅兔还是以自贡的最为地道，既为发源地，也就兔肉嫩到极致，辣到极致。食客如我，虽一边喊辣，却一边在辣椒中找寻兔肉，味蕾已经完全打开。 每个城市，无论大小，都是有她独特的魅力和有趣之处的。城市自贡，与省会成都相比自然要小一些，却也有着其有趣别致之处。吃罢兔肉，在讲述制盐和城市发展历史的博物馆里，惊奇地发现每一处盆栽都雅致非常，不由得敬佩这里的馆长勤于侍弄。因为近来爱好上花草，所以在感叹历史的同时，特别喜爱这里的素净的氛围。 出产豆花的富顺就在自贡境内，豆花和盐，两个看似不相干的词，却也有着历史上命运的息息相关。豆腐发源于安徽寿县，自贡的富顺由于产盐，一时成为贸易往来频繁的富庶之地，于是很快豆腐就流传到了富顺。人们偶然发现用盐兑水蘸着嫩豆花远比炒豆腐这种中原吃法更加美味，自此嫩豆花+蘸水这个组合就流传了下来。 我们去的是晨光豆花，路边一个普普通通的苍蝇馆子，却也人生鼎沸，四川人对烹饪极为讲究，但是对就餐环境反而不甚在意，几个小方桌，小板凳，大家就热热闹闹的坐在了路边。这里最为招牌的就是豆花饭，豆子味浓郁，先要喝一口甘甜醇香的窖水，再夹一筷豆花，配上蘸水，入口时更让人惊喜。原来这家蘸水搭配讲究，老板自己搭配的香辣酱上除了撒上一把香菜，竟然还有薄荷。这可是吃蘸水头一回，难怪这家的招牌上，老板很自豪的印着“售卖蘸水”四个大字。来自寿县的男朋友在旁连连称赞，“好吃，好吃”，我在一边不禁想到，这大概是富顺与寿县又一次有意思的相逢。 烧牛肉是传统盐工菜的代表，自贡靠牛采卤，盐工干活辛苦，牛肉成为了自贡食物主要来源，几百年慢慢流传下来，烧牛肉成为了自贡街头巷尾最受人们欢迎的家常菜品。大安烧牛肉这家店，据说老板已经在这里开了二十多年，每日选择带筋牛肉小火慢炖，加入萝卜和笋子一起烧，最后撒一把海椒面和自贡井盐，最大程度激发出牛肉和萝卜的甘甜。 大安烧牛肉是自贡此行最后一站。回去时，街头恰巧碰到农夫推车卖新鲜竹笋，于是欣欣然选了几个带回成都，打算回去尝试自贡的蹄筋烧笋子。清明此行，不虚此行。","link":"/2017/04/04/zigong/"},{"title":"慢SQL排查- join太慢","text":"一次慢SQL的排查。 背景我负责的统一权限系统的管理端首页优化功能上线了一批新功能接口。发现一个接口响应延时比较长（近1s）的接口，接口如下： 由于接口逻辑层比较简单，只是调service查表的数据逻辑，初步判断是查DB效率太低。通过查看rds慢查询日志得到了验证。 定位到查询的业务SQL ，SQL语句为：123456789101112selectr.apply_type as applyType, count(DISTINCT(r.apply_value)) as countfromt1 apply,t2 rwhereapply.id = r.workflow_apply_idand apply.is_delete = 0and r.is_delete = 0and username = &quot;tomcat&quot;and status = 1group by r.apply_type 该语句的业务逻辑为返回用户正在申请中的权限类型：权限数列表。其中表t1为权限申请记录表，表t2 为申请记录与申请实体关系表，upm_data.t2表的 is_delete = 0 行数908247。 explain 一下看看: 发现出现了影响性能的join - BNL这里回顾下几种类型的join：1select * from t1 straight_join t2 on (t1.a=t2.b); //t1 行数N， t2行数M- NLJ (Index Nested-Loop Join) 表t1 join（驱动）表t2。如果可以使用被驱动表的索引，则为NLJ。执行过程类似于嵌套循环。总的扫描行数是N+N*2log2M BNL (Block Nested-Loop Join)表t1 join（驱动）表t2。如果不可以使用被驱动表t2的索引，则每次到t2匹配的时候，都要做一次全表扫描。执行次数为N*M。 BNL 的算法流程：Situation1: join_buffer放得下 把表t1的数据读入线程内存join_buffer中，由于我们这个语句写的是select *, 因此是把整个表t1 放入内存； 扫描表t2, 把表t2中的每一行取出来，跟join_buffer中的数据做对比，满足join条件的，作为结果集的一部分返回。BNL中，表t1和t2都做了一次全表扫描，因此总的扫描行数是N+M。由于join_buffer是以无序数组的方式组织的，因此对于表t2中的每一行，都要做M次判断，总共需要join_buffer内存判断次数是N*M次 (内存操作)。另外BNL选大表做驱动表，还是选小表做驱动表都是一样的。 Situation2: join_buffer放不下如果放不下t1的所有数据的话，策略很简单，就是分段放。 扫描表 t1，顺序读取数据行放入 join_buffer 中，放完第 88 行 join_buffer 满了，继续第 2 步； 扫描表 t2，把 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回； 清空 join_buffer； 继续扫描表 t1，顺序读取最后的 12 行数据放入 join_buffer 中，继续执行第 2 步。内存判断次数是N*M次 , 这种情况选择小表作为驱动表。 总的来说，如果要使用join，要避免出现BNL， 要给被驱动的表的字段加索引。另外，要小表驱动大表（在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。）。 再回看为什么会出现BNL? 被驱动表upm_workflow_apply_info_relation的驱动字段workflow_apply_id 并没有加索引。导致在表被join时候，MySQL使用了BNL算法。因此优化第一条就是加索引：另外第二条优化，经同事提醒驱动表upm_workflow_apply_v2的username 字符串的索引查询效率要远低于user_id 的索引效率。因此修改查询语句条件 “username =” 改为 “user_id =”123456789101112selectr.apply_type as applyType, count(DISTINCT(r.apply_value)) as countfromupm_workflow_apply_v2 apply,upm_workflow_apply_info_relation rwhereapply.id = r.workflow_apply_idand apply.is_delete = 0and r.is_delete = 0and user_id = &quot;123123&quot;and status = 1group by r.apply_type 结论遇到join语句性能瓶颈的时候，explain一下，如果出现了BNL，一定要优化，要给被驱动表加索引。要小表驱动大表","link":"/2019/09/11/慢SQL排查-join太慢/"},{"title":"RocketMQ源码分析5--Client之Consumer模块","text":"本文是RocketMQ源码分析系列之五，如有疑问或者技术探讨，可以email me,欢迎探讨. RocketMQ中Consuemer由用户部署，支持Push和Pull两种消费模式，支持集群消费和广播消息，提供实时的消息订阅机制。 Client模块的Producer和Consumer源码结构引用此文的图。 Producer和Consumer的共同逻辑，例如定期更新NameServer地址列表，定期更新TopicRoute，发送网络请求封装在MQClientInstance, MQClientAPIImpl, MQAdminImpl类。 集群消费，广播消息以及Pub/SubConsumer Group: Consumer的集合，这类Consumer通常消费一类消息，且消费逻辑一致。 集群消费: 一个Consumer Group重点Consumer实例平摊消费消息。例如，某个Topic有9条消息，其中一个Consumer Group有三个实例，那么每个实例只消费其中的三条消息。多个Consumer Group之间是Pub/Sub发布订阅模式。默认，Consumer是集群消费模式. 广播消费：一条消息被多个Consumer消费，即使这些Consumer属于同一个 Consumer Group消息也会被Consumer Group中的每个Consumer 都消费一次，广播消费中的 Consumer Group 概念可以认为在消息划分方面无意义。 1234567891011121314151617181920212223/** * Message model */public enum MessageModel { /** * broadcast */ BROADCASTING(\"BROADCASTING\"), /** * clustering */ CLUSTERING(\"CLUSTERING\"); private String modeCN; MessageModel(String modeCN) { this.modeCN = modeCN; } public String getModeCN() { return modeCN; }} 消息的推和拉RocketMQ是以拉模式为主，兼有推模式。 1.Push, 即Producer主动推送给Consumer: 发出消息到达broker后，broker马上把消息投递给Consumer. 客户端使用demo如下：1234567891011DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\"consumeGroup_001\"); consumer.setNamesrvAddr(\"127.0.0.1:9876\"); consumer.subscribe(\"Propolinse\", \"*\"); //subscribe a topic consumer.registerMessageListener(new MessageListenerConcurrently() { @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) { System.out.println(Thread.currentThread().getName() + \" Receive New Messages: \" + msgs); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } }); 2.Pull，即Broker收到Producer生产的消息后什么也不做，只等着Consumer在需要消费消息时候，主动向Broker拉取消息。客户端使用demo如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253private static final Map&lt;MessageQueue, Long&gt; offsetTable = new HashMap&lt;MessageQueue, Long&gt;(); public static void main(String[] args) throws InterruptedException, MQClientException { DefaultMQPullConsumer consumer = new DefaultMQPullConsumer(\"consumeGroup_002\"); consumer.setNamesrvAddr(\"127.0.0.1:9876\"); consumer.start(); // 首先通过打算消费的Topic拿到对应的MessageQueue的集合 Set&lt;MessageQueue&gt; mqs =consumer.fetchSubscribeMessageQueues(\"Propolinse\"); for (MessageQueue mq : mqs) {//遍历MessageQueue集合 SINGLE_MQ: //针对每个MessageQueue批量取消息，一次取完后，记录该队列下一次要取的开始offset，直到取完了，再换另一个MessageQueue while (true) { try { PullResult pullResult = consumer.pullBlockIfNotFound(mq, null, getMessageQueueOffset(mq), 32); System.out.println(pullResult); putMessageQueueOffset(mq, pullResult.getNextBeginOffset()); switch (pullResult.getPullStatus()) { case FOUND: List&lt;MessageExt&gt; messageExtList = pullResult.getMsgFoundList(); for (MessageExt m : messageExtList) { System.out.println(new String(m.getBody())); } break; case NO_MATCHED_MSG: break; case NO_NEW_MSG: break SINGLE_MQ; case OFFSET_ILLEGAL: break; default: break; } } catch (Exception e) { e.printStackTrace(); } } } consumer.shutdown(); } private static void putMessageQueueOffset(MessageQueue mq, long offset) { offsetTable.put(mq, offset); } private static long getMessageQueueOffset(MessageQueue mq) { Long offset = offsetTable.get(mq); if (offset != null) return offset; return 0; } 实际上，在RocketMQ中无论是Push还是Pull, 底层都是通过Consumer从Broker拉消息实现的（PullAPIWrapper.pullKernelImpl）。为了做到能够实时接收消息，RocketMQ使用长轮询方式，保证消息实时性和Push方式一致。这种长轮询类似Web QQ收发消息机制。 底层上看Consumer使用RocketMQ的client- pullMessage接口，通过remoting模块实现与Broker通信，调用pullMessageProcessor读取Broker存储的文件消息。1PullResult pullResult = this.mQClientFactory.getMQClientAPIImpl().pullMessage(brokerAddr, requestHeader, timeoutMillis, communicationMode, pullCallback); pullMessageProcessor 调用DefaultMessageStore读取消息：123final GetMessageResult getMessageResult = this.brokerController.getMessageStore().getMessage(requestHeader.getConsumerGroup(), requestHeader.getTopic(), requestHeader.getQueueId(), requestHeader.getQueueOffset(), requestHeader.getMaxMsgNums(), messageFilter); Push方式里，consumer把轮询过程封装了，并注册MessageListener监听器，取到消息后，唤醒MessageListener的consumeMessage()来消费，对用户而言，感觉消息是被推送过来的。 Pull方式里，取消息的过程需要用户自己写，首先通过打算消费的Topic拿到MessageQueue的集合，遍历MessageQueue集合，然后针对每个MessageQueue批量取消息，一次取完后，记录该队列下一次要取的开始offset，直到取完了，再换另一个MessageQueue。 之所以采用Pull模式为主，是因为RocketMQ的主要应用场景是金融交易链路。因此需要将稳定可靠放在首位，因此采用了长连接轮询拉的模式。 Push和Pull的使用场景https://cloud.tencent.com/document/product/406/4791 场景1：Producer 生产速率 VS Consumer消费速率 如果Producer的生产速率大于Consumer消费速率, Push方式由于无法得知Consumer的状态，所以只要有数据产生，就会不断推送给Consumer一堆Consumer无法处理的消息，这时候Consumer只能不是reject就是error，然后来回踢皮球。 反观Pull模式，Consumer可以按需消费，不用担心自己处理不了的消息来骚扰自己，而broker堆积消息也会相对简单，无需记录每一个要发送消息的状态，只需要维护所有消息的队列和偏移量就可以了。所以对于慢消费，消息量有限且到来的速度不均匀的情况，Pull模式比较合适。 场景2：消息的实时性 采用Push模式，一旦消息到达，服务端就会立刻将消息推送给Consumer,这种方式实时性是非常好的。而Pull模式，如果想要保证实时性，就只能采用长连接轮询的方式去拉消息(RocketMQ就是如此)。 场景3：消息延迟与忙等 这是Pull模式最大的短板。由于主动权在消费方，消费方无法准确地决定何时去拉取最新的消息。如果一次Pull取到消息了还可以继续去Pull，如果没有Pull取到则需要等待一段时间重新Pull。但等待多久就很难判定了。你可能会说，我可以有xx动态Pull拉取时间调整算法，但问题的本质在于，有没有消息到来这件事情决定权不在消费方。也许1分钟内连续来了1000条消息，然后半个小时没有新消息产生，可能你的算法算出下次最有可能到来的时间点是31分钟之后，或者60分钟之后，结果下条消息10分钟后到了，是不是很让人沮丧？ 当然也不是说延迟就没有解决方案了，业界较成熟的做法是从短时间开始（不会对broker有太大负担），然后指数级增长等待。比如开始等5ms，然后10ms，然后20ms，然后40ms……直到有消息到来，然后再回到5ms。 即使这样，依然存在延迟问题：假设40ms到80ms之间的50ms消息到来，消息就延迟了30ms，而且对于半个小时来一次的消息，这些开销就是白白浪费的。 在RocketMQ里，有一种优化的做法——长轮询 Pull ，来平衡推拉模型各自的缺点。基本思路是：消费者如果尝试拉取失败，不是直接return,而是把连接挂在那里wait,服务端如果有新的消息到来，把连接notify起来，这也是不错的思路。但海量的长连接block对系统的开销还是不容小觑的，还是要合理的评估时间间隔，给wait加一个时间上限比较好。 启动Consumer 引用 RocketMQ客户端最佳实践 OffsetStore","link":"/2018/03/31/client-consumer-md/"},{"title":"分布式Redis探讨","text":"为什么需要使用Redis集群Redis是目前互联网热点数据缓存解决方案的技术选型之一。在大型Web应用中，缓存存储的数据量巨大，在这种情况单机Redis难以支撑服务，分布式横向扩展Redis多实例协同运行。另外，Redis的工作模型是单线程工作的，对于目前硬件机器大多是多核CPU，几十G内存的主机来说是一种浪费。更为实际的应用方式是，一台机器同时运行多个Redis实例。 集群遇到的问题相对于单机，分布式服务会遇到很多问题，本文试图探讨并解决： Redis的主从复制是怎么实现的？ Redis的集群模式是怎么实现的？ Redis的key是如何寻址的？ 使用Redis如何设计分布式锁？使用ZK可以吗?如何实现。 Redis集群实现方案分片(Sharding)，顾名思义是分布式服务将数据根据其特征（一般是根据key进行哈希）分发到不同的Redis服务器实例上。通常有客户端分片，服务端分片以及代理分片三种方式。 客户端分片这种方案是将分片工作放在Redis客户端。利用哈希算法对数据的key进行散列，特定的key映射到特定的Redis实例上，这样客户端就知道该向哪个Redis节点操作数据。目前Jedis已经支持客户端分片。 客户端分片的好处是可以不依赖第三方分布式中间件，客户端和Redis服务端都处于一种轻量灵活的状态。但是缺点非常明显，比如扩容–当想要增加Redis实例的时候，Rehash进行数据迁移的代价是非常大的。再比如，Redis分布式服务经常遇到的问题利用主从容错，保证master/slave实例一致会给客户端带来不可避免的问题。 服务器分片服务器分片典型的产品是Redis 3.0。Redis 3.0提供的Redis Cluster是一种Redis实例P2P模型，依靠Gossip协议进行消息同步。在服务端逻辑上分成16384个slot槽，客户端发送到Redis cluster的key会哈希分发到16384个槽中的某个。而Redis集群中的Redis实例负责分摊6384个槽中的一部分。当Redis实例横向扩展时候，只需要对槽做再分配，将槽中的键值对迁移。另外，对于Redis实例容灾问题，官方推荐将Redis配置成主从结构，即一个master节点，多个slave从节点。如果主节点失效，Redis Cluster会根据选举算法从Slave节点中选择一个上升为主节点，整个集群继续对外提供服务。 这种架构对于客户端来说，整个Cluster被看做一个整体，客户端可以连接任意一个Redis实例进行操作，就像操作单一Redis实例一样。大大解放了Redis客户端。 但是，正如Codis作者所说, 服务端分片将分布式的逻辑和存储引擎的逻辑绑定在了一起，集群升级困难，运维困难。没有一个中心，无法获知集群处于什么状态。 Proxy-based代理分片该方案引入一个代理层(Proxy)将分片工作交给这一层从而对多Redis实例进行统一管理和分配，使Redis客户端只需要向Proxy操作，代理接收客户端的数据请求，根据key做哈希映射，分发给Redis实例。实现了Redis Server解耦，Redis Server只专门做Redis存储。代理分片的重要设计思想是：将分布式的逻辑和存储引擎隔离。典型的产品是Twemproxy和Codis。 这种代理分片的好处是结合了服务器分片和客户端分片的优势，即服务端实例彼此独立，线性可伸缩，同时分片可以集中管理（proxy来管理）。 不过，难以避免的缺点是部署起来很麻烦，例如codis组件繁多，部署起来困难。另外，增加了一层代理会增加网络转发开销。Codis作者也承认,单机且不开pipeline的情况下，大概会损失40%左右性能（不过这对于多Proxy多Redis实例来说并不是个事儿）。 Codis设计思想Codis 3.x 由以下组件组成： Codis Server 可以理解为Redis Server, 存储和分布式逻辑隔离。 Codis Proxy 客户端连接Redis的代理服务。负责分布式逻辑。 对于同一个业务集群而言，可以同时部署多个codis-proxy实例以实现proxy HA 不同codis-proxy之间由codis-dashboard保证状态同步 Codis Dashboard 集群管理工具。支持 codis-proxy、codis-server 的添加、删除，以及据迁移等操作。在集群状态发生改变时，codis-dashboard 维护集群下所有 codis-proxy 的状态的一致性。 对于同一个业务集群而言，同一个时刻 codis-dashboard 只能有 0个或者1个； 所有对集群的修改都必须通过 codis-dashboard 完成。 Codis Admin 集群管理的命令行工具。 可用于控制 codis-proxy、codis-dashboard 状态以及访问外部存储。 Codis FE 集群管理界面。 通过配置文件管理后端 codis-dashboard 列表，配置文件可自动更新。 Zookeeper 负责分布式服务的服务发现。用来存放数据路由表和codis-proxy节点的原信息，codis-config发起的命令都会通过zookeeper同步到各个存活的codis-proxy. Codis数据存储Codis借鉴了Redis的Pre-sharding思想,将数据根据key进行哈希crc32(key)%1024映射存储在1024个slot(0-1023)里面。slot是逻辑概念，每个slot的数据实际上由某个特定的Redis实例物理存储。即一个slot对应一个codis-group. 一个codis-group为codis redis 实例单元（一个master和n个slave组成。） 例如系统有两个Redis Servier Group, 那么Redis Servier Group与slot对应关系如下：123Redis Servier Group slot1 0~4992 500~1023 当新增一个codis group时候，slot会重新分配。Codis重新分配有两种方法：第一种，通过Codisconfig手动重新分配，指定每个Redis Servier Group对应的slot的范围。1234Redis Servier Group slot1 0~4992 500~6993 700-1023 第二种：通过Codis管理工具Codisconfig的rebalance功能，会自动根据每个Redis Server Group的内存对slot进行迁移，以实现数据的均衡 Codis数据迁移当Codis扩容或者缩容时候就会进行数据迁移。数据迁移的最小单位是slot。而对于每个Redis实例来说是没有分布式逻辑在其中的，它们只是实现proxy关于数据传输的指令： 选取特定的slot中的数据传输给另一个Redis实例，传输成功后，把本地的数据删除。整个过程是原子的。 引用dongxu.h一次典型的迁移流程： codis-config 发起迁移指令如 pre_migrate slot_1 to group 2 codis-config 等待所有的 proxy 回复收到迁移指令, 如果某台 proxy 没有响应, 则标记其下线 (由于proxy启动时会在zk上注册一个临时节点, 如果这个proxy挂了, 正常来说, 这个临时节点也会删除, 在codis-config发现无响应后, codis-config会等待30s, 等待其下线, 如果还没下线或者仍然没有响应, 则codis-config 将不会释放锁, 通知管理员出问题了) 相当于一个2阶段提交 codis-config 标记slot_1的状态为 migrate, 服务该slot的server group改为group2, 同时codis-config向group1的redis机器不断发送 SLOTSMGRT 命令, target参数是group2的机器, 直到group1中没有剩余的属于slot_1的key 迁移过程中, 如果客户端请求 slot_1 的 key 数据, proxy 会将请求转发到group2上, proxy会先在group1上强行执行一次 MIGRATE key 将这个键值提前迁移过来. 然后再到group2上正常读取 迁移完成, 标记slot_1状态为online Codis HA主从复制Codis的HA可以分为Redis HA和Proxy HA. Codis 引入了Redis Server Group, 通过指定一个Master Redis实例和多个slave Redis实例实现了Redis的高可用。但是当一个Master Redis挂掉之后，Codis是不会自动提升Slave为Master的。Codis作者提到之所以这么设计是因为考虑到主从数据一致性的问题：当Master挂掉之后，Master上的数据是否已经同步到Slave上是没法保证的，所以不如直接报个错给用户。如果想实现salve自动提升为master，可以使用codis-ha工具，该工具会在检测到master挂掉的时候主动应用主从切换策略，提升单个salve成为新的master。 对于Proxy HA，由于proxy是无状态的，当加入多个proxy，并且每个新增proxy都会去Zookeeper上注册，就会使每个proxy上视角一致从而实现了高可用。 Codis分布式锁解决方案对于Redis并发冲突，可以使用乐观锁和分布式锁进行解决。乐观锁的缺点在于，随着计算机并发数的增加，程序的重试的次数可能会越来越多，导致资源被白白浪费。因此可以通过锁来减少对WATCH命令的使用，从而达到避免重试，提升性能并在某些情况简化代码。 在Redis集群多机器的情况下，推荐不直接使用操作系统级别或者是编程语言级别的锁，而是使用Redis构建锁。这和“范围”有关：为了对Redis存储的数据进行排他性访问，客户端需要访问一个锁，这个锁必须定义在一个可以让所有客户端都看得见的范围之内 - Redis本身，因此我们需要把锁构建在Redis里面，进而实现一个分布式锁。 错误的实现锁可能出现的问题： 持有锁的进程因为操作时间过长儿导致锁被自动释放，但进程本身并不知道这一点，甚至还可能错误释放掉其他进程持有的锁。 一个持有锁并打算执行长期操作的进程已崩溃，但其他想要获取锁的进程不知道哪个进程持有着锁，也无法检测出持有锁的进程已经崩溃，只能白白等待锁被释放 在上一个进程持有的锁过期之后，其他多个进程同时尝试获取锁，并且都获取了锁 情况1和情况23同时出现，导致多个进程获得了锁，而每个进程都以为自己是唯一一个获得锁的进程。 在Codis中，Codis在做任何操作的时候，都会到ZooKeeper拿一个锁以保证是唯一的操作实例，这也防止了路由表被改坏。尤其在数据迁移的时候，通过锁保证了不可能同时有多个slots处于迁移状态。 参考资料 Codis官方文档 Codis作者谈设计思想1-3 知乎关于Redis集群讨论 彭东稳：http://www.ywnds.com/?p=6579 《Redis in Action》 基于Zookeeper的分布式锁 Codis作者黄东旭：细说分布式Redis架构设计和那些踩过的坑","link":"/2018/03/06/distributed-redis/"},{"title":"Elastic Search 学习笔记-3 (深入了解ES搜索原理)","text":"We will not venture into Lucene’s implementation details, but rather stick to how the inverted indexis used and built. That is what influences how we can search and index. inverted indexes and index terms倒排索引将单词（terms）映射为包含该单词的文本。可以说，an index term is the unit of search. 由分词产生的terms直接决定哪些类型的搜索高效，哪些类型的搜索不高效。由于单词在倒排索引中是有序的，因此we can efficiently find the things given term prefixes. When all we have is an inverted index, we want everything to look like a string prefix problem. – ngram分词。 building indexes当创建一个倒排索引时候，有许多性能优化点要关注：search speed，index compactness, indexing speed and the time it takes for new changes to become visible. search speed和index compactness是有关系的， 1234inverted index compatness(+) -&gt;data to be processed (-) -&gt;the data fit in memory(+) -&gt;the efficicency of update(-) 倒排索引被压缩地越小，要处理的数据就越小，进而可以放更多的数据到内存，这样查询速度也越快。但是，倒排索引被压缩程度越大，这也导致更新性能的牺牲。 事实上，ES的倒排索引写到硬盘之后是不变的（immutable）。这样便于并发处理，不需要加锁。而且一旦倒排索引被读入内存后，不需要再修改缓存，提高了查询性能。（因此，在Lucene存储经常变化的数据并不是一个好方法） 而在ES/Lucene中update a document 操作是先从索引中删除该document（删除是一个位图操作不算update），再重新插入该document。可见update操作是一个耗费资源的操作。there is no in-place update of values. 当新插入索引中一个文档的时候，索引的变化先被缓冲在内存中，最后一起flush to硬盘。 而什么时候flush to硬盘取决于 1. how quickly changes must be visble 2. 内存中可供缓存的容量 3. IO saturation. index update既然倒排索引由于查询性能需要是不可变的，那么需要一个方法在保留不变形的前提下实现倒排索引的更新。这也是索引分段的原因。 通过增加新的补充索引反映新近的修改，而不是重新写整个倒排索引。每个倒排索引都会被轮流查询到，从最早的开发，查询完再对结果进行合并。 在per segment search基础上，Lucene introduce the concept of “commit point”, 提交点列出所有已知的文件. 逐段搜索会以如下流程进行工作： 新文档被收集到内存索引缓存（ES与硬盘之间是文档系统缓存）， 见 Figure 1, “一个在内存缓存中包含新文档的 Lucene 索引” 。 不时地, 缓存被提交 ： 一个新的段–一个追加的倒排索引–被写入磁盘。 一个新的包含新段名字的 提交点 被写入磁盘。 磁盘进行 同步 — 所有在文件系统缓存中等待的写入都刷新到磁盘，以确保它们被写入物理文件。 新的段被开启，让它包含的文档可见以被搜索。 内存缓存被清空，等待接收新的文档。 当一个查询被触发，所有已知的段按顺序被查询。词项统计会对所有段的结果进行聚合，以保证每个词和每个文档的关联都被准确计算。 这种方式可以用相对较低的成本将新文档添加到索引。 Near real time search之所以说ES，Lucene是近实时搜索，是因为upddates buffer flush to disk机制使得一个新的文档从索引到可被搜索的延迟显著降低了。而由于per-segment search按段搜索的发展，新文档在几分钟之内可被检索。 index segmentsLucene中索引由多个不可改变的index segment/子索引组成。当新创建/更新一个文档, 索引先写到内存中缓冲着，触发一定限制条件后(flush)写入硬盘，生成一个独立的子索引 – 子索引在Lucene中叫做segment。当执行搜索的时候，Lucene在每个分段上执行搜索，过滤出所有删除，在所有分段上合并结果。每隔一段时间，这些segments会触发一次合并，被标记为删除的document会被抛弃，这也解释了为什么插入更多的document会导致a smaller index size. Merge的触发： 通过配置mergefactor这个参数控制硬盘中有多少个segments. MergeFactor越大耗费内存越多，索引速度也会越快些。但太大譬如300，最后合并的时候还是很慢。Batch indexing 应 MergeFactor&gt;10。 segments flush to disk最常见的原因是 “continuous index refreshing”(默认每秒refresh一次)。当segment被flush 到硬盘之后，这些更改的数据可以被搜索到，enables近实时搜索near real-time search.1a flush = a new segment to be created + invalidate some caches + trigger a merge When indexing throughput is important, e.g. when batch (re-)indexing, it is not very productive to spend a lot of time flushing and merging small segments. Therefore, in these cases it is usually a good idea to temporarily increase the refresh_interval-setting, or even disable automatic refreshing altogether. One can always refresh manually, and/or when indexing is done. 但是，per-segment search 还有瓶颈-磁盘，一个新的文档更改形成新的段之后，需要linux文件操作fsync来确保被物理性地写入磁盘，这样在断电的时候就不会丢失数据。但是fsync操作代价很大，如果每次索引一个文档都去执行一次的话会造成很大的性能问题。 因此，ES将fsync从整个过程中移除，允许新段被写入和打开–使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。可以通过配置refresh_interval来配置新段在缓存中被打开–namely 在缓存中可以被搜索的概率。 refresh_interval 可以在既存索引上进行动态更新。 在生产环境中，当你正在建立一个大的新索引时，可以先关闭自动刷新，待开始使用该索引时，再把它们调回来： PUT /my_logs/_settings{ “refresh_interval”: -1 } PUT /my_logs/_settings{ “refresh_interval”: “1s” } elasticsearch indexes以上讲的Lucene index, 现在让我们重回ES index. 一个ES index 由 若干主分片组成和若干副本组成。每个分片都是一个Lucene index.123456a ElasticSearch index = {Lucene indexes}n = {index segments}n*mES index -&gt; Lucene indexes -&gt; Lucene index segmentsn is # Lucene indexesm is # index segments 当你在ES索引上执行搜索的时候，最终搜索会落在所有segment上。所以可以说下面两个场景实质上是一样的：121. search 2 ES index with 1 shard each2. search 1 index with 2 shard SummaryTo summarize, these are the important properties to be aware of when it comes to how Lucene builds, updates and searches indexes on a single node: How we process the text we index dictates how we can search. Proper text analysis is important.Indexes are built first in-memory, then occasionally flushed in segments to disk.Index segments are immutable. Deleted documents are marked as such.An index is made up of multiple segments. A search is done on every segment, with the results merged.Segments are occasionally merged.Field and filter caches are per segment.Elasticsearch does not have transactions Reference:1 . https://www.elastic.co/blog/found-elasticsearch-from-the-bottom-up2 . http://elasticsearch.cn/book/elasticsearch_definitive_guide_2.x/dynamic-indices.html","link":"/2017/04/12/es-note3/"},{"title":"使用Flink进行权限申请实时推荐","text":"题图不相关，图为2019鸟巢五月天演唱会所拍。本文是笔者将Flink用到公司业务上的一次实践，目的是为了实现实时推荐效果，提升笔者所负责的系统的用户体验。 背景权限申请服务是滴滴UPM统一权限系统一个重要的用户使用入口。目前，权限申请量已达周10000单， 申请用户周4000人。 随着申请量日益增多，我们发现用户反馈大量集中在：“用户想要要访问xxx页面/想做xxx，要申请UPM权限，但是用户不知道这个xxx页面是UPM的什么业务子系统”，“用户要访问xxx页面，用户虽然知道这是xxx系统的，但是不知道申请什么角色”。 抽象来说，很多用户使用UPM的痛点为不清楚要申请哪一系统的哪个权限。矛盾点在于UPM的定位是公司级统一权限系统（最新数据显示接入UPM系统1000+个，使用UPM鉴权系统近800个）。这种平台定位就会导致用户要先选择业务系统，再选择并申请权限。而用户的核心诉求就是申请权限，然而由于很多用户对业务系统的了解不足, 会出现用户在UPM申请权限难的困境。为了提升用户申请权限的体验，我们做了权限申请实时推荐。 方案首先调研了近5天鉴权的用户发起申请率。在UPM中，用户，业务系统，UPM三者的关系如下图所示，用户访问业务系统，业务系统会携带用户信息（通常是用户账号）调用UPM鉴权接口到UPM服务端进行鉴权。UPM服务端返回给业务系统鉴权结果，服务端据此做判断，在界面返回给用户是否有权限。用户从业务系统使用受阻转而到访问UPM的行为路径：用户使用某一业务系统（访问业务系统页面），业务系统判断用户没有权限，推荐来权限系统UPM申请权限。 如果用户真的需要访问这一页面的权限（还有很多用户是误点进来），用户就会到UPM去申请权限。 鉴权的用户发起申请率鉴权的用户发起申请率 = 发起申请的用户中有过鉴权记录 / 发起申请的用户数 调研发现近5天（10-29~11.3）鉴权的用户发起申请率 为 77.51%。据此判断针对用户在UPM鉴权记录进行权限实时推荐来解决用户申请权限难的痛点是有必要的。 实现根据用户鉴权日志（UPM鉴权日志 1.2亿 line/天， 120G/天）实现权限实时申请推荐的数据链路如图所示： 业务数据采集：UPM鉴权业务日志采集到Kafka; Flink聚合计算：由于申请权限推荐这一场景对于鉴权日志分析到实时性比较高，因此采用Flink流计算对鉴权日志实时处理, 并输出结果到下游Kafka。 业务应用部分：根据用户最新鉴权记录，进行下游Kafka消费，并根据业务具体情况制定具体推荐策略，例如只推荐开放申请的app, app推荐黑名单机制（如果有定时调用鉴权接口的系统，会被纳入黑名单不进行推荐。）等等。并将用户的最近鉴权记录格式化存储到MySQL和Redis中。 推荐API部分：业务提供接口根据用户名推荐用户要申请权限的系统和权限。 Flink实时计算鉴权日志部分，为避免大量重复鉴权记录对下游kafka造成积压，因此采用了10s的window对鉴权结果记录进行聚合。计算逻辑为以下几点： 针对鉴权日志map出每条鉴权日志涉及到到业务系统appId, 鉴权用户的用户名 username，鉴权接口upmAPI, 鉴权时间authenticatedTime等字段； filter过滤留下response 鉴权结果日志（除去request部分）； 针对不同的接口的返回，记录用户鉴权失败与否，作为扩展功能，记录在authenticatedResult字段。鉴权接口分为两类： 鉴权返回yes or no 的 /user/check/feature接口 (流量Top1接口）。这类接口根据接口返回结果是否等于false可以直接判断用户鉴权是否失败。 第二类是获取用户所有权限后业务方自行逻辑判断的接口。例如 /user/get/roles (流量Top2接口，获取用户所有角色）, /user/get/areas（获取用户所有地区）, /user/get/flags(获取用户所有flags) 等等。这一类接口由于有业务方自行判断的逻辑，目前认为返回接口为空的时候，为用户鉴权失败。 格式化结果数据，sink到下游kafka.Flink 清洗鉴权日志核心代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546DataStream&lt;StreamMessage&lt;UPMCoreLog&gt;&gt; inStream = sourceStream .flatMap(new UPMFlagMapFunc()) // 只留response .filter(new UPMFliterFunc()) // watermark .assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;StreamMessage&lt;UPMCoreLog&gt;&gt;() { @Override public long extractAscendingTimestamp(StreamMessage&lt;UPMCoreLog&gt; element) { //鉴权实践作为eventTime时间戳 long timestamp = element.getPyload().getAuthenticatedTime().getTime(); return timestamp; } }) // key by {appid+username} .keyBy(new KeySelector&lt;StreamMessage&lt;UPMCoreLog&gt;, Object&gt;() { @Override public String getKey(StreamMessage&lt;UPMCoreLog&gt; streamMessage) throws Exception { return streamMessage.getPyload().generateIndentifying(); } }) // tumbling time window每10s统计一次鉴权情况 .window(TumblingEventTimeWindows.of(Time.seconds(10))) // 聚合结果 .reduce(new ReduceFunction&lt;StreamMessage&lt;UPMCoreLog&gt;&gt;() { @Override public StreamMessage&lt;UPMCoreLog&gt; reduce(StreamMessage&lt;UPMCoreLog&gt; upmStreamMessage, StreamMessage&lt;UPMCoreLog&gt; t1) throws Exception { // 保留一条数据 UPMCoreLog baseline = upmStreamMessage.getPyload(); UPMCoreLog valuein = t1.getPyload(); // 更新鉴权时间新的记录 if (valuein.getAuthenticatedTime().after(baseline.getAuthenticatedTime())) { return t1; } else { return upmStreamMessage; } } });//sinkDataStream&lt;String&gt; outStream = inStream.map(new MapFunction&lt;StreamMessage&lt;UPMCoreLog&gt;, String&gt;() { @Override public String map(StreamMessage&lt;UPMCoreLog&gt; value) throws Exception { UPMCoreLog pyload = value.getPyload(); return pyload.toString(); }}); 通过Flink对用户的每条鉴权结果进行判断，清洗出格式化的用户鉴权记录存储到下游Kafka，消息记录如下所示：123456789{ \"username\":\"XXX\", // 用户 \"appId\":962, // 业务系统ID \"authenticatedTime\":1572770351000, \"upmAPI\":\"/user/get/roles\", // 鉴权接口 \"isReq\":false, \"authenticatedResult\":false // 鉴权是否成功} 有了下游Kafka存储用户鉴权结果的格式化记录后，就可以实时消费用户鉴权结果消息，将用户最新被拦截无权限的业务系统信息存储到Redis/MySQL 中，以此做权限申请推荐。 产品效果: 难点&amp;接下来工作 用户申请权限的实时推荐逻辑是基于用户访问业务系统的的鉴权结果日志进行的，但是由于业务方使用UPM鉴权接口比较多样，有时候会造成推荐结果不准确/难以cover住用户真正想要。例如，有的业务方针对自己业务系统比较多的情况，做了一个类似网关的门户系统，门户系统整合了业务方的所有系统。当用户访问门户系统，门户系统携带用户的身份信息及门户系统管理的所有系统appId, 分别到UPM这里鉴权。这样会导致UPM难以获得用户在某一业务系统的最实时的鉴权记录，从而导致推荐的不准确。 用户申请权限终究是想申请业务系统的角色/标识位/地区等。因此，接下来我们会重点做权限的推荐。权限的推荐难度有以下几点： 根据上文所述，鉴权接口分为两类，鉴权返回yes or no 的 /user/check/feature接口 (流量Top1接口）。这类接口根据接口返回结果是否等于false可以直接判断用户鉴权是否失败。业务可以通过feature失败的记录，为用户推荐关联的角色。但是如果业务方设置关联feature 权限点的角色太多的话，也会造成给用户推荐的角色太多的情况， 这样很可能导致用户更为迷惑。 第二类是获取用户所有权限后业务方自行逻辑判断的接口。例如 /user/get/roles (流量Top2接口，获取用户所有角色）, /user/get/areas（获取用户所有地区）, /user/get/flags(获取用户所有flags) 等等。这一类接口由于有业务方自行判断的逻辑，目前认为返回接口为空的时候，为用户鉴权失败。这一类接口，UPM服务端是无法猜测用户需要申请的角色。因此使用这类接口的业务方，我们无法做去权限推荐。 用户申请权限推荐只根据鉴权记录这一维度是否有些单一，后续还需要调研是否需要结合用户岗位进行推荐。","link":"/2019/11/21/flink-upm/"},{"title":"Go interface源码解析","text":"Go Interface源码分析在Go语言中，interface是一个非常重要的概念，不仅可以用来表示任意数据类型的抽象，还可以用来定义一组method集合，实现duck-type programming，到达泛型化编程的目的。所以，深入学习Go中interface的实现很有必要。 Definitioninterface的实现在代码runtime\\runtime2.go中（1.9.2版本），根据是否包含方法，分为iface和eface两种。 eface123456789101112131415161718192021222324type eface struct { _type *_type data unsafe.Pointer}// Needs to be in sync with ../cmd/link/internal/ld/decodesym.go:/^func.commonsize,// ../cmd/compile/internal/gc/reflect.go:/^func.dcommontype and// ../reflect/type.go:/^type.rtype.type _type struct { size uintptr //type size ptrdata uintptr // size of memory prefix holding all pointers hash uint32 //hash of type;avoids computation in hash table tflag tflag align uint8 fieldalign uint8 kind uint8 // type mask alg *typeAlg // gcdata stores the GC type data for the garbage collector. // If the KindGCProg bit is set in kind, gcdata is a GC program. // Otherwise it is a ptrmask bitmap. See mbitmap.go for details. gcdata *byte str nameOff //string form ptrToThis typeOff // type for pointer to this type, may be zero} eface是对不包含任何方法的数据类型的抽象，如： 12x := 1var y interface{} = x 在运行时的内部表示中，y就是eface类型。其中，_type是对一种具体类型的描述，可以认为是Go语言中所有类型的公共描述。data则是指向真实数据的指针。 iface123456789101112131415161718192021222324252627282930type iface struct { tab *itab data unsafe.Pointer}// layout of Itab known to compilers// allocated in non-garbage-collected memory// Needs to be in sync with// ../cmd/compile/internal/gc/reflect.go:/^func.dumptypestructs.type itab struct { inter *interfacetype //interface type description _type *_type //origin type link *itab //hash表头指针 hash uint32 // copy of _type.hash. Used for type switches. bad bool // type does not implement interface inhash bool // has this itab been added to hash? unused [2]byte fun [1]uintptr // variable sized}type interfacetype struct { typ _type pkgpath name mhdr []imethod}type imethod struct { name nameOff ityp typeOff} iface是包含方法接口的内部表示，通过更复杂的结构itab封装，itab各字段详细描述如下表： 字段 类型 说明 inter *interfacetype 对接口声明原型的描述，包括包路径，方法表等 _type *_type 对实现该接口的原始真实类型的描述 link *itab itab hash表对应槽位首地址 hash uint32 copy from _type bad bool 标记位 inhash bool 是否加入itab hash表 unused [2]byte 保留字段 fun [1]uintptr 方法地址表 Exampleinterface只是对类型的上层抽象，在runtime过程中，还是要映射到具体的类型。下面通过一些例子，观察接口的内部实现和使用方式。 类型转化eface1234567891011package mainimport ( \"fmt\")func main() { x := 1 var y interface{} = x fmt.Println(y)} 查看汇编生成的汇编代码 代码第九行，即接口赋值的实现，调用了runtime.convT2E64方法，其具体实现为： 1234567891011121314func convT2E64(t *_type, elem unsafe.Pointer) (e eface) { ... var x unsafe.Pointer if *(*uint64)(elem) == 0 { x = unsafe.Pointer(&amp;zeroVal[0]) } else { x = mallocgc(8, t, false) //分配一个8字节的对象 *(*uint64)(x) = *(*uint64)(elem) } e._type = t e.data = x return} 形象一些的内存表示如下图： iface类似的，对于包含参数的接口转换，runtime包中给出的实现为： 12345678910func convT2I(tab *itab, elem unsafe.Pointer) (i iface) { t := tab._type ... x := mallocgc(t.size, t, true) typedmemmove(t, x, elem) i.tab = tab i.data = x return} 无论iface或者eface都包含类型和值两部分，在类型转换过程中，由于类型信息不会改变，直接通过拷贝指针赋值，而具体的值则根据原始类型的大小，复制一份新的内容。 接口嵌套接口嵌套允许我们给接口新增特性，实现类似继承的效果。在介绍接口嵌套前，先看下接口类型itab中，接口方法的组织方式。在前文对itab接口的定义中指出，fun [1]uintptr是方法地址表，但只包含一个指针，那多个方法的接口如何实现呢？ 1234567891011121314151617181920212223242526package maintype MyInterface interface { Print() Hello() World() AWK()}type MyStruct struct {}func (me MyStruct) Print() {}func (me MyStruct) Hello() {}func (me MyStruct) World() {}func (me MyStruct) AWK() {}func Foo(me MyInterface) { me.Print() me.Hello() me.World() me.AWK()}func main() { var me MyStruct Foo(me)} 同样反编译Foo函数的调用： ​ 从上到下，依次调用Print,Hello,World,AWK方法，而方法的地址则分别来源于栈顶指针SP加上相对偏移量 ：SP+0x18+0x30, SP+0x18+0x28, SP+0x18+0x38,SP+0x18+0x20。SP+0x18对应了me的itab地址，而fun相对偏移量可以计算为：8+8+8+4+1+1+2 = 32 = 0x20，而0x20是AWK函数的位置，然后0x28是Hello, 0x30是Print, 0x38是World。这说明： fun[0]地址后依次写入了其他方法的函数指针 函数指针之间按照字典序存放 方法地址的写入参考：runtime\\iface.go\\additab 了解iface的方法布局，接着看下接口嵌套的实现方式： 123456789101112131415161718192021222324252627282930package maintype Interface1 interface { Print() Hello()}type Interface2 interface { Interface1 World() AWK()}type MyStruct struct {}func (me MyStruct) Print() {}func (me MyStruct) Hello() {}func (me MyStruct) World() {}func (me MyStruct) AWK() {}func Foo(me Interface2) { me.Print() me.Hello() me.World() me.AWK()}func main() { var me MyStruct Foo(me)} 上述代码经过反编译得到的结果和刚刚的例子一致，说明接口嵌套，在实现上是对方法在fun上的平铺。 类型断言在代码中，常常需要对interface类型进行断言，达到识别接口原生类型的目的。一般有两种写法： 12345678910func do(v interface{}) { n := v.(int) // might panic}func do(v interface{}) { n, ok := v.(int) if !ok { // 断言失败处理 }} 第一种写法容易引入panic，第二种方法更加安全，主要原因是底层断言函数，对1个和2个返回值的处理过程略有差异： 1234567891011121314151617181920212223242526272829303132func assertI2I(inter *interfacetype, i iface) (r iface) { tab := i.tab if tab == nil { //为nil直接panic // explicit conversions require non-nil interface value. panic(&amp;TypeAssertionError{\"\", \"\", inter.typ.string(), \"\"}) } if tab.inter == inter { r.tab = tab r.data = i.data return } r.tab = getitab(inter, tab._type, false) //第三个参数，不允许类型判断失败，否则panic r.data = i.data return}func assertI2I2(inter *interfacetype, i iface) (r iface, b bool) { tab := i.tab if tab == nil { //nil接口，直接返回失败 return } if tab.inter != inter { //true允许失败，r返回nil tab = getitab(inter, tab._type, true) if tab == nil { return } } r.tab = tab r.data = i.data b = true return} 由于eface类型的接口比较简单，类型断言判断type即可，其实前文go demo里面，关于int类型的断言调用编译器可以完成判断，后文主要以转iface类型的接口为例说明。 当被断言类型为nil时，一个直接panic，另外一个返回false，这是第一点不同。第二点差异主要在getitab方法的最后一个参数上，看下getitab的具体实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116func getitab(inter *interfacetype, typ *_type, canfail bool) *itab { if len(inter.mhdr) == 0 { throw(\"internal error - misuse of itab\") } // easy case if typ.tflag&amp;tflagUncommon == 0 { if canfail { return nil } name := inter.typ.nameOff(inter.mhdr[0].name) panic(&amp;TypeAssertionError{\"\", typ.string(), inter.typ.string(), name.name()}) } h := itabhash(inter, typ) //根据inter和itab的type计算hash值 // look twice - once without lock, once with. // common case will be no lock contention. var m *itab var locked int for locked = 0; locked &lt; 2; locked++ { if locked != 0 { lock(&amp;ifaceLock) } for m = (*itab)(atomic.Loadp(unsafe.Pointer(&amp;hash[h]))); m != nil; m = m.link {//查询itab的hash表中是否已经包含itab和inter关联的itab记录 if m.inter == inter &amp;&amp; m._type == typ {//找到记录 if m.bad { if !canfail { //异常处理 // this can only happen if the conversion // was already done once using the , ok form // and we have a cached negative result. // the cached result doesn't record which // interface function was missing, so try // adding the itab again, which will throw an error. additab(m, locked != 0, false) } m = nil } if locked != 0 { unlock(&amp;ifaceLock) } return m //返回查找结果 } } } //hash表中未找到记录 m = (*itab)(persistentalloc(unsafe.Sizeof(itab{})+uintptr(len(inter.mhdr)-1)*sys.PtrSize, 0, &amp;memstats.other_sys)) //分配新的itab m.inter = inter m._type = typ additab(m, true, canfail) //加入itab的hash表 unlock(&amp;ifaceLock) if m.bad { return nil } return m}func additab(m *itab, locked, canfail bool) { inter := m.inter typ := m._type x := typ.uncommon() // both inter and typ have method sorted by name, // and interface names are unique, // so can iterate over both in lock step; // the loop is O(ni+nt) not O(ni*nt). ni := len(inter.mhdr) nt := int(x.mcount) xmhdr := (*[1 &lt;&lt; 16]method)(add(unsafe.Pointer(x), uintptr(x.moff)))[:nt:nt] j := 0 for k := 0; k &lt; ni; k++ { //inter方法列表遍历 i := &amp;inter.mhdr[k] itype := inter.typ.typeOff(i.ityp) name := inter.typ.nameOff(i.name) iname := name.name() ipkg := name.pkgPath() if ipkg == \"\" { ipkg = inter.pkgpath.name() } for ; j &lt; nt; j++ { //typ方法列表遍历 t := &amp;xmhdr[j] tname := typ.nameOff(t.name) if typ.typeOff(t.mtyp) == itype &amp;&amp; tname.name() == iname {// find the method pkgPath := tname.pkgPath() if pkgPath == \"\" { pkgPath = typ.nameOff(x.pkgpath).name() } if tname.isExported() || pkgPath == ipkg { if m != nil { ifn := typ.textOff(t.ifn) *(*unsafe.Pointer)(add(unsafe.Pointer(&amp;m.fun[0]), uintptr(k)*sys.PtrSize)) = ifn // 方法写入新生成的itab方法表 } goto nextimethod } } } // didn't find method if !canfail { if locked { unlock(&amp;ifaceLock) } panic(&amp;TypeAssertionError{\"\", typ.string(), inter.typ.string(), iname}) } m.bad = true //不匹配的itab break nextimethod: } if !locked { throw(\"invalid itab locking\") } h := itabhash(inter, typ) m.link = hash[h] m.inhash = true atomicstorep(unsafe.Pointer(&amp;hash[h]), unsafe.Pointer(m))} iface的实现中，包含了一个所有itab实例的hash表： 1234var ( ifaceLock mutex // lock for accessing hash hash [hashSize]*itab) itab可以视为一个pair，关联了一个抽象的接口类型interfacetype和一种具体类型实例type的pair。当执行类型断言时，首先根据目标interfacetype和当前接口itab中的type计算hash值，在hash表中，查找是否有相应的记录，如果有，表示之前存在当前接口的真实类型type到interfacetype的转换记录，返回对应的itab。否则，新建一个itab实例，调用additab加入hash表。 additab则首先比对interfacetype和type的方法列表是否一致，如果interfacetype包含type没有的方法，说明这个itab是一个不合法的实例，标记位bad。如果完全符合，则初始化新生成的itab的方法表fun（*(*unsafe.Pointer)(add(unsafe.Pointer(&amp;m.fun[0]), uintptr(k)*sys.PtrSize)) = ifn ），并加入hash表中。这就解释了前文iface fun列表的实现原理。其次，由于方法列表按照字典序排列，所以校验方法是否匹配的两层for循环其实只要O(m+n)而不是O(mn)的复杂度。 itab可以说是Go语言duck-typing的底层原理，其interfacetype和itab.type关联为type的设计，满足一个实例实现多个接口的功能和一个接口多个实现实例的功能。所以，下面实例的断言都是成功的： 123456789101112131415161718192021222324252627282930package mainimport \"fmt\"type Interface1 interface { Print()}type Interface2 interface { Print()}type Interface3 interface { Hello()}type Mystruct struct {}func (strct Mystruct) Print() {}func (strct Mystruct) Hello() {}func main() { var s Mystruct var i1 Interface1 i1 = s i2 := i1.(Interface2) //ture i3 := i1.(Interface3) //true fmt.Print(i2, i3)} nil判断代码中可能需要判断一个接口是否为空值，最常见的就是error接口： 123if err != nil { //} interface并不是一个指针，它的底层实现由两部分组成，一个是类型，一个值，也就是类似于：(Type, Value)。只有当类型和值都是nil的时候，才等于nil 123456789func do() error { // error(*doError, nil) var err *doError return err // nil of type *doError}func main() { err := do() fmt.Println(err == nil) //false} 123456789101112func do() *doError { // nil of type *doError return nil}func wrapDo() error { // error (*doError, nil) return do() // nil of type *doError}func main() { err := wrapDo() // error (*doError, nil) fmt.Println(err == nil) // false}","link":"/2019/01/04/go-interface/"},{"title":"一条更新语句在MySQL是怎么执行的","text":"本文试图从原理层面讲解一条更新语句在MySQL是怎么执行的。1update t set b = 200 where id = 2 语句的执行过程如下： 客户端（通常是你的服务）发出更新语句” update t set b = 200 where id = 2 “ 并向MySQL服务端建立连接； MySQL连接器负责和客户端建立连接，获取权限，维持和管理连接； MySQL拿到一个查询请求后，会先到查询缓存看看（MySQL8.x已经废弃了查询缓存），看之前是否已经执行过，如果执行过，执行语句及结果会以key-value形式存储到内存中，如果命中缓存会返回结果。如果没命中缓存，就开始真正执行语句。分析器会先做词法分析，识别出关键字update，表名等等；之后还会做语法分析，判断输入的语句是否符合MySQL语法； 经过分析器，MySQL已经知道语句是要做什么。优化器接着会选择使用哪个索引（如果多个表，会选择表的连接顺序）； MySQL服务端最后一个阶段是执行器会调用引擎的接口去执行语句； 事务开始（任何一个操作都是事务），写undo log ，记录记录上一个版本数据，并更新记录的回滚指针和事务ID； 执行器先调用引擎取id=2这一行。id是主键，引擎直接用树搜索找到这一行； 如果id=2这一行所在的数据页本来就在内存 中，就直接返回给执行器更新； 如果记录不在内存，接下来会判断索引是否是唯一索引； 如果不是唯一索引，InnoDB会将更新操作缓存在change buffer中； 如果是唯一索引，就只能将数据页从磁盘读入到内存，返回给执行； 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据； 引擎将这行数据更新到内存中，同时将这个更新操作记录到redo log 里面； 执行器生成这个操作的binlogbinlog ； 执行器调用引擎的提交事务接口； 事务的两阶段提交：commit的prepare阶段：引擎把刚刚写入的redo log刷盘； 事务的两阶段提交：commit的commit阶段：引擎binlog刷盘。 MySQL基本架构MySQL可以分为Server层和存储引擎层两部分。 Server层包括连接器、查询缓存、分析器、优化器、执行器。涵盖MySQL的大多数核心服务功能，以及所有的内置函数（日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 存储引擎层负责数据的存储和提取。其架构是插件式的，支持InnoDB,MyISAM,Memory等多个存储引擎。现在最常用的存储引擎是InnoDB, 它从MySQL5.5.5开始成为了默认存储引擎。 Undo log 简述【概述】Undo log 是InnoDB MVCC事务特性的重要组成部分。当我们对记录做了变更操作时就会产生undo记录，undo记录默认被记录到系统表ibdata中，但是从MySQL 5.6以后 也可以使用独立的Undo 表空间。 【作用】其作用是保存记录的老版本数据，当一个旧的事务需要读取数据时，为了能读取到老版本的数据，需要顺着undo链找到满足其可见性的记录。当版本链很长时，通常可以认为是个比较耗时的耗时操作。因此可以用来回滚，崩溃恢复，MVCC。大多数对数据的变更操作包括INSERT/DELETE/UPDATE，其中INSERT操作在事务提交前只对当前事务可见，因此产生的Undo日志可以在事务提交后直接删除，而对于UPDATE/DELETE则需要维护多版本信息，在InnoDB里，UPDATE和DELETE操作产生的Undo日志被归成一类，即update_undo。 【产生时机】事务开始之前，将当前的数据版本生成Undo log, Undo log也会产生redo log 来保证Undo log的可靠性。 【释放时机】当事务提交后，Undo log并不能立马被删除，而是放入待清理的链表，由purge 线程判断是否由其他事务在使用undo 段中表的上一个事务之前的版本信息，决定是否可以清理undo log的日志空间。 【存储结构】InnoDB采用回滚段的方式来维护Undo log的并发写入和持久化。回滚段实际上是一种Undo 文件组织方式，Undo内部由多个回滚段组成，即 Rollback segment，一共有128个，保存在ibdata系统表空间中，分别从resg slot0 - resg slot127，每一个resg slot，也就是每一个回滚段，内部由1024个undo segment 组成。为了便于管理和使用undo记录，在内存中维持了如下关键结构体对象：1.所有回滚段都记录在 trx_sys-&gt;rseg_array，数组大小为128，分别对应不同的回滚段；2.rseg_array 数组类型为trx_rseg_t，用于维护回滚段相关信息；3.每个回滚段对象trx_rseg_t 还要管理undo log信息，对应结构体为trx_undo_t, 使用多个链表来维护trx_undo_t信息；4.事务开启时，会专门给他指定一个回滚段，以后该事务用到的undo log页，就从该回滚段上分配；5.事务提交后，需要purge的回滚段会被放到purge队列上（purge_sys-&gt;purge_queue)。 Change Buffer简述123当需要更新一个数据页：1. 如果数据页在内存 — 直接更新2. 如果数据页不在内存，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘读入这个数据页了。在下次查询需要访问这个数据页时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式保证这个数据逻辑的正确性。 另外，虽然叫change buffer, 实际上此操作也是可以持久化的数据。将change buffer中的操作应用到原始数据页，得到最新结果的过程叫merge。除了访问这个数据页会触发merge 外，系统有后台线程会定期merge. 在db正常关闭的时候，也会执行merge。 — 如果能够将更新操作先记录在change buffer，减少读磁盘，更新语句的执行速度会得到明显的提升 。 使用场景Change buffer的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做purge之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。 因此对于写多读少的业务，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。这种业务模型常见的是账单类，日志类的系统。 反过来，假设一个业务的更新模式就是写入之后马上会做查询，那么即使满足了条件，将先更新记录在change buffer,但之后由于马上要访问这个数据页，会立即出发purge过程。这样随机访问IO的次数不会减少,反而增加了change buffer的维护代价，所以对于这种业务模式来说，change buffer反而起到了副作用。 另外，只有普通索引才能使用到change buffer, 唯一索引不能用。因为唯一索引每次都要将数据页读入内存判断唯一性，所以没必要使用change buffer了。 Redo log简述123保证事务的持久性。日志先行(WAL 先写日志，再写磁盘。)，即在持久化数据文件前，保证之前的redo 日志已经写在磁盘。记录的是新数据的备份。在事务提交前，只要将Redo Log持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是RedoLog已经持久化。系统可以根据RedoLog的内容，将所有数据恢复到最新的状态。具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里，并更新内存[具体操作参见change buffer]，这个时候更新就算完成了。同时Innodb引擎会在适当的时候，将这个操作记录更新到磁盘里，而这个更新往往是在系统比较空闲的时候做。（redo log 类似MQ解耦，异步操作，把随机IO的写磁盘变成了顺序IO的写日志。） WAL好处：1.利用WAL技术，数据库将随机写换成了顺序写，大大提升了数据库性能。2.保证crash safe : 有了redo log 可以保证即使数据库发生异常重启，之前提交的记录都不会丢失。 WAL坏处：但是也会带来内存脏页问题，内存脏页会后台线程自动flush,也会由于数据页淘汰而触发flush. flush脏页的过程会占用资源，可能导致查询语句的响应时间长一些。 Redo log 特点 InnoDB的redo log是固定大小的，比如可以配置为一组4个文档，每个1GB，从头开始写，写到末尾就又回到开头循环写。redo log通过使用两个指针checkpoint&amp;writepos来控制数据更新到数据文件速度。另外，redo log是InnoDB引擎特有的日志。 WAL /redo log V.S. change bufferWAL /redo log 提升性能的核心机制即尽量减少随机写磁盘的IO消耗（转成顺序写）。而Change buffer 的提升性能的核心机制是节省更新语句中随机读磁盘的IO消耗 。 两阶段提交2PC2PC即Innodb对于事务的两阶段提交机制。当MySQL开启binlog的时候，会存在一个内部XA的问题：事务在存储引擎层（redo）commit的顺序和在binlog中提交的顺序不一致的问题。如果不使用两阶段提交，那么数据库的状态有可能用它的日志恢复出来的库的状态不一致。 事务的commit分为prepare和commit两个阶段：1、prepare阶段：redo持久化到磁盘（redo group commit），并将回滚段置为prepared状态，此时binlog不做操作。2、commit阶段：innodb释放锁，释放回滚段，设置提交状态，binlog持久化到磁盘，然后存储引擎层提交。 参考 丁奇 《MySQL实战45讲》 数据库内核月报 - MySQL 引擎特性 InnoDB undo log 漫游 两阶段提交","link":"/2019/02/08/how-update-executes-in-mysql/"},{"title":"Hive 源码解析之 Hive 基本框架和执行入口","text":"Hive简介在介绍Hive的框架和执行流程之前，这里首先对Hive进行简要的介绍。 Hive 是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据抽取，转化，加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为Hive QL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。 这里假设已经对Hive的各个组成部分、作用以及Hive QL语言有了基本的认识，不再做详细的解释，更加关注的是Hive源码级别的解析。下面是从网络上摘取的两个关于Hive框架的解析图： 从框架图中我们可以看见从用户提交一个查询（假设通过CLI入口）直到获取最终结果，Hive内部的执行流程主要包括： CLI 获取用户查询，解析用户输入的命令，提交给Driver； Driver 结合编译器（COMPILER）和元数据库（METASTORE），对用户查询进行编译解析； 根据解析结果（查询计划）生成MR任务提交给Hadoop执行； 获取最终结果； 源码分析我们试图根据Hive的源码对上述过程的每一步进行解析, 使用的源码版本1.1.0。话不多说，首先看看CLI如何解析用户的输入，并提交给Driver类执行的。这个过程主要涉及的类是org\\apache\\hadoop\\hive\\cli\\CliDriver.java。 main入口执行入口，main函数，创建CliDriver实例，接受用户输入参数，开始运行。 1234public static void main(String[] args) throws Exception { int ret = new CliDriver().run(args); System.exit(ret);} 这里用到了创建CliDriver实例，看看CliDriver的构造函数内部都做了什么操作： 123456public CliDriver() { SessionState ss = SessionState.get(); conf = (ss != null) ? ss.getConf() : new Configuration(); Log LOG = LogFactory.getLog(&quot;CliDriver&quot;); console = new LogHelper(LOG); } 首先获取一个SessionState， SessionState封装了一个会话的关联的数据，包括配置信息HiveConf，输入输出流，指令类型，用户名称、IP地址等等。SessionState 是一个与线程关联的静态本地变量ThreadLocal，任何一个线程都对应一个SessionState，能够在Hive代码的任何地方获取到（大量被使用到），以返回用户相关或者配置信息等。12345678910111213141516171819202122private static ThreadLocal&lt;SessionState&gt; tss = new ThreadLocal&lt;SessionState&gt;();public static SessionState get() { return tss.get(); }public static SessionState start(HiveConf conf) { //创建一个SessionState SessionState ss = new SessionState(conf); return start(ss); }public static SessionState start(SessionState startSs) { setCurrentSessionState(startSs); .....}public static void setCurrentSessionState(SessionState startSs) { //将SessionState与线程本地变量tss关联 tss.set(startSs); Thread.currentThread().setContextClassLoader(startSs.getConf().getClassLoader()); } 接着CliDriver的构造函数来说，获取到SessionState之后，就初始化配置信息org.apache.hadoop.conf.Configuration conf. runCliDriver实例创建完毕，调用run（args）, 开始处理用户输入。run方法的函数体比较长，为了方便阅读，下面按照代码的出现顺序，依次解析。 step 1. 对输入的指令进行初步解析，提取-e -h hiveconf hivevar等参数信息，设置用户提供的系统和Hive环境变量。详细实现，参考OptionsProcessor类，不再详细描述。 12345OptionsProcessor oproc = new OptionsProcessor();if (!oproc.process_stage1(args)) { return 1; } step 2. 初始化Log4j日志组件 12345678boolean logInitFailed = false;String logInitDetailMessage;try { logInitDetailMessage = LogUtils.initHiveLog4j();} catch (LogInitializationException e) { logInitFailed = true; logInitDetailMessage = e.getMessage();} step 3. 初始化HiveConf，并根据HiveConf实例化CliSessionState，设置输入输出流为标准控制台。 CliSessionState 继承了SessionState类，创建了一些记录用户输入的字符串，在实例化的过程中，主要是用来记录HiveConf，并生成一个会话ID，参见SessionState构造函数. 123456789101112CliSessionState ss = new CliSessionState(new HiveConf(SessionState.class));ss.in = System.in;try { ss.out = new PrintStream(System.out, true, &quot;UTF-8&quot;); ss.info = new PrintStream(System.err, true, &quot;UTF-8&quot;); ss.err = new CachingPrintStream(System.err, true, &quot;UTF-8&quot;); }catch (UnsupportedEncodingException e) { return 3;} step 4. 根据stage1解析的参数内容，填充CliSessionState的字符串，比如用户输入了-e 则这个stage就把-e 对应的字符串赋值给CliSessionState的 execString成员。 123if (!oproc.process_stage2(ss)) { return 2;} step 5. 在允许打印输出的模式下，如果日志初始化失败，打印失败信息 1234567if (!ss.getIsSilent()) { if (logInitFailed) { System.err.println(logInitDetailMessage); } else { SessionState.getConsole().printInfo(logInitDetailMessage); }} step 6. 将用户命令行输入的配置信息和变量等，覆盖HiveConf的默认值 12345HiveConf conf = ss.getConf(); for (Map.Entry&lt;Object, Object&gt; item : ss.cmdProperties.entrySet()) { conf.set((String) item.getKey(), (String) item.getValue()); ss.getOverriddenConfigurations().put((String) item.getKey(), (String) item.getValue()); } step 7. 设置当前回话状态，执行CLI驱动 12345678SessionState.start(ss);try { return executeDriver(ss, conf, oproc);} finally { ss.close(); }} executeDriver在进入executeDriver之前，我们可以认为Hive处理的是用户进入Hive程序的指令，到此用户已经进入了Hive，Cli的Driver将不断读取用户的HiveQL语句并解析，提交给Driver。executeDriver函数内部出了根据用户参数做出的一些执行响应外，还设置了用户HiveQL的执行历史记录，也就是方便我们使用上下标键查看之前执行的指令的功能，不再详述。executeDriver函数内部核心的代码是通过while循环不断按行读取用户的输入，然后调用ProcessLine拼接一条命令cmd，传递给processCmd处理用户输入。下面就来看看processCmd函数。 processCmd 首先是设置当前clisession的用户上一条指令，然后使用正则表达式，将用户输入的指令从空格，制表符等出断开（tokenizeCmd函数），得到token数组。 1234567CliSessionState ss = (CliSessionState) SessionState.get();ss.setLastCommand(cmd);// Flush the print stream, so it doesn&apos;t include output from the last commandss.err.flush();String cmd_trimmed = cmd.trim();String[] tokens = tokenizeCmd(cmd_trimmed); 然后根据用户的输入，进行不同的处理，这边的处理主要包括： quit或exit： 关闭回话，退出hive source： 文件处理？不清楚对应什么操作 ! 开头： 调用Linux系统的shell执行指令 本地模式：创建CommandProcessor, 执行用户指令 限于篇幅原因，前面三种情况的代码不再详述，重点介绍Hive的本地模式执行，也就是我们常用的HiveQL语句，DFS命令等的处理方式： 12345678try { CommandProcessor proc = CommandProcessorFactory.get(tokens, (HiveConf) conf); ret = processLocalCmd(cmd, proc, ss);} catch (SQLException e) { console.printError(&quot;Failed processing command &quot; + tokens[0] + &quot; &quot; + e.getLocalizedMessage(), org.apache.hadoop.util.StringUtils.stringifyException(e)); ret = 1;} 其中，CommandProcessor是一个接口类，定义如下:1234public interface CommandProcessor { void init(); CommandProcessorResponse run(String command) throws CommandNeedRetryException;} CommandProcessorFactory根据用户指令生成的tokens和配置文件，返回CommandProcessor的一个具体实现。123456789101112131415161718192021public static CommandProcessor get(String[] cmd, HiveConf conf) throws SQLException { CommandProcessor result = getForHiveCommand(cmd, conf); if (result != null) { return result; } if (isBlank(cmd[0])) { return null; } else { if (conf == null) { return new Driver(); } Driver drv = mapDrivers.get(conf); if (drv == null) { drv = new Driver(); mapDrivers.put(conf, drv); } drv.init(); return drv; } } 其中getForHiveCommand函数首先根据tokens的第一个字串，也就是用户输入指令的第一个单词，在HiveCommand这个enum中定义的一些非SQL查询操作集合中进行匹配，确定相应的HiveCommand类型。在依据HiveCommand选择合适的CommandProcessor实现方式，比如dfs命令对应的DFSProcessor，set命令对应的SetProcessor等，如果用户输入的是诸如select之类的SQL查询， getForHiveCommand返回null，直接在get函数中根据配置文件conf选择或者生成一个Driver类实例，并作为CommandProcessor返回。详细的代码参考CommandProcessorFactory和HiveCommand类。 processLocalCmd到此Hive对用户的一个指令cmd，配置了回话状态CliSessionState，选择了一个合适的CommandProcessor， CliDriver将进行他的最后一步操作，提交用户的查询到指定的CommandProcessor，并获取结果。这一切都是在processLocalCmd中执行的。 processLocalCmd函数的主体是一个如下的循环：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788do { try { needRetry = false; if (proc != null) { //如果CommandProcessor是Driver实例 if (proc instanceof Driver) { Driver qp = (Driver) proc; //获取标准输出流，打印结果信息 PrintStream out = ss.out; long start = System.currentTimeMillis(); if (ss.getIsVerbose()) { out.println(cmd); } qp.setTryCount(tryCount); //driver实例运行用户指令，获取运行结果响应码 ret = qp.run(cmd).getResponseCode(); if (ret != 0) { qp.close(); return ret; } // 统计指令的运行时间 long end = System.currentTimeMillis(); double timeTaken = (end - start) / 1000.0; ArrayList&lt;String&gt; res = new ArrayList&lt;String&gt;(); //打印查询结果的列名称 printHeader(qp, out); // 打印查询结果 int counter = 0; try { if (out instanceof FetchConverter) { ((FetchConverter)out).fetchStarted(); } while (qp.getResults(res)) { for (String r : res) { out.println(r); } counter += res.size(); res.clear(); if (out.checkError()) { break; } } } catch (IOException e) { console.printError(&quot;Failed with exception &quot; + e.getClass().getName() + &quot;:&quot; + e.getMessage(), &quot;\\n&quot; + org.apache.hadoop.util.StringUtils.stringifyException(e)); ret = 1; } //关闭结果 int cret = qp.close(); if (ret == 0) { ret = cret; } if (out instanceof FetchConverter) { ((FetchConverter)out).fetchFinished(); } console.printInfo(&quot;Time taken: &quot; + timeTaken + &quot; seconds&quot; + (counter == 0 ? &quot;&quot; : &quot;, Fetched: &quot; + counter + &quot; row(s)&quot;)); } else { //如果proc不是Driver，也就是用户执行的是非SQL查询操作，直接执行语句，不自信FetchResult的操作 String firstToken = tokenizeCmd(cmd.trim())[0]; String cmd_1 = getFirstCmd(cmd.trim(), firstToken.length()); if (ss.getIsVerbose()) { ss.out.println(firstToken + &quot; &quot; + cmd_1); } CommandProcessorResponse res = proc.run(cmd_1); if (res.getResponseCode() != 0) { ss.out.println(&quot;Query returned non-zero code: &quot; + res.getResponseCode() + &quot;, cause: &quot; + res.getErrorMessage()); } ret = res.getResponseCode(); } } } catch (CommandNeedRetryException e) { //如果执行过程中出现异常，修改needRetry标志，下次循环是retry。 console.printInfo(&quot;Retry query with a different approach...&quot;); tryCount++; needRetry = true; } } while (needRetry); 前面对函数中关键的执行语句已经给出了注释，这里单独对printHeader进行一下说明。printHeader函数通过调用driver.getSchema.getFiledSchema，获取查询结果的列集合 ，然后依次打印出列名。12345678910111213141516private void printHeader(Driver qp, PrintStream out) { List&lt;FieldSchema&gt; fieldSchemas = qp.getSchema().getFieldSchemas(); if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CLI_PRINT_HEADER) &amp;&amp; fieldSchemas != null) { // Print the column names boolean first_col = true; for (FieldSchema fs : fieldSchemas) { if (!first_col) { out.print(&apos;\\t&apos;); } out.print(fs.getName()); first_col = false; } out.println(); } }","link":"/2016/03/20/hive0/"},{"title":"JVM调试工具入门","text":"上周末连续两天凌晨都收到了系统的内存使用率过高报警，在分析监控系统记录的内存使用率曲线和内存使用情况后发现，主要是因为在老年代迟迟没有触发full gc导致监控系统连续多次监测到可用内存过低，而触发的报警。在系统触发一次full gc之后，内存使用率会显著下降，报警也没有持续下去。由于无法复现问题，具体原因仍未找到，但是通过此过程，学习到的内存分析工具与方法，却值得记录一番。 jstat The jstat tool displays performance statistics for an instrumented HotSpot Java virtual machine (JVM). jstat是HotSpot Java虚拟机的性能统计工具。其基本的语法描述如下: jstat [generalOption | outputOptions | vmid [interval[s|ms] [count]]] generalOptiongeneralOption是针对jstat功能的描述，包括两个参数 -help 与 -options ，分别用于提示jstat的用法和支持的统计选项。该选项具有排他性，只能单独使用。 outputOptionsoutputOptions包括两类参数：状态统计 和 格式化输出。状态统计参数用于指定jstat命令希望获取虚拟机哪方面的信息，而格式化参数则用于控制命令输出的展示样式。 jstat支持的状态统计参数（即jstat -options的输出）及功能描述如下: -class 统计类加载行为 -complier 统计HotSpot即时编译器的行为 -gc 统计关于堆内存垃圾回收的行为 -gccapacity 统计堆内存中各分区的使用情况 -gccause 垃圾回收行为汇总，比-gcutil多输出最近两次垃圾回收的原因 -gcnew,-gcold 新生代，老年代行为信息（内存量，阈值，垃圾回收次数等） -gcnewcapacity 新生代内存容量和使用量信息 -gcoldcapacity 老年代内存容量和使用量信息 -gcpermcapacity 持久区内存容量和使用量信息 -gcutil 垃圾回收行为汇总 -printcompilation HotSpot编译方法统计 格式化参数包括三个：-h n ，-t 和 -JjavaOption. -h参数指定每隔n行重新显示一次列名;-t参数控制在输出第一列添加时间戳信息;-JjavaOption用于传递javaOption到java程序启动参数。比如，-J-Xms48m 设置java启动最小内存为48M。 vmid Virtual machine identifier, a string indicating the target Java virtual machine (JVM). vmid是待监测的目标java程序标识符，可用 jps 和Linux系统下的 ps 等操作获取。vmid参数也支持以URI形式指定的远程主机上运行的java程序，不常用，不再赘述。 interval and count这两个参数用于控制jstat命令监测并输出的频率，interval默认参数为毫秒，如果设置了该参数，jstat命令将每隔interval的时间输出一次，count控制jstat命令输出样例的个数，也就是输出的行数。如果不设置，默认为无限，jstat会一直进行输出直到目标程序退出或者jstat命令终止。 样例使用 -gcutil 参数查看进程16058发生的垃圾回收行为，每2秒打印一次结果，一共打印5次。命令输出的每一列都使用简称的形式展示，下图中 S0 表示Survivor 0区的空间使用比例， E, O, P 分别代表Eden, Old和Perm空间使用率，YGC 表示young gc的次数，YGCT 表示young gc消耗的时间。GCT 则用来统计执行gc的总时间。 使用 -gc 参数查看更详细的垃圾回收与堆内存信息。 S0 以及 E 等各内存区域标识后缀中 C 表示 Capacity, U 表示Utilization，参数对应的数值显示的是真实容量，而不是百分比。 使用 -gcnew 参数查看新生代的内存和垃圾回收情况，由于命令附带了参数 t ，所以第一列打印了时间戳的信息。 TT 参数表示对象在gc时被放入老年代的年龄期限阈值，MTT 参数表示最大阈值. 这两个参数之间 MTT 设定了 TT 可取的最大值，TT 实际控制着对象进入老年代的年龄限制，会随着垃圾回收过程而发生变化。 当年龄从1开始的对象大小累计超过了Survivor区域的1/2(TargetSurvivorRatio所定义)时，会计算一个Thenuring Threshold，超过这个年龄的新生代对象会进入到老年代，即使这时候新生代还有很多的空间。注意MaxTenuringThreshold只是设置了最大的Thenuring Threshold，不是说只有大于Max Tenuring Threshold才会进入到老年代，而是只要超过了计算出来的Tenuring Threshold就会进入老年代，MaxTenuringThreshold规定了Tenuring Threshold的最大值而已。Tenuring Threshold这个值在每一轮GC后都会动态计算，它与TargetSurvivorRatio以及Survivor区的大小有关系，TargetSurivivor默认是50即Survivor的1/2, 会计算出一个Desired Survivor Size，当年龄从1开始的对象大小累计超过了这个Desired Survivor Size，那么这个age就是Tenuring Threshold的值 使用 -gcnewcapacity 参数查看新生代的内存占用情况。NGC : new generation capacity, 新生代内存大小。NGCMN 表示新生代分配内存的最小值，NGCMX 新生代分配内存的最大值，NGCMX=S0CMX+S1CMX+ECMX. jstackjstack用于打印指定java进程或者核心文件中所有java线程当前时刻正在执行的方法堆栈追踪情况，也就是线程的snapshot。生成线程的快照主要用于定位线程长时间出现停顿的原因：如死锁，等待外部资源等。jstack的命令格式如下： jstack [option] pid/executable core/[server-id@]remote-hostname-or-IP pid待追踪的java进程ID，可使用jps等方式获得 executable产生core dump的java可执行程序(jar文件) core打印栈追踪信息的核心文件 options-F 当正常输出的请求(jstack [-l] pid)不被响应时，强制执行stack dump -l 除了堆栈外，打印关于锁的附加信息 -m 如果调用本地方法，同时打印java和本地C/C++栈帧 -h 打印帮助 jstack在分析死锁，阻塞等性能问题上非常有用，根据打印的堆栈信息可以定位到出问题的代码段。定位问题的思路根据要解决的问题而发生不同，比如可以首先找到java进程中最耗cpu的线程，再根据线程id在jstack的输出中定位，或者使用指定的线程名称定位。下面看一下jstack的输出格式： main 线程名称 prio=5 线程优先级为5 tid=7f8a7c001800 jvm中线程标识符 nid=0x700000a19000 16进制表示的本地线程标识符 runnnable 线程状态 [70000a17000] 线程的起始地址 从第二行起为函数调用栈 上述信息中最重要的状态莫过于线程的状态，当程序发生问题时往往能够据此帮我们找到问题的所在。在jstack的输出中，线程所处的状态包括： Runnable: 正在运行 Wait on condition: 该状态出现在线程等待某个条件的发生。具体是什么原因，可以结合 stacktrace来分析。最常见的情况是线程在等待网络的读写。如果网络数据没准备好，线程就等待在那里。另外一种出现 Wait on condition的常见情况是该线程在 sleep，等待 sleep的时间到了时候，将被唤醒。 Wait for monitor entry: Java通过对象监视器来进行线程的互斥与同步的，每个对象都有一个对象监视器，在对对象加锁的过程中，任何时刻只有一个线程拥有这个对象监视器，其他请求获得此资源的线程将会被分为两种：在线程获取到对象监视器，但等待的资源仍未到达时，线程可能调用Object.wait()，释放锁并进入Object.wait()的状态，重新等待；而那些从未获得过此对象监视器的线程就将被标识为Wait for monitor entry的状态。 Object.wait: 等待对象监视器（锁）的状态。关于两种等待状态可以参考下面的图（来源于网络）： jmapjmap用于生成java进程的heapdump或者堆内存的详细信息。可以用来分析java程序堆内存被各种实例占据的比例或者GC回收了哪些对象等信息。jmap的命令格式与jstack一致，不再赘述。 options&lt;no option&gt; 打印每一个共享对象的起始地址，范围等信息 -dump:\\[live,]format=b,file=&lt;filename&gt; 以二进制形式打印java堆的dump信息到指定文件中，指定live参数，只打印存活的对象 -heap 显示java堆的详细信息：GC算法，堆配置以及分代情况 -histo\\[:live] 显示堆中每一个java类实例的个数，占据空间的大小，类名全称等。live参数控制输出存活对象。 -premstat 以classLoader为统计入口，显示永久代的生存状态。每个加载器的名称，存活状态，地址，父加载器和已经加载类的数量等信息将被打印。 -F 在正常的命令对-dump或者-histo没有响应时，强制执行，生成dump信息 -J&lt;flag&gt; map启动时传递给jvm的参数，比如在64位机器上就要使用jmap -J-d64 -heap pid来执行命令。 参考链接1.http://docs.oracle.com/javase/7/docs/technotes/tools/share/jstat.html2.http://docs.oracle.com/javase/7/docs/technotes/tools/share/jstack.html3.http://docs.oracle.com/javase/7/docs/technotes/tools/share/jmap.html4.https://my.oschina.net/feichexia/blog/1965755.http://go-on.iteye.com/blog/16738946.http://blog.csdn.net/iter_zc/article/details/418023657.《深入理解JVM虚拟机》周志明著","link":"/2016/10/17/jvm-debug/"},{"title":"JVM以及垃圾回收器的工作原理","text":"Java 虚拟机（Java virtual machine，JVM）是运行 Java 程序必不可少的机制。 JVM实现了Java语言最重要的特征：即平台无关性。这是因为：编译后的 Java 程序指令并不直接在硬件系统的 CPU 上执行，而是由 JVM 执行。JVM屏蔽了与具体平台相关的信息，使Java语言编译程序只需要生成在JVM上运行的目标字节码（.class）,就可以在多种平台上不加修改地运行。Java 虚拟机在执行字节码时，把字节码解释成具体平台上的机器指令执行。因此实现java平台无关性。它是 Java 程序能在多平台间进行无缝移植的可靠保证，同时也是 Java 程序的安全检验引擎（还进行安全检查）。 JVM是编译后的 Java 程序（.class文件）和硬件系统之间的接口 （ 编译后：javac 是收录于 JDK 中的 Java 语言编译器。该工具可以将后缀名为. java 的源文件编译为后缀名为. class 的可以运行于 Java 虚拟机的字节码。） JVM architecture 图片摘自 httpjavapapers.comjavajava-garbage-collection-introduction JVM = 类加载器 classloader + 执行引擎 execution engine + 运行时数据区域 runtime data areaclassloader 把硬盘上的class 文件加载到JVM中的运行时数据区域, 但是它不负责这个类文件能否执行，而这个是 执行引擎 负责的。 classloader作用：装载.class文件 classloader 有两种装载class的方式 （时机）： 隐式：运行过程中，碰到new方式生成对象时，隐式调用classLoader到JVM 显式：通过class.forname()动态加载 双亲委派模型（Parent Delegation Model）： 类的加载过程采用双亲委托机制，这种机制能更好的保证 Java 平台的安全。该模型要求除了顶层的Bootstrap class loader启动类加载器外，其余的类加载器都应当有自己的父类加载器。子类加载器和父类加载器不是以继承（Inheritance）的关系来实现，而是通过组合（Composition）关系来复用父加载器的代码。每个类加载器都有自己的命名空间（由该加载器及所有父类加载器所加载的类组成，在同一个命名空间中，不会出现类的完整名字（包括类的包名）相同的两个类；在不同的命名空间中，有可能会出现类的完整名字（包括类的包名）相同的两个类）。 双亲委派模型的工作过程为： 1.当前 ClassLoader 首先从自己已经加载的类中查询是否此类已经加载，如果已经加载则直接返回原来已经加载的类。 每个类加载器都有自己的加载缓存，当一个类被加载了以后就会放入缓存，等下次加载的时候就可以直接返回了。 2.当前 classLoader 的缓存中没有找到被加载的类的时候，委托父类加载器去加载，父类加载器采用同样的策略，首先查看自己的缓存，然后委托父类的父类去加载，一直到 bootstrap ClassLoader. 3.当所有的父类加载器都没有加载的时候，再由当前的类加载器加载，并将其放入它自己的缓存中，以便下次有加载请求的时候直接返回。 使用这种模型来组织类加载器之间的关系的好处主要是为了安全性，避免用户自己编写的类动态替换 Java 的一些核心类，比如 String，同时也避免了重复加载，因为 JVM 中区分不同类，不仅仅是根据类名，相同的 class 文件被不同的 ClassLoader 加载就是不同的两个类，如果相互转型的话会抛java.lang.ClassCaseException. 类加载器 classloader 是具有层次结构的，也就是父子关系。其中，Bootstrap 是所有类加载器的父亲。如下图所示： Bootstrap class loader： 父类当运行 java 虚拟机时，这个类加载器被创建，它负责加载虚拟机的核心类库，如 java.lang. 等。例如 java.lang.Object 就是由根类加载器加载的。需要注意的是，这个类加载器不是用 java 语言写的，而是用 CC++ 写的。 Extension class loader这个加载器加载出了基本 API 之外的一些拓展类。 AppClass Loader加载应用程序和程序员自定义的类。 除了以上虚拟机自带的加载器以外，用户还可以定制自己的类加载器（User-defined Class Loader）。Java 提供了抽象类 java.lang.ClassLoader，所有用户自定义的类加载器应该继承 ClassLoader 类。 这是JVM分工自治生态系统的一个很好的体现。 引用：http://www.importnew.com6581.html 执行引擎作用： 执行字节码，或者执行本地方法 运行时数据 JVM 运行时数据区 (JVM Runtime Area) 其实就是指 JVM 在运行期间，其对JVM内存空间的划分和分配。JVM在运行时将数据划分为了6个区域来存储。 程序员写的所有程序都被加载到运行时数据区域中，不同类别存放在heap, java stack, native method stack, PC register, method area. 下面对各个部分的功能和存储的内容进行描述： 1、PC程序计数器：一块较小的内存空间，可以看做是当前线程所执行的字节码的行号指示器, NAMELY存储每个线程下一步将执行的JVM指令，如该方法为native的，则PC寄存器中不存储任何信息。Java 的多线程机制离不开程序计数器，每个线程都有一个自己的PC，以便完成不同线程上下文环境的切换。 2、java虚拟机栈：与 PC 一样，java 虚拟机栈也是线程私有的。每一个 JVM 线程都有自己的 java 虚拟机栈，这个栈与线程同时创建，它的生命周期与线程相同。虚拟机栈描述的是Java 方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 3、本地方法栈：与虚拟机栈的作用相似，虚拟机栈为虚拟机执行执行java方法服务，而本地方法栈则为虚拟机使用到的本地方法服务。 4、Java堆：被所有线程共享的一块存储区域，在虚拟机启动时创建，它是JVM用来存储对象实例以及数组值的区域，可以认为Java中所有通过new创建的对象的内存都在此分配。 Java堆在JVM启动的时候就被创建，堆中储存了各种对象，这些对象被自动管理内存系统（Automatic Storage Management System，也即是常说的 “Garbage Collector（垃圾回收器）”）所管理。这些对象无需、也无法显示地被销毁。 JVM将Heap分为两块：新生代New Generation和旧生代Old Generation Note: 堆在JVM是所有线程共享的，因此在其上进行对象内存的分配均需要进行加锁，这也是new开销比较大的原因。 鉴于上面的原因，Sun Hotspot JVM为了提升对象内存分配的效率，对于所创建的线程都会分配一块独立的空间，这块空间又称为TLAB TLAB仅作用于新生代的Eden Space，因此在编写Java程序时，通常多个小的对象比大的对象分配起来更加高效 5、方法区 方法区和堆区域一样，是各个线程共享的内存区域，它用于存储每一个类的结构信息，例如运行时常量池，成员变量和方法数据，构造函数和普通函数的字节码内容，还包括一些在类、实例、接口初始化时用到的特殊方法。当开发人员在程序中通过Class对象中的getName、isInstance等方法获取信息时，这些数据都来自方法区。 方法区也是全局共享的，在虚拟机启动时候创建。在一定条件下它也会被GC。这块区域对应Permanent Generation 持久代。 XX：PermSize指定大小。 6、运行时常量池其空间从方法区中分配，存放的为类中固定的常量信息、方法和域的引用信息。 GCjavapapers Java garbage collection is an automatic process to manage the runtime memory used by programs. By doing it automatic JVM relieves the programmer of the overhead of assigning and freeing up memory resources in a program. java 与 C语言相比的一个优势是，可以通过自己的JVM自动分配和回收内存空间。 何为GC？ 垃圾回收机制是由垃圾收集器Garbage Collection GC来实现的，GC是后台的守护进程。它的特别之处是它是一个低优先级进程，但是可以根据内存的使用情况动态的调整他的优先级。因此，它是在内存中低到一定限度时才会自动运行，从而实现对内存的回收。这就是垃圾回收的时间不确定的原因。 为何要这样设计：因为GC也是进程，也要消耗CPU等资源，如果GC执行过于频繁会对java的程序的执行产生较大的影响（java解释器本来就不快），因此JVM的设计者们选着了不定期的gc。 GC有关的是 runtime data area 中的 heap（对象实例会存储在这里） 和 gabage collector方法。 程序运行期间，所有对象实例存储在运行时数据区域的heap中，当一个对象不再被引用（使用），它就需要被收回。在GC过程中，这些不再被使用的对象从heap中收回，这样就会有空间被循环利用。 GC为内存中不再使用的对象进行回收，GC中调用回收的方法–收集器garbage collector. 由于GC要消耗一些资源和时间，Java 在对对象的生命周期特征（eden or survivor）进行分析之后，采用了分代的方式进行对象的收集，以缩短GC对应用造成的暂停。 在垃圾回收器回收内存之前，还需要一些清理工作。 因为垃圾回收gc只能回收通过new关键字申请的内存（在堆上），但是堆上的内存并不完全是通过new申请分配的。还有一些本地方法（一般是调用的C方法）。这部分“特殊的内存”如果不手动释放，就会导致内存泄露，gc是无法回收这部分内存的。所以需要在finalize中用本地方法(native method)如free操作等，再使用gc方法。显示的GC方法是system.gc() 垃圾回收技术方法一：引用计数法。简单但速度很慢。缺陷是：不能处理循环引用的情况。 方法二：停止-复制(stop and copy)。效率低，需要的空间大，优点，不会产生碎片。 方法三：标记 - 清除算法 (mark and sweep)。速度较快，占用空间少，标记清除后会产生大量的碎片。 JAVA虚拟机中是如何做的？ java的做法很聪明，我们称之为自适应的垃圾回收器，或者是自适应的、分代的、停止-复制、标记-清扫式垃圾回收器。它会根据不同的环境和需要选择不同的处理方式。 heap组成由于GC需要消耗一些资源和时间的，Java在对对象的生命周期特征进行分析后，采用了分代的方式来进行对象的收集，即按照新生代、旧生代的方式来对对象进行收集，以尽可能的缩短GC对应用造成的暂停.heap 的组成有三区域世代：(可以理解随着时间，对象实例不断变换heap中的等级，有点像年级) 新生代 Young Generation Eden Space 任何新进入运行时数据区域的实例都会存放在此 S0 Suvivor Space 存在时间较长，经过垃圾回收没有被清除的实例，就从Eden 搬到了S0 S1 Survivor Space 同理，存在时间更长的实例，就从S0 搬到了S1 旧生代 Old Generationtenured同理，存在时间更长的实例，对象多次回收没被清除，就从S1 搬到了tenured Perm 存放运行时数据区的方法区 Java 不同的世代使用不同的 GC 算法。 Minor collection：新生代 Young Generation 使用将 Eden 还有 Survivor 内的数据利用 semi-space 做复制收集（Copying collection）， 并将原本 Survivor 内经过多次垃圾收集仍然存活的对象移动到 Tenured。 Major collection 则会进行 Minor collection，Tenured 世代则进行标记压缩收集。 To note that 这个搬运工作都是GC完成的，这也是garbage collector 的名字来源，而不是叫garbage cleaner. GC负责在heap中搬运实例，以及收回存储空间。 GC工作原理JVM 分别对新生代和旧生代采用不同的垃圾回收机制 何为垃圾？Java中那些不可达的对象就会变成垃圾。那么什么叫做不可达？其实就是没有办法再引用到该对象了。主要有以下情况使对象变为垃圾： 1.对非线程的对象来说，所有的活动线程都不能访问该对象，那么该对象就会变为垃圾。 2.对线程对象来说，满足上面的条件，且线程未启动或者已停止。 例如：(1)改变对象的引用，如置为null或者指向其他对象。 1234Object x=new Object();object1Object y=new Object();object2x=y;object1 变为垃圾x=y=null;object2 变为垃圾 (2)超出作用域 if(i==0){ Object x=new Object();object1 }//括号结束后object1将无法被引用，变为垃圾 (3)类嵌套导致未完全释放 class A{ A a; } A x= new A();//分配一个空间 x.a= new A();//又分配了一个空间 x=null;//将会产生两个垃圾 (4)线程中的垃圾 class A implements Runnable{ void run(){ …. } } main A x=new A();object1 x.start(); x=null;//等线程执行完后object1才被认定为垃圾 这样看，确实在代码执行过程中会产生很多垃圾，不过不用担心，java可以有效地处理他们。 JVM中将对象的引用分为了四种类型，不同的对象引用类型会造成GC采用不同的方法进行回收： （1）强引用：默认情况下，对象采用的均为强引用 （GC不会回收） （2）软引用：软引用是Java中提供的一种比较适合于缓存场景的应用 （只有在内存不够用的情况下才会被GC） （3）弱引用：在GC时一定会被GC回收 （4）虚引用：在GC时一定会被GC回收","link":"/2015/03/05/jvmOutline/"},{"title":"Lambda表达式详解","text":"编写简洁而灵活的代码，能够轻松地适应不断变化的需求 导读： 通过本文，你可以了解为什么Jdk1.8引入lambda表达式，一些设计思想；还有你怎么使用lambda表达式让你的代码更加优雅一些。另外，你还可以接触一点流(Stream)和并行开发的知识. jdk1.8于2014年3月18日发布，是目前(2017年8月)jdk最新以及最重要的版本。相对于之前的jdk版本，1.8对于语言，编译器，库，工具和JVM等多个方面的进行了提升。本文主要聊聊Jdk1.8新提出的lambda表达式。 为什么要引入lambda表达式在了解lambda表达式的用法之前，不如先弄清为什么要引入lambda表达式。关于此问题，现在有几类主流解释： 1. 将函数式编程优点添加到面向对象的Java中众所周知，Java是面向对象语言（遥记得在上大学时候，Java女老师那句经典的“万事万物皆对象”）。的确，Java语言发展这么多年来一直是“重对象，轻函数”的设计理念。函数对于Java这种依赖于对象存在的语言似乎不那么重要，Java语言设计者们在设计时也不那么考虑函数，因此造成Java语言无法将函数作为函数的输入参数传递(只能借助 匿名内部类 这种别扭的方式)。例如线程池处理代码： 示例1： 12345678ThreadPoolTaskExecutor threadPooltaskExecutor = getTaskExcutor(5,10,30,\"sendMsgExecutor-\");threadPooltaskExecutor.initialize();threadPooltaskExecutor.execute(new Runnable() { @Override public void run() { System.out.println(\"this is gsm!\"); }}); 纵观其他倍受欢迎的开发语言，例如JavaScript这种函数式编程语言就可以轻易实现传递函数的功能。函数式编程语言提供了一个非常强大的功能，就是闭包。闭包是一个定义在一个函数内部的函数，它可以获取超越其作用域的外部变量。在本质上讲，闭包就是将函数内部和函数外部连接起来的一个桥梁（是不是很像内部类？）。Viral Patel认为Java为了弥补缺失函数式编程的不足而引入了lambda表达式，lambda表达式不是闭包，却在功能上无限接近闭包。 例如线程池处理代码就可以用如下的lambda表达式写出，你会发现代码终于不需要写的那么繁琐了： 示例2： 12345678 ThreadPoolTaskExecutor threadPooltaskExecutor = getTaskExcutor(5,10,30,\"sendMsgExecutor-\"); threadPooltaskExecutor.initialize(); threadPooltaskExecutor.execute(() -&gt; { System.out.println(\"this is gsm!\"); }); 2. 行为参数化《Java 8实战》的作者，Java 8的布道者Mario Fusco认为，相比于Java世界里普遍的传递数值到方法中，传递一个lambda表达式到函数的意义在于程序员们可以传递一种行为到方法，从而提高了API的复用性以及面向对象编程的抽象能力。 传递一种行为可以理解为通过API传递代码，我们可以回看示例2, 我们通过给execute传递了一段代码，能够让execute按照我们想要的方式执行某种行为。 再举个栗子，根据不同规则统计某一类数的和： 示例3：1234567891011121314public int sumAll(List&lt;Integer&gt; numbers, Predicate&lt;Integer&gt; p) { int total = 0; for (int number : numbers) { if (p.test(number)) { total += number; } } return total;}sumAll(numbers, n -&gt; true);sumAll(numbers, n -&gt; n % 2 == 0);sumAll(numbers, n -&gt; n &gt; 3); 示例里，我们不仅仅传递数值(List&lt;Integer&gt; numbers)到方法sumAll中，还传递判断的行为Predicate，借此实现高扩展的统计不同类别的列表的方法。 3. 为了支持更优雅及高效的遍历操作，以及支持一种优于集合的操作-流集合是Java中使用最多的API，我们每天都要对集合进行各种各样的操作。遍历操作可以分为两种 外部遍历 内部遍历 外部遍历是用户自己使用Java collection的接口手写遍历： 示例4：获取商品列表中商品的名称，形成一个名称列表 1234567891011121314151617Goods goods1 = new Goods();goods1.setName(\"P&amp;G\");Goods goods2 = new Goods();goods2.setName(\"DELL\");Goods goods3 = new Goods();goods3.setName(\"APPLE\");Goods goods4 = new Goods();goods4.setName(\"LENOVO\");List&lt;Goods&gt; skuList = Arrays.asList(goods1,goods2,goods3,goods4); // 初始化一个商品列表List&lt;String&gt; goodsNameList = new ArrayList&lt;&gt;();Iterator&lt;String&gt; iterator = skulist.iterator();while(iterator.hasNext()) { Goods goods = iterator.next(); goodsNameList.add(goods.getName());} 内部遍历方式则是将遍历责任从客户端转移到了服务端（Streams库）。服务端Streams使用内部遍历，可以选择自己适合的遍历方式帮用户把遍历做了。 示例5：123List&lt;String&gt; goodsNameList = skuList.stream() .map(goods -&gt; goods.getName()) //getName()方法参数化map，提取商品名称 .collect(Collectors.toList()); Streams 库提供一些预先定义好的操作，用户只需要用声明的方法指定想要使用什么操作(像map,filter,collect…)即可。大多数这类操作都接收Lambda表达式作为参数，让用户定制自己需要传递的行为, 而用户并不需要自己去实现。内部遍历最优雅的点在于，流操作可以优化处理顺序，原来collection遍历多次的操作，通过流只需要遍历一次即可。 Streams甚至还可以选择一种适合你硬件的数据表示和并行实现，利用多核CPU并行处理集合。要知道，jdk8主推parallel programming遍历, 从而达到更加高效率的遍历。 引用《java8实战》中的比较内部遍历和外部遍历的图，可以让我们更加清晰的明白二者的差异 正菜来了：究竟什么是lambda 表达式lambda表达式最初来源于学术界开发出来的一套用来描述计算的λ演算法。lambda表达式由三部分组成： 参数列表 箭头 -&gt; lambda主体（实际操作行为） lambda表达式没有return语句，return已经隐含在lambda主体之中了。 语法SynTax:1(parameters) -&gt; expression 或者1(parameters) -&gt; {statements} 栗子Example:123456781. (int x, int y) -&gt; x + y // -&gt;左边： lambda参数， -&gt;右边：lambda主体.// 此lambda表达式具有两个int型的输入参数，并返回int型的和。2. (Goods good) -&gt; good.getName.isEqual(\"P&amp;G\") //此lambda表达式有Goods good类型的参数，并返回boolean的返回值。3. () -&gt; 42 // 无参，返回int4. (String s) -&gt; System.out.println(s) // 输入string, 无返回5. x -&gt; 2 * x //6.(Apple a1, Apple a2) -&gt; a1.getWeight().compareTo(a2.getWeight()) //具有两个Apple类型的参数，返回一个int：比较两个Apple的重量 函数式接口 functional interfaceJdk库中已经定义了大量函数式接口，例如1234public interface Runnable { void run(); }public interface Callable&lt;V&gt; { V call() throws Exception; }public interface ActionListener { void actionPerformed(ActionEvent e); }public interface Comparator&lt;T&gt; { int compare(T o1, T o2); boolean equals(Object obj); } 函数式接口是一种特殊的SAM类型（Single Abstract Method）--只定义一个抽象方法的接口。@FunctionalInterface标注一个接口即表示该接口是一个函数式接口，如果你用@FunctionalInterface定义了一个接口，而它却不是函数式接口的话，编译器将返回一个提示原因的错误。 Lambda表达式是函数式接口的实例。以函数式接口为参数的方法，可以在调用时使用lambda表达式作为参数。 例如jdk1.8中java.lang.Runnable的源码：1234@FunctionalInterfacepublic interface Runnable { public abstract void run();} TaskPoolTaskExecutor源码：123456789public void execute(Runnable task) { ThreadPoolExecutor executor = this.getThreadPoolExecutor(); try { executor.execute(task); } catch (RejectedExecutionException var4) { throw new TaskRejectedException(\"Executor [\" + executor + \"] did not accept task: \" + task, var4); }} 我们在调用线程池执行方法execute12345threadPooltaskExecutor.execute(() -&gt; { // 你的代码逻辑; } ); lambda表达式与匿名类匿名类可以看做是lambda表达式的前世今生。因此，我们给lambda表达式传递参数的时候，若该形参在内部类中需要被使用，那么该形参必须要为final. 参考：[1] Maurice Naftalin : http://www.lambdafaq.org/are-lambda-expressions-objects/ [2] Viral Patel: http://viralpatel.net/blogs/Lambda-expressions-Java-tutorial/ [3] Mario Fusco: https://dzone.com/articles/why-we-need-lambda-expression [4] Raoul-Gabriel Urma, Mario Fusco, Alan Mycroft, 人民邮电出版社, 《Java8实战》","link":"/2017/09/10/lambda/"},{"title":"MySQL是怎么保证高可用的","text":"MySQL高可用系统的基础即主备切换, 而MySQL的几乎所有的高可用架构，都直接依赖于binlog。 binlog的三种格式statement当binlog_format=statement时，binlog里面记录的就是SQL语句的原文。由于只记录语句，缺乏执行的上下文，可能出现主备选择不同的索引而执行结果不同的情况。一般线上不推荐用此格式。 rowrow格式记录的非SQL语句原文. 记录的是被操作的数据行的主键id以及执行的server id，从而保证了主备的一致性。缺点：很占空间。 mix折中statement和row。 主备同步流程在状态 1 中，虽然节点 B 没有被直接访问，但是依然建议把节点 B（也就是备库）设置成只读（readonly）模式。这样做，有以下几个考虑： 有时候一些运营类的查询语句会被放到备库上去查，设置为只读可以防止误操作； 防止切换逻辑有 bug，比如切换过程中出现双写，造成主备不一致； 可以用 readonly 状态，来判断节点的角色。 备库B和主库A之间维持了一个长连接。主库A内部由一个线程，专门用于服务备库B的这个长连接。一个事务日志同步的完成过程是这样的： 在备库B上通过change master命令，设置主库A的IP、端口、用户名、密码，以及要从哪个位置开始请求binlog, 这个位置包含文件名和日志偏移量。 在备库不上执行start_slave命令，这时候备库会启动两个线程，就是图中io_thread和sql_thread。其中io_thread负责与主库建立连接。 主库A校验完用户名、密码后，开始按照备库B传来的位置，从本地读取binlog,发给B。 备库B拿到binlog后，写到本地文件，称为中转日志 relay log. sql_thread读取中转日志，解析出日志里的命令，并执行。 主备延迟诚然，MySQL高可用的基础是主备切换。然而主备切换可能会遇到由于主备延迟导致的问题问题。 主备切换可能是一个主动运维动作，比如软件升级，主库所在机器按计划下线，也可能是被动操作，比如主库所在机器掉电。 与数据同步有关的时间点主要包括以下三个：1.主库 A 执行完成一个事务，写入 binlog，我们把这个时刻记为 T1;2.之后传给备库 B，我们把备库 B 接收完这个 binlog 的时刻记为 T2;3.备库 B 执行完成这个事务，我们把这个时刻记为 T3。所谓主备延迟，就是同一个事务，在备库执行完成的时间和主库执行完成的时间之间的差值，也就是 T3-T1。如果在备库执行show slave status命令，它的返回结果会显示second_behind_master, 用于表示当前备库延迟了多少秒。 主备延迟可能发生在以下情形（分钟级延迟）： 在有些部署条件下，备库所在机器的性能要比主库所在机器性能差；–解决方案：机器对称部署 备库压力大；– 解决方案：一主多从，分担读的压力。通过binlog输出到外部系统，例如Hadoop。让外部系统提供统计类查询能力。 大事务。 例如一次性事务做大量的CUD 大表DDL 由于有主备延迟的存在，所以在主备切换的过程中，就相应有不同的策略：可靠性优先策略(停服切)和可用性优先策略（在线切）。 主备切换策略可靠性优先策略 判断备库 B 现在的 seconds_behind_master，如果小于某个值（比如 5 秒）继续下一步，否则持续重试这一步； 把主库 A 改成只读状态，即把 readonly 设置为 true； 判断备库 B 的 seconds_behind_master 的值，直到这个值变成 0 为止； 把备库 B 改成可读写状态，也就是把 readonly 设置为 false； 把业务请求切到备库 B。 优点：不存在主备数据不一致。 缺点：切换流程存在不可用时间。步骤2后，主库A和备库B都处于readonly状态，也就是这时系统处于不可写状态，直到步骤5完成后才能恢复。 实际应用中，建议使用可靠性优先的策略。 可用性优先策略先4，5调整到最开始执行。 优点：几乎没有不可用时间。 缺点：可能出现数据主备不一致。 备库并行复制能力如果备库执行日志的速度（消费能力）持续低于主库生成日志的速度（生产能力），那这个延迟就可能成了小时级别。而且对于一个压力持续比较高的主库来说，备库很可能永远追不上主库的节奏。MySQL5.6之前，MySQL只支持单线程复制，由此主库并发高，TPS高时就会出现严重主备延迟问题。关于主备的并行复制能力，需要关注的是图中黑色的两个箭头。一个箭头代表客户端写入主库（由于行锁的存在，写入的并发度支持的很高）。另一个箭头代表的是备库上sql_thread执行中转日志relay log。 要提高备库并行复制能力，需要M有SQL sql_thread多线程复制。线程模型：coordinator在分发的时候，需要满足以下这两个基本要求：1.不能造成更新覆盖。这就要求更新同一行的两个事务，必须被分发到同一个worker中。2.同一个事务不能被拆开，必须放到同一个worker中。 不同部署结构的同步策略双主结构的同步流程节点A和节点B互为主备。这样会存在“循环复制”问题：业务逻辑在节点 A 上更新了一条语句，然后再把生成的 binlog 发给节点 B，节点 B 执行完这条更新语句后也会生成 binlog。 解决方案： 规定两个库的server id必须不同，如果相同，则它们之间不能设定为主备关系； 一个备库接到binlog并重放过程中，生成与原binlog的server id相同的新的binlog; 每个库在收到从自己的主库发过来的日志后，先判断server id, 如果跟自己相同，表示这个日志是自己生成的，直接丢弃这个日志。 一主多从部署生产环境中，MySQL部署结构往往是一主多从。图中，虚线箭头表示的是主备关系，也就是 A 和 A’互为主备， 从库 B、C、D 指向的是主库 A。一主多从的设置，一般用于读写分离，主库负责所有的写入和一部分读，其他的读请求则由从库分担。 如果主库发生故障，主备切换问题。相比于一主一备的切换流程，一主多从结构在切换完成后，A’会成为新的主库，从库 B、C、D 也要改接到 A’。正是由于多了从库 B、C、D 重新指向的这个过程，所以主备切换的复杂性也相应增加了。 一主多从的主备切换有以下两个策略：基于位点的主备切换和基于GTID的主备切换。 基于位点的主备切换当我们把节点B设置成节点A’的从库的时候，需要执行一条change master命令：1234567CHANGE MASTER TO MASTER_HOST=$host_name MASTER_PORT=$port MASTER_USER=$user_name MASTER_PASSWORD=$password MASTER_LOG_FILE=$master_log_name MASTER_LOG_POS=$master_log_pos 其中最后两个参数MASTER_LOG_FILE和MASTER_LOG_POS表示，要从主库的master_log_name文件的master_log_pos这个位置的日志继续同步。而这个位置即“同步位点”， 也就是主库对应的文件名和日志偏移量。 同步位点很难精确取到，只是一个大概位置，找位点的时候，需要找一个“稍微往前”的，然后再通过判断跳过那些在从库B上已经执行过的事务。 以上操作缺点是复杂且容易出错，MySQL5.6 引入GTID彻底解决了这个困难。 基于GTID的主备切换Global Transaction Identifier, 全局事务ID，是一个事务在提交的时候生成的，是这个事务的唯一标识。它由两部分组成：1GTID=server_uuid:gno 其中： server_uuid 是一个实例第一次启动时，自动生成的，是一个全局唯一的值； gno是一个整数，初始值是1，每次提交事务的时候分配给这个事务，并加1。 在GTID模式下，每个事务都会跟一个GTID一一对应。只需要启动一个MySQL实例的时候，加上参数:每个MySQL实例都维护了一个GTID集合，用来对应“这个实例执行过的所有事务”。 在GTID模式下，备库B要设置为新主库A’的从库的语法如下：123456CHANGE MASTER TO MASTER_HOST=$host_name MASTER_PORT=$port MASTER_USER=$user_name MASTER_PASSWORD=$password master_auto_position=1 // 表示这个主备关系使用的是GTID协议，取消MASTER_LOG_FILE和MASTER_LOG_POS 我们把实例A’的GTID集合记为set_a, 实例B的GTID集合记为set_b。将A’提升为新的主，B指向A‘的主备切换逻辑： 实例B指定主库A’，基于主备协议建立连接； 实例B把set_b发给主库A’； 实例A’算出set_a与set_b的差集，判断A’本地是否包含了这个差集需要的所有binlog事务；a. 如果不包含，表示A’已经把实例B需要的binlog给删除掉了，直接返回错误；b. 如果确认全部包含，A’从自己的binlog文件里面，找出第一个不在set_b的事务，发给B； 之后就从这个事务开始，往后读文件，按顺序取binlog发给B去执行。 这个逻辑里面包含了一个设计思想：在基于GTID的主备关系里，系统认为只要建立主备关系，就必须保证主库发给备库的日志是完整的。因此，如果实例B需要的日志已经不存在，A’就拒绝把日志发给B。 这样主备切换实现：只需要B,C,D分别执行change master指向A’即可。严谨的说，主备切换部署不需要找位点了，而是找位点这个工作，在实例A’内部就已经自动完成了。但是由于这个工作是自动的，所以对于维护人员来说非常友好。 双活部署生产环境引入DBProxy做代理，Client访问数据库，首先通过LVS提供的VIP。主节点在主机房，因此无论是主机房还是从机房流量的写操作，都会落入主机房的DB。从机房是主机房的从库，读是分离的，读一半会走本机房，但是部分读会落到主库，因此会有少部分流量落到主机房。如果事务中的读，都会落到主机房。 可用性目标： 主机房MySQL实例不可用，能够通过Proxy到可用的MySQL实例。 办公网到主机房网络中断，可以同时通过访问备用机房读数据。当主机房挂掉后，Proxy实现自动切换，无需人工接入。 可以看出生产环境的DB可用性是利用一个中间层代理来提升的。简单介绍下代理：代理主要做负载均衡、读写分离、安全认证、失败重连、连接池、表路由、表hash，还有一些其他的功能，如强制读主库、压测使用shadow表。dbproxy本身是分布式的，自带高可用，其中一台挂掉不会影响业务访问数据库。dbproxy主要是为了高可用和后面的扩展性服务。 另外，双活部署也会存在不一致的问题：1、因为数据库的主从复制本身是异步的，有一定的延迟。因此写入之后马上读取的时候，可能路由到从库，而读取到错误的数据，这时就需要走强一致的逻辑，直接读主库（/{“router”:”m”}/ select order_id from order_info limit 1）。但是这个操作不能配置到所有的查询上，不然的话从库作为读分流的作用就失效了，并且主库不一定能抗住所有的读操作2、乱码问题，业务读取数据库数据，发现出现乱码问题。是因为在写入数据和读取数据的时候使用了不同的编码字符集，此时在链接数据库的时候需要显式地指定字符集，使得写入和读取数据的时候使用相同的字符集（显式指定是为了防止后端dbproxy到db的长连接已经被设置过字符集）。正常的一般都选择UTF-8即可，带有表情emoji的使用UTF8mb4 读写分离读写分离的目标：分担主库压力。 读写分离可以A：客户端端主动做负载均衡买这种模式一般会把DB连接信息放在客户端的连接层，客户端选择DB进行查询。带 proxy 的架构，对客户端比较友好。客户端不需要关注后端细节，连接维护、后端信息维护等工作，都是由 proxy 完成的。但这样的话，对后端维护团队的要求会更高。而且，proxy 也需要有高可用架构。因此，带 proxy 架构的整体就相对比较复杂。 读写分离可能遇到的坑由于主从可能存在延迟，客户端执行完一个更新事务后马上发起查询，如果查询选择的是从库的话，就可能读到刚刚事务更新之前的状态 – “过期读” 强制组主库方案查询请求可以分为以下两类：1、对于必须要拿到最新结果的请求，强制将其发到主库上。比如，在一个交易平台上，卖家发布商品以后，马上要返回主页面，看商品是否发布成功。那么，这个请求需要拿到最新的结果，就必须走主库。2、对于可以读到旧数据的请求，才将其发到从库上。在这个交易平台上，买家来逛商铺页面，就算晚几秒看到最新发布的商品，也是可以接受的。那么，这类请求就可以走从库。 此方案是用的最多的，且我厂线上使用的此方案。 sleep方案从库sleep 1s，解决1s内的主备延迟。超过1s就无用了。 判断主备无延迟方案可以通过second_behind_master/比对主备位点/比对主备GTID集合判断。 其他方案还有，配合semi-sync方案，等主库位点方案，等GTID方案。 参考 https://time.geekbang.org/column/article/76446 https://time.geekbang.org/column/article/77083","link":"/2019/02/17/master-slave-sync/"},{"title":"Mysql存储机制-数据页管理","text":"Mysql中，索引即数据。Index page是数据表中一条条数据和索引的载体，是最重要的一种页面类型。本章讨论和验证了索引页的物理存储结构 Page structureIndex page的物理视图如下图所示。除去通用的页面头部FIL Header和Tailer，页面体主要包含三个部分：Index Page Header,Records Space,Record Directory。Index Page Header是Index Page特有的页头，Record Space是用户记录空间，包括已使用和空闲空间两部分，Record Directory是一个稀疏索引，主要用于加速rec在页内的查询速度。 Index Page HeaderIndex Page Header是索引页独有的信息，其中各字段(红色部分)所包含的信息说明如下表： Macro Bytes Desc PAGE_N_DIR_SLOTS 2 Page directory中的slot个数 PAGE_HEAP_TOP 2 当前Page内已使用的空间的末尾位置，即free space的开始位置 PAGE_N_HEAP 2 Page内所有记录个数，包含用户记录，系统记录以及标记删除的记录，同时当第一个bit设置为1时，表示这个page内是以Compact格式存储的 PAGE_FREE 2 指向标记删除的记录链表的第一个记录 PAGE_GARBAGE 2 被删除的记录链表上占用的总的字节数，属于可回收的垃圾碎片空间 PAGE_LAST_INSERT 2 指向最近一次插入的记录偏移量，主要用于优化顺序插入操作 PAGE_DIRECTION 2 用于指示当前记录的插入顺序以及是否正在进行顺序插入，每次插入时，PAGE_LAST_INSERT会和当前记录进行比较，以确认插入方向，据此进行插入优化 PAGE_N_DIRECTION 2 当前以相同方向的顺序插入记录个数 PAGE_N_RECS 2 Page上有效的未被标记删除的用户记录个数 PAGE_MAX_TRX_ID 8 最近一次修改该page记录的事务ID，主要用于辅助判断二级索引记录的可见性。 PAGE_LEVEL 2 该Page所在的btree level，根节点的level最大，叶子节点的level为0 PAGE_INDEX_ID 8 该Page归属的索引ID 除去红色部分的索引头之外，索引的根节点root page还记录中间节点段和叶子节点段的信息，称为FSEG Header： Macro Bytes Desc PAGE_BTR_SEG_LEAF 10 leaf segment在inode page中的位置 PAGE_BTR_SEG_TOP 10 non-leaf segment在inode page中的位置 Record Space用户记录（后简称rec）组织方式采用的是按键值有序的单向链表方式来组织的。Record Space空间中，最前面两条记录永远是：infimum 和 supremum这两条，分别用来标记虚拟的最前面一个记录和最后面一个记录。 Mysql中rec是可变长的，包含record header和record data两部分。Innodb引擎中，对rec支持不同的编码格式，接下来将会以Compact模式为例介绍聚簇索引和非聚簇索引的叶子节点和中间节点的物理结构。在此之前，先声明一个定义：Innodb引擎中，把record data的起始地址称为‘origin’，使用N表示，record header使用负偏移量，如N-2来表示。 The record headerrecord header的结构如下图。 自底向前看，各部分的含义为： Next Record Offset，2字节，该页面中，当前记录按主键升序排列的下一条记录的页内偏移量 Record Type，rec的类型，可选值包括：用户数据conventional(0)，索引数据node pointer(1)，infimum(2)以及supermum(3) Order/Heap no，用户记录在page中按插入顺序对应的物理编号，infimum始终取0，supremum始终取1，用户记录从2开始计数 Number of Records Owned，当该值为非0时，表示当前记录占用page directory里一个slot，并和前一个slot之间存在这么多个记录。 Info Flags，标记位，目前只使用了两个bit，一个用于表示该记录是否被标记删除(REC_INFO_DELETED_FLAG)，另一个bit(REC_INFO_MIN_REC_FLAG)如果被设置，表示这个记录是当前level最左边的page的第一个用户记录，即最小的记录。 Nullable field bitmap，可选，标示值为NULL的列的bitmap，每个位标示一个列，如果一个列为空，在rec data中不存储任何值，只在这里mark Variable field length array，可选，记录可变长列的长度数组。如果列的最大长度为255字节，使用1byte；否则，0xxxxxxx (one byte, length=0..127), or 1exxxxxxxxxxxxxx (two bytes, length=128..16383, extern storage flag) Clustered Indexes聚簇索引下叶子节点中记录的结构如下图 从rec data的起始位置开始，依次包含以下字段： Cluster Key Fields，索引字段，以字节流的形式依次拼接索引字段 Transaction ID，事务id，6字节，上次修改当前记录的事务id Roll Pointer，回滚指针，上一次修改当前记录的事务在回滚段中的回滚记录的位置 Non-Key Fields，非索引字段，以字节流形式依次拼接的所有非索引字段 聚簇索引下非叶子节点中记录的结构相比于叶子节点要简单很多：首先，由于非叶子节点不支持MVCC，所以记录中不包含事务和回滚指针；其次，由于索引字段不能为空，所以rec header和rec data中都不包含Non-Key Fields。具体结构如下图。 从rec data的起始位置开始，依次包含以下字段： 当前记录所对应的叶子节点中按主键排序最小的主键值 叶子节点的页号 Secondary Indexes非聚簇索引（后简称二级索引）的叶子节点结构和聚簇索引类似，除去少了事务ID和回滚指针外，大体类似。需要特别指出的是，如果二级索引字段和聚簇索引字段有重叠，那么重叠的部分将被记录在Secondary Key Fields中。比如：一张表包含聚簇索引（a,b,c）和二级索引（a,d），那么，Secondary Key Fields将包含a,d, Cluster Key Fields将只包含（b,c）。结构如下图： 非聚簇索引的非叶子节点结构如下图，与聚簇索引类似，不再赘述。 Directory由于rec在page内是按照主键升序的单链表形式组织的，如果不使用任何辅助结构，那么在page内检索一条记录就需要从infimum开始，依次比对，效率不高。为了加快页内的数据查找，会按照记录的顺序，每隔4~8个数量的用户记录，就分配一个slot 。每个slot占用2个字节，存储记录的页内偏移量，可以理解为在页内构建的一个很小的索引(sparse index)来辅助二分查找。 Page Directory的slot分配是从Page末尾（倒数第八个字节开始）开始逆序分配的。在查询记录时。先根据Page directory 确定记录所在的范围，然后在据此进行线性查询。","link":"/2018/11/13/mysql-index-page/"},{"title":"MyBatis generator自动生成代码详细配置","text":"MyBatisMyBatis Generator (下文简称MGB)是Mybatis官方推出的MyBatis和iBatis代码生成器。引入MGB jar包后，MGB会根据指定的配置文件读取数据库表生成一个访问数据库的接口，实现对数据库进行基本的CRUD甚至是一些联表操作。 使用MGB会生成： Java POJO类 MyBatis/iBatis SQL Map XML 文件(具体sql代码)。 mapper 文件(相当于DAO，定义操作数据库的java接口) 1. 使用mybatis-generator实现CRUD1.1. 下载MGB JARMBG下载地址 1.2. 配置mybatis-generator-config.xml如果要想实现MGB代码生成器，需要先配置一个基础的XML配置文件(本文定义为mybatis-generator-config.xml)。该配置文件主要告诉MBG三件事： 1.如何连接到数据库 2.生成什么POJO对象(根据哪张表生成对象，对象存放在哪个文件路径下) 3.mapper文件生成哪些操作数据库的方法 mybatis-generator-config.xml 的文件结构为：1234567891011121314151617181920212223242526&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC \"-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN\" \"http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd\"&gt;&lt;generatorConfiguration&gt; &lt;!-- 具体配置内容 --&gt; &lt;properties&gt; &lt;/properties&gt; &lt;classPathEntry&gt; 指定mgb jar包位置 &lt;/classPathEntry&gt; &lt;context&gt; &lt;property&gt; (0个或多个) &lt;plugin&gt; (0个或多个) 使用哪些插件 &lt;commentGenerator&gt; (0个或1个) 生成的代码要不要注释 &lt;jdbcConnection&gt; (1个) 配置连接的jdbc &lt;javaTypeResolver&gt; (0个或1个) 配置java数据类型和sql数据类型映射 &lt;javaModelGenerator&gt; (1个) 生成PO类的位置 &lt;sqlMapGenerator&gt; (0个或1个) mapper映射文件生成的位置 &lt;javaClientGenerator&gt; (0个或1个) mapper接口生成的位置 &lt;table&gt; (1个或多个) 根据哪个mysql table生成 &lt;/context&gt;&lt;/generatorConfiguration&gt; 详细配置及对应含义可以参考. 这里重点说下生产环境常用的配置。 context元素context元素用于指定生成POJO对象的环境，例如指定连接的数据库，要生成对象的类型和路径。 context属性defaultModelType定义了MBG如何生成实体类。一般选择flat模型,该模型为每一张表只生成一个实体类。这个实体类包含表中的所有字段。这种模型最简单，推荐使用。 一般情况下，我们使用如下的配置即可：1&lt;context id=\"Mysql\" defaultModelType=\"flat\"&gt; &lt;plugin&gt;元素使用插件。插件用于扩展或修改通过MyBatis Generator (MBG)代码生成器生成的代码。生产环境常见的插件： &lt;plugin type=&quot;org.mybatis.generator.plugins.RenameExampleClassPlugin&quot;/&gt; 该插件用于重命名生成的XXXExample类，通过配置 searchString和replaceString属性来完成。具体通过配置searchString和replaceString属性使用正则表达式实现。 &lt;plugin type=&quot;org.mybatis.generator.plugins.EqualsHashCodePlugin&quot;/&gt; 这个插件用来给生成的Java POJO类生成equals和hashcode方法 &lt;plugin type=&quot;org.mybatis.generator.plugins.SerializablePlugin&quot;/&gt; 这个插件用来给生成的Java POJO类生成的Java模型类添加序列化接口，并生成serialVersionUID字段 更多插件可以参见mybatis官方插件库mybatis官方插件库和第三方插件库. 一份完整的mybatis-generator-config 示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC \"-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN\" \"http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd\"&gt;&lt;generatorConfiguration&gt; &lt;!-- step 1: classPathEntry:数据库的JDBC驱动,换成你自己的驱动路径 --&gt; &lt;classPathEntry location=\"D:\\Program Files\\apache-maven-3.0.3-bin\\maven-repository\\mysql\\mysql-connector-java\\5.1.20\\mysql-connector-java-5.1.20.jar\"/&gt; &lt;context id=\"MBG\" targetRuntime=\"MyBatis3\" defaultModelType=\"flat\"&gt; &lt;!-- defaultModelType指定生成pojo的模型 --&gt; &lt;!-- 使用插件 --&gt; &lt;!--&lt;plugin type=\"plugin.SelectByPagePlugin\" /&gt; --&gt; &lt;plugin type=\"org.mybatis.generator.plugins.RenameExampleClassPlugin\"&gt; &lt;!-- 此处是将Example改名为Criteria --&gt; &lt;property name=\"searchString\" value=\"Example$\"/&gt; &lt;property name=\"replaceString\" value=\"Criteria\"/&gt; &lt;/plugin&gt; &lt;plugin type=\"org.mybatis.generator.plugins.EqualsHashCodePlugin\"/&gt; &lt;plugin type=\"org.mybatis.generator.plugins.SerializablePlugin\"/&gt; &lt;commentGenerator&gt; &lt;!-- 去除自动生成的注释 --&gt; &lt;property name=\"suppressAllComments\" value=\"true\"/&gt; &lt;property name=\"suppressDate\" value=\"true\"/&gt; &lt;/commentGenerator&gt; &lt;!--数据库连接的信息：驱动类、连接地址、用户名、密码 --&gt; &lt;jdbcConnection driverClass=\"com.mysql.jdbc.Driver\" connectionURL=\"jdbc:mysql:loadbalance://xxx.xxx.xxx.xxx:3306/database_name?useUnicode=true&amp;amp;characterEncoding=UTF8\" userId=\"username\" password=\"password\"&gt; &lt;/jdbcConnection&gt; &lt;!-- 默认false，把JDBC DECIMAL 和 NUMERIC 类型解析为 Integer，为 true时把JDBC DECIMAL 和 NUMERIC 类型解析为java.math.BigDecimal --&gt; &lt;javaTypeResolver&gt; &lt;property name=\"forceBigDecimals\" value=\"false\"/&gt; &lt;/javaTypeResolver&gt; &lt;!-- targetProject:生成PO类的位置 --&gt; &lt;javaModelGenerator targetPackage=\"com.jd.myproject.domain.actPC\" targetProject=\"D:\\GitPro\\myproject\\myproject-domain\\src\\main\\java\"&gt; &lt;!-- enableSubPackages:是否让schema作为包的后缀 --&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;!-- 从数据库返回的值被清理前后的空格 --&gt; &lt;property name=\"trimStrings\" value=\"true\"/&gt; &lt;/javaModelGenerator&gt; &lt;!-- targetProject:mapper映射文件生成的位置 如果maven工程只是单独的一个工程，targetProject=\"src/main/java\" 若果maven工程是分模块的工程，targetProject=\"所属模块的名称\"，例如： targetProject=\"ecps-manager-mapper\"，下同--&gt; &lt;sqlMapGenerator targetPackage=\"actPC\" targetProject=\" D:\\GitPro\\myproject\\myproject-dao\\src\\main\\resources\\sqlmap\"&gt; &lt;!-- enableSubPackages:是否让schema作为包的后缀 --&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;/sqlMapGenerator&gt; &lt;!-- targetPackage：mapper接口生成的位置 --&gt; &lt;javaClientGenerator type=\"XMLMAPPER\" targetPackage=\"com.jd.myproject.dao.actPC.mapper\" targetProject=\"D:\\GitPro\\myproject\\myproject-dao\\src\\main\\java\"&gt; &lt;!-- enableSubPackages:是否让schema作为包的后缀 --&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;/javaClientGenerator&gt; &lt;!-- tableName:用于自动生成代码的数据库表；domainObjectName:对应于数据库表的javaBean类名 --&gt; &lt;!-- 指定数据表 --&gt; &lt;table tableName=\"student_audit\" domainObjectName=\"StudentAudit\" enableCountByExample=\"true\" enableUpdateByExample=\"true\" enableDeleteByExample=\"false\" enableSelectByExample=\"true\" selectByExampleQueryId=\"true\" enableInsert=\"true\"&gt; &lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 添加到如下路径 1.3. 生成mybatis-generator-maven-plugin在domain的pom文件中添加mybatis-generator的maven plugin插件。 123456789101112131415161718&lt;dependencies&gt; ... &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt; &lt;configuration&gt; &lt;!--配置文件的位置--&gt; &lt;configurationFile&gt;src/main/resources/mybatis-generator-config.xml &lt;/configurationFile&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 使用idea的maven插件直接快速生成, 对domain进行 mvn clean,mvn package。会发现maven 生成了一个 MGB的一个插件 1.4. MGB生成代码直接点击mybatis-generator:generate就可生成。 2. 使用mybatis-generator实现高级操作123456&lt;!-- 指定数据表 --&gt;&lt;table tableName=\"test_table\" domainObjectName=\"TestPojo\" enableCountByExample=\"true\" enableUpdateByExample=\"true\" enableDeleteByExample=\"false\" enableSelectByExample=\"true\" selectByExampleQueryId=\"true\" enableInsert=\"true\"&gt;&lt;/table&gt; 在mybatis-generator-config.xml中我们定义了一系列ByExample形式的操作数据库的java接口。如果把enableCountByExample 等等这些ByExample接口都置为false，那么MGB只会生成基本的CRUD增删改查的接口，难以满足生产环境mysql的复杂语句，例如根据条件搜索，统计数量，甚至是一些联表操作。 当把enableCountByExample 等等这些ByExample接口都置为true后，会生成一个xxxExample类，该类可以理解为一个动态配置where条件的对象。xxxExample类包括一个静态内部类Creteria，Creteria包含一个Criteria条件集合，集合内的条件是由And逻辑与连接的。当使用or方法会创建一个Criteria属性，Criteria和Criteria对象之间是逻辑或。 另外：Example类的distinct字段用于指定DISTINCT查询。orderByClause字段用于指定ORDER BY条件,这个条件没有构造方法,直接通过传递字符串值指定。 123456789101112131415161718public class StudentExample { protected String orderByClause; protected boolean distinct; protected List&lt;Criteria&gt; oredCriteria; public void or(Criteria criteria) { oredCriteria.add(criteria); } public Criteria or() { Criteria criteria = createCriteriaInternal(); oredCriteria.add(criteria); return criteria; } ...} 另外，如果仍然有一些情况mgb自动生成的代码仍然满足不了我们的需求，如进行联表。我们可以扩展example类。 例如实现联表操作。在xxxExample类增加方法1234public Criteria multiColumnOrClause(String value) { addCriterion(\"(b.name like \" +\"\\\"%\" +value +\"%\\\"\" + \" OR id = \" + Integer.valueOf(value)+\")\"); return (Criteria)this;} 在mapper中增加联表语句即可。 3. 使用mybatis打印sql操作日志MyBatis内置有日志工厂，可以使用Log4j对每一条数据库操作进行日志打印。这对开发环境快速定位问题非常重要。通过配置log4j.properties可以实现： 日志记录每一条数据库操作语句 日志记录每一条数据库操作的输入 日志记录每一条数据库操作的结果 1234567891011121314151617181920212223log4j.rootLogger=DEBUG,FileLoglog4j.logger.java.sql = DEBUGlog4j.logger.java.sql.Connection = DEBUGlog4j.logger.java.sql.Statement = DEBUGlog4j.logger.java.sql.PreparedStatement = DEBUGlog4j.logger.java.sql.ResultSet = DEBUGlog4j.logger.com.ibatis = DEBUGlog4j.logger.com.ibatis.common.jdbc.SimpleDataSource = DEBUGlog4j.logger.com.ibatis.common.jdbc.ScriptRunner = DEBUGlog4j.logger.com.ibatis.sqlmap.engine.impl.SqlMapClientDelegate = DEBUG# Console config 屏幕打印#log4j.appender.stdout=org.apache.log4j.ConsoleAppender#log4j.appender.stdout.Target=System.out#log4j.appender.stdout.layout=org.apache.log4j.PatternLayout#log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} [%7r] [%5p] - %30.30c - %m \\n#File Log config 文件输出log4j.appender.FileLog=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.FileLog.File=D:/export/ejshop/jshope.loglog4j.appender.FileLog.layout=org.apache.log4j.PatternLayoutlog4j.appender.FileLog.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} [%7r] [%5p] - %30.30c - %m \\nlog4j.appender.FileLog.encoding=UTF-8 参考 MBG官网","link":"/2017/11/01/mybatis/"},{"title":"MySQL 数据可靠性是怎么保证的？","text":"只要redo log和binlog保证持久化到磁盘，就能保证MySQL异常重启后，数据可以恢复。而讨论MySQL数据丢失应该从InnoDB事务数据丢失和数据库复制导致数据丢失两方面讨论。 何为丢数据一般我们希望把一系列的数据作为一个原子操作，这样的话，这一系列操作，要么提交，要么全部回滚掉。当我们提交一个事务，数据库要么告诉我们事务提交成功了，要么告诉我们提交失败。 数据库为了效率等原因，数据只保存在内存中，没有真正的写入到磁盘上去(WAL)。如果数据库响应为“提交成功”，但是由于数据库挂掉，操作系统，数据库主机等任何问题导致这次“提交成功”的事务对数据库的修改没有生效，那么我们认为这个事务的数据丢失了。这个对银行或者支付宝这种业务场景来说是不能接受的。所以，保证数据不丢失也是数据库选择的一个重要衡量指标。 InnoDB事务数据丢失InnoDB和Oracle都是利用redo 来保证数据一致性的。如果你有从数据库新建一直到数据库挂掉的所有redo，那么你可以将数据完完整整的重新build出来。但是这样的话，速度肯定很慢。所以一般每隔一段时间，数据库会做一个checkpoint的操作，做checkpoint的目的就是为了让在该时刻之前的所有数据都”落地”。这样的话，数据库挂了，内存中的数据丢了，不用从最原始的位置开始恢复，而只需要从最新的checkpoint来恢复。将已经提交的所有事务变更到具体的数据块中，将那些未提交的事务回滚掉。 因此，保证事务的redo日志刷到磁盘就成了事务数据是否丢失的关键。 而InnoDB为了保证日志的刷写的高效，使用了内存的log buffer，另外，由于InnoDB大部分情况下使用的是文件系统，(linux文件系统本身也是有buffer的)而不是直接使用物理块设备，这样的话就有两种丢失日志的可能性：日志保存在log_buffer中，机器挂了，对应的事务数据就丢失了;日志从log buffer刷到了linux文件系统的buffer，机器挂掉了，对应的事务数据就丢失了。当然，文件系统的缓存刷新到硬件设备，还有可能被raid卡的缓存，甚至是磁盘本身的缓存保留，而不是真正的写到磁盘介质上去了。 redolog 写入机制redo log 三种状态： 存在redo log buffer中，物理上在MySQL进程内存中； 写到磁盘write, 但是没有持久化fsync,物理上是在文件系统的page cache里面; 持久化到磁盘，对应的是hard disk. 日志写到 redo log buffer 是很快的，wirte 到 page cache 也差不多，但是持久化到磁盘的速度就慢多了。 InnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。 为了控制 redo log 的写入策略，InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，控制redo log 的刷新。它有三种可能取值： 1.设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ; 这样可能丢失1s的事务数据。 2.设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘；这样的话，数据库对IO的要求非常高，如果底层硬件提供的IOPS比较差，MySQL数据库 并发很快就会由于硬件IO的问题而无法提升。（当然，InnoDB的组提交方法为降低IOPS做了很大优化） 3.设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。如果只是MySQL数据库挂掉了，由于文件系统没有问题，那么对应的事务数据并没有丢失。只有在数据库所在的主机操作系统损坏或者突然掉电的情况下，数据库的事务数据可能丢失1秒之类的事务数据。这样的好处就是，减少了事务数据丢失的概率，而对底层硬件的IO要求也没有那么高(log buffer写到文件系统中，一般只是从log buffer的内存转移的文件系统的内存缓存中，对底层IO没有压力) 注意，事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的 redo log，也是可能已经持久化到磁盘的。 组提交MySQL是如何保证在频繁刷redo log到磁盘情况的同时保证较高的TPS的？ – 一次刷一组 - 组提交。 LSNLSN：log sequence number日志逻辑序列号。LSN是单调递增的，用来对应redo log的一个个写入点。 每次写入长度为length的redo log， LSN的值就会加上length。 第一个到达的事务会被选为这组的leader; 等 trx1 要开始写盘的时候，这个组里面已经有了三个事务，这时候 LSN 也变成了 160； trx1 去写盘的时候，带的就是 LSN=160，因此等 trx1 返回时，所有 LSN 小于等于 160 的 redo log，都已经被持久化到磁盘； 这时候 trx2 和 trx3 就可以直接返回了。 注意，这里“组”的划分为在MySQL进程内存的所有事务组成一组。在并发更新场景下，第一个事务写完redo log buffer以后，接下来fsync 越晚调用，组员可能越多，节约IOPS的效果就越好。 为了让fsync带的组员更多，MySQL的优化：拖时间。两阶段提交的实际图为：MySQL为了让组提交的效果更好，把redo log做fsync的时间拖到了步骤1后,这样binlog也可以组提交了，这样可以减少IOPS的消耗。 数据库复制导致数据丢失MySQL提供异步的数据同步机制。利用这种复制同步机制，当数据库主库无法提供服务时，应用可以快速切换到跟它保持同步的一个备库中去。备库继续为应用提供服务，从而不影响应用的可用性。这里有一个关键的问题，就是应用切换到备库访问，备库的数据需要跟主库的数据一致才能保证不丢失数据。由于目前MySQL还没有提供全同步的主备复制解决方案所以这里也是可能存在数据丢失的情况。 目前MySQL提供两种主备同步的方式：异步(asynchronous)和半同步(Semi-sync)。 异步的方式下，几个线程都是独立的，相互不依赖。而在半同步的情况下，主库的事务提交需要保证至少有一个备库的IO线程已经拉到了数据，这样保证了至少有一个备库有最新的事务数据，避免了数据丢失。这里称为半同步，是因为主库并不要求SQL线程已经执行完成了这个事务。 半同步在MySQL 5.5才开始提供，并且可能引起并发和效率的一系列问题，比如只有一个备库，备库挂掉了，那么主库在事务提交10秒(rpl_semi_sync_master_timeout控制)后，才会继续，之后变成传统的异步方式。所以目前在生产环境下使用半同步的比较少。 在异步方式下，如何保证数据尽量不丢失就成了主要问题。这个问题其实就是如何保证数据库的binlog不丢失，尽快将binlog落地，这样就算数据库挂掉了，我们还可以通过binlog来将丢失的部分数据手工同步到备库上去(MHA会自动抽取缺失的部分补全备库)。 MySQL提供一个sync_binlog参数来控制数据库的binlog刷到磁盘上去。虽然binlog也有binlog cache，但是MySQL并没有控制binlog cache同步到文件系统缓存的相关考虑。所以我们这里不涉及binlog cache。所以很多MySQL DBA设置的sync_binlog并不是最安全的1，而是100或者是0。这样牺牲一定的一致性，可以获得更高的并发和性能。 InnoDB与MySQL协同根据上面section 2和section 3可知，redo log和binlog都影响数据丢失。但是他们分别在InnoDB和MySQL server层维护。由于一个事务可能使用两种事务引擎，所以MySQL用两段式事务提交来协调事务提交。 两阶段提交2PC即Innodb对于事务的两阶段提交机制。当MySQL开启binlog的时候，会存在一个内部XA的问题：事务在存储引擎层（redo log）commit的顺序和在binlog中提交的顺序不一致的问题。如果不使用两阶段提交，那么数据库的状态有可能用它的日志恢复出来的库的状态不一致。 事务的commit分为prepare和commit两个阶段：1、prepare阶段：redo持久化到磁盘（redo group commit），并将回滚段置为prepared状态，此时binlog不做操作。2、commit阶段：innodb释放锁，释放回滚段，设置提交状态，binlog持久化到磁盘，然后存储引擎层提交。 崩溃恢复一条更新语句的2PC流程图如下： 崩溃恢复目标避免binlog 主备库恢复数据不一致；避免binlog恢复出来的数据与redolog恢复出来的数据不一致。 崩溃恢复时的判断原则 如果redo log里面的事务是完整的，也就是已经有了commit标识，则直接提交； 如果redo log里面的事务只有完整的prepare,则判断对应的事务binlog是否存在并完整： a.如果是，则提交事务； b.否则，回滚事务。 时刻A：写入redo log处于prepare阶段之后，写binlong之前，放生了crash. 由于此时binlog还没写，redo log 也还没提交，所以崩溃恢复的时候，事务会回滚。这时候binlog还没写，所以也不会传到备库。时刻B：对应原则2a。 MySQL引入了binlog-checksum参数用来验证binlog的正确性以此验证事务binlog的正确性。而redo log 和 binlog 的数据格式有一个共同的数据字段 - XID。 崩溃恢复的时候，会按顺序扫描redo log: 如果碰到既有prepare, 又有commit的redo log，就直接提交； 如果碰到只有prepare，而没有commit的redo log, 就拿着这个redo log的XID 去binlog 找对应的事务。以此关联起redo log和binlog。 两阶段提交是经典 的分布式系统问题。举例来说，对于InnoDB引擎，如果redo log提交完了，事务就不能回滚。而如果redo log直接提交，然后binlog 写入失败，则InnoDB又回滚不了，数据和binlog日志又不一致了，因为存在binlog和redo log两种日志且存在主从环境，就需要两阶段提交。两阶段提交就是为了给所有人一个机会，当每个人都说“我OK”的时候，再一起提交。 那如果只用binlog做崩溃恢复，避免费事的两阶段提交可以吗？ 答案是binlog是不能做崩溃恢复的。原因1 - binlog 没有能力恢复“数据页”。如下图因为binlog 只记录了逻辑操作，而非像redo log记录数据页磁盘的数据变更。当mysql crash时候，例如binlog1 记录的事件，如果没有WAL 刷盘，binlog是会丢数据的。也就是binlog没有能力恢复数据页。 但是binlog 也有自己的特殊用途，不能完全取消： 归档。redo log 是循环写，这样历史日志没法保留，redo log无法起到归档的作用。 复制。MySQL高可用的基础，就是binlog复制。还有很多公司有异构系统（比如一些数据分析系统），这些系统就靠消费MySQL的binlog 来更新自己的数据。 如何通过redo log做崩溃恢复？ 1.如果是正常运行的实例的话，数据页被修改后，跟磁盘的数据页不一致，称为脏页。最终数据罗盘，就是把内存中的数据页写盘，这个过程与redo log无关系。2.InnoDB如果判断一个数据页可能在崩溃恢复时候丢失了更新，就会将它读到内存，然后让redo log 更新内存内容。更新完成后，内存页变成了脏页，就回到了第一种情况。 binlog 写入机制binlog写入时机：sql语句或者事务执行完。binlog 写入逻辑比较简单：事务执行过程中，先把日志写到binlog cache， 事务提交的时候，再把binlog cache写到binlog文件中。每个线程有自己binlog cache, 但是共用同一份binlog文件。其中write，指的是把日志写入到文件系统的page cache,并没有把数据持久化到磁盘，所以速度比较快。fsync是将数据持久化到磁盘的操作。一般情况下，fsync才占磁盘IOPS. write 和 fsync 的时机，是由参数 sync_binlog 控制的：1.sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync；2.sync_binlog=1 的时候，表示每次提交事务都会执行 fsync；3.sync_binlog=N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。 在出现 IO 瓶颈的场景里，将 sync_binlog 设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成 0，比较常见的是将其设置为 100~1000 中的某个数值。风险是当主机发生异常重启，会丢失最近N个事务的binlog日志。 WAL机制强大的原因 redo log和binlog都是顺序写，磁盘的顺序写比随机写速度要快； 组提交机制，可以大幅度降低磁盘的IOPS消耗。 最后一个问题如果想避免MySQL出现IO瓶颈，可以怎么做提升性能？ 设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，减少 binlog 的写盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但没有丢失数据的风险。将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000）。这样做的风险是，主机掉电时会丢 binlog 日志。将 innodb_flush_log_at_trx_commit 设置为 2。这样做的风险是，主机掉电的时候会丢数据。 参考 http://blog.51cto.com/xiaoze/1607601 丁奇-极客时间","link":"/2019/02/15/mysql-data-relable/"},{"title":"Mysql存储机制—表空间结构","text":"本文基于InnDB存储引擎源码，试图解释Mysql数据表在InnoDB引擎下的组织管理方式。本文重点对用户表空间的物理和逻辑结构进行阐述，属于Mysql存储机制系列之一。 InnoDB引擎对Mysql数据的管理，在物理层表示上（即磁盘实际存储的文件），包括日志文件、主系统表空间文件ibdata、undo tablespace文件、临时表空间文件以及用户表空间。这些文件具有统一的结构，本文以用户表空间为例，进行展开。 概述InnoDB 的每个数据文件都归属于一个表空间，不同的表空间使用一个唯一标识的space id来标记。例如ibdata1, ibdata2… 归属系统表空间，拥有相同的space id。用户创建表产生的ibd文件，则认为是一个独立的tablespace，只包含一个文件。 InnoDB引擎对表空间在内部的逻辑视图如下： 1、表空间 默认情况下，只有一个表空间ibdata1，所有数据存放在这个空间内 如果启用了innodb_file_per_table，则每张表内的数据可以单独放到一个表空间内 每个表空间只存放数据、索引和InsertBuffer Bitmap页，其他数据还在ibdata1中 2、Segment段（InnoDB引擎自己控制） 数据段：B+ tree的叶子节点 索引段：B+ tree的非叶子节点 回滚段 3、extent区 每个区的大小为1M，页大小为16KB，即一个区一共有64个连续的页（区的大小不可调节，页可以） 4、Page页 InnoDB磁盘管理的最小单位 默认每个页大小为16KB，可以通过innodb_page_size来设置（4/8/16K） 每个页最多存放7992行数据 页有不同的类型，如果表空间管理页，INODE节点页，插入缓存页以及存放数据的索引页等 5、Row行 对应数据表里一条条记录，后续单独对页内数据组织方式进行说明 InnoDB引擎对表空间组织的物理视图如下： 接下来将对上图中的Page内容和组织管理方式进行逐一说明。 File HeaderInnoDB磁盘管理的最小单位是页，所有页都有两个统一的结构：页头（FIL Header），占据页面的前38个字节；页尾（FIL Trailer），占据页面末尾8字节。 FIL Header页头包含的数据项如下： Macro Offset description FIL_PAGE_SPACE_OR_CHKSUM 0 4.0.14版本前，表示space id; 之后表示校验和 FIL_PAGE_OFFSET 4 当前页的偏移量（Page no） FIL_PAGE_PREV 8 前驱节点的偏移量 FIL_PAGE_NEXT 12 后继节点的偏移量 FIL_PAGE_LSN 16 最近一次修改该page的LSN FIL_PAGE_TYPE 24 页面类型 FIL_PAGE_FILE_FLUSH_LSN 26 Checkpoint for system tablespace FIL_PAGE_SPACE_ID 34 space id FIL_PAGE_OFFSET表示当前页在其表空间中的相对偏移量，也就是页号。FIL_PAGE_TYPE表示当前页的类型，常用的页面类型如下表： MACRO VALUE DESCRIPTION FIL_PAGE_INDEX 17855 B-tree索引页 FIL_PAGE_TYPE_ALLOCATED 0 磁盘新分配未使用的页 FIL_PAGE_UNDO_LOG 2 Undo log页 FIL_PAGE_INODE 3 INODE页 FIL_PAGE_TYPE_SYS 6 系统页 FIL_PAGE_TYPE_FSP_HDR 8 file space header page FIL_PAGE_TYPE_XDES 9 extent desc page FIL_PAGE_TYPE_BLOB 10 未压缩的BLOB页 InnoDB引擎中，索引B-tree上，在同一Level的索引页会按照双向链表的方式组织起来，FIL_PAGE_PREV，FIL_PAGE_NEXT则提供了一个索引页的前驱和后继节点的页号。需要指出的是，BLOB类型的页面是按照单链表的方式组织，不存在FIL_PAGE_PREV结构。FIL_PAGE_INODE页记录Segment的组织信息，一个Segment对应页内一条Inode Entry。 FIL Trailer page trailer是在文件末尾的最后8个字节， 低位4个字节是用来表示page页中数据的checksum,高位4位是用来存储FIL_PAGE_LSN的部分信息。 File Space Header Page参考上文的物理视图，表空间第一页的类型总是为FIL_PAGE_TYPE_FSP_HDR，负责记录整个表空间的使用情况和标志位的状态信息。页面中，紧跟着通用页头FIL Header的是File Space Header结构，共112字节。 接着，每个XDES entry结构40个字节，描述一个extent的使用情况。一个File Space Header Page内最多包含256个XDES entry，即该page同时用于跟踪随后的256个extent(约256MB文件大小)的空间管理，所以每隔256MB就要创建一个类似的数据页，类型为FIL_PAGE_TYPE_XDES，XDES Page除了File Space Header结构全填0之外，其他都和FIL_PAGE_TYPE_FSP_HDR页具有相同的数据结构，可以称之为extent描述页。 File Space HeaderFile Space Header 的结构如下表： Macro OFFSet DESCRIPTIon FSP_SPACE_ID 0 space id FSP_NOT_USED 4 保留字段 FSP_SIZE 8 当前表空间的页面总数 FSP_FREE_LIMIT 12 当前未初始化的最小Page no FSP_SPACE_FLAGS 16 标志位信息 FSP_FRAG_N_USED 20 FSP_FREE_FRAG链表上已被使用的Page数，用于快速计算该链表上可用空闲Page数 FSP_FREE 24 所有page都没有被使用的extent链表 FSP_FREE_FRAG 40 有可分配的page的extent链表 FSP_FULL_FRAG 56 所有page均已被分配的extent链表 FSP_SEG_ID 72 新生成段的id（current max seg id +1） FSP_SEG_INODES_FULL 80 已被完全用满的Inode Page链表 FSP_SEG_INODES_FREE 96 至少存在一个空闲Inode Entry的Inode Page被放到该链表上 FSP_FREE_LIMIT记录当前未初始化的最小Page no，随着数据表的持续写入，表空间也在不断增长，InnoDB引擎需要对当前的存储文件进行扩展（涉及磁盘IO操作），为了减少扩展的次数，每次回多预留一些空间，这些未被纳入管理的剩余页面，就是通过FSP_FREE_LIMIT来标识。FSP_FREE，FSP_FREE_FRAG，FSP_FULL_FRAG三个字段根据整个表空间中extent（包括当前File Space Header Page所能管理的256个extent之外的所有extent）的使用情况，划分为三个链表进行维护。FSP_SEG_INODES_FULL，FSP_SEG_INODES_FREE两个字段根据Inode page页的使用情况，划分为已填满和可分配两个链表进行管理 List Node为了管理Page，Extent这些数据块，在文件中记录了许多的节点以维持具有某些特征的链表，例如在在文件头维护的inode page链表，空闲、用满以及碎片化的Extent链表等等。 由于这些链表节点信息都是写在实际磁盘上的，所以与常用的基于结构体和内存指针组成的内存链表不同，文件链表存储的地址信息则是通过4bytes的页号和2byte的页内偏移量组成的。 在InnoDB里链表头称为FLST_BASE_NODE，其结构如下表： Macro bytes desc FLST_LEN 4 列表长度 FLST_FIRST 6 列表首节点地址 FLST_LAST 6 列表尾节点地址 BASE NODE维护了链表的头指针和末尾指针，每个节点称为FLST_NODE，结构如下表： Macro bytes desc FLST_PREV 6 前驱节点地址 FLST_NEXT 6 后继节点地址 类似的，InnoDB引擎内部（内存中）对各种内存链表的结构也做了统一的封装，其基本结构与文件链表相同，具体代码实现如下： 123456789101112131415template &lt;typename TYPE&gt;struct ut_list_base { typedef TYPE elem_type; ulint count;/*!&lt; count of nodes in list */ TYPE* start;/*!&lt; pointer to list start, NULL if empty */ TYPE* end;/*!&lt; pointer to list end, NULL if empty */};template &lt;typename TYPE&gt;struct ut_list_node { TYPE* prev; /*!&lt; pointer to the previous node, NULL if start of list */ TYPE* next; /*!&lt; pointer to next node, NULL if end of list */}; 基础节点(base node)包含了链表的长度和首位指针信息。普通的元素节点则只包含前驱后继节点的信息。 XDES EntryXDES Entry 结构描述了一个extent内部所有page的使用情况(默认64page)。其结构如下表： macro offset DESC XDES_ID 0 extent所属的segment id XDES_FLST_NODE 8 维持Extent链表的双向指针节点 XDES_STATE 20 extent的使用状态 XDES_BITMAP 32 维护page使用情况的位图 XDES_STATE可能取值的状态为： MAcro VALUE desc XDES_FREE 1 分区在file space的free list上 XDES_FREE_FRAG 2 在file space的free fragment list上 XDES_FULL_FRAG 3 在file space的full fragmentlist上 XDES_FSEG 4 该extent在XDES_ID的SEGMENT上 通过XDES_STATE信息，我们只需要一个FLIST_NODE节点就可以维护每个extent的信息，是处于全局表空间的链表上，还是某个btree segment的链表上。 XDES_BITMAP中每一个page占两位，共64*2=128bit，其中一位XDES_FREE_BIT表示当前page的使用情况，另外一位XDES_CLEAN_BIT暂未使用。 Insert Buffer Page插入缓存页，暂不深入讨论。 Inode PageInnoDB使用Inode Page来维护段（Segment）的使用信息，一般idb文件系统空间的第三页就是Inode Page。数据即索引，在创建一个b-tree索引时，默认生成2个segment，分别用于管理b-tree的中间节点和叶子节点。 Inode Page的内部组织方式如上述物理视图，出去页面通用页头和页尾之外，主要包含两个部分：Inode entry list和Inode page list。 Inode Entry每个Inode Entry描述了一个段内的extent以及page的使用情况，具体的结构如下： Macro offset desc FSEG_ID 0 Segment id(0表示未使用) FSEG_NOT_FULL_N_USED 8 FSEG_NOT_FULL链表上已被使用的Page数量 FSEG_FREE 12 分配给该Segment完全且未使用的extent链表 FSEG_NOT_FULL 28 部分可用的extent链表 FSEG_FULL 44 已使用完的extent链表 FSEG_MAGIC_N 60 魔数 FSEG_FRAG_ARR[0] 64 碎页数组首页地址 … … … FSEG_FRAG_ARR[31] 64+32*4 碎页数组尾页地址 除了维持当前segment下的extent链表外，为了节省内存，减少碎页，每个Entry还占有32个独立的page用于分配，每次分配时总是先分配独立的Page，当填满32个数组项时，再在每次分配时都分配一个完整的Extent，并在XDES PAGE中将其Segment ID设置为当前的段id。 Page List一个Innode page里最多存放85个entry，对于idb文件来说，除去回滚段之外，只有索引段，除非为一张表申请42个索引，否则一个Innode page足够使用。而如果未开启innodb_file_per_table选项的数据库来说，所有的信息都存储在ibdata文件，可能需要多个Inode page来维护段信息。Innodb将这些Inode page通过链表的方式组织起来，在页头之后存储了链表的base节点，具体可以参考List Node一节对通用链表结构的描述，此处不再详述。 总结本文以idb文件为切入点，介绍了Innodb引擎对底层文件的组织方式。在idb文件中，除去实际存储数据和索引节点信息的Index Page之外（后续介绍页内组织方式的时候详细阐述），其他类型的page都做了简要的介绍，各页面之间相互索引和定位的组织方式如下图： 自底向上来看，第一页Page0对应File space header页，其内部的256个entry数组，记录了前256个extent的使用情况。第二页Page1是插入缓存页。第三页Page2是Inode Page，其内部的每一个entry，记录了当前段内个分区的使用情况，以链表的方式组织起来，此外还各自单独占据了一部分独立的Page用于分配。第四页是在我们创建索引后，为根节点独立分配的一个root page，其中记录了中间节点段和叶子节点段的地址信息。 参考文献 MySQL · 引擎特性 · InnoDB 文件系统之文件物理结构 MySQL系列：innodb源码分析之表空间管理 The physical structure of InnoDB index pages","link":"/2018/11/13/mysql-tablespace/"},{"title":"MySQL索引二三事","text":"MySQL索引原理索引在MySQL中也叫做键(Key), 是存储引擎层面实现，用于快速找到记录的一种数据结构。 索引的Pros &amp; Cons索引可以让服务器快速定位到表的指定位置。除了这个作用，索引还有其他的附加作用。 索引大大减少了服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表。（由于B+树索引是按照顺序存储数据的，所以MySQL可以用来做ORDER BY和Group BY操作。因为数据是有序的，所以B+tree会将相关的列值都存储在一起。最后因为索引中存储了实际的列值，所以某些查询只使用索引就能够完成全部查询。） 索引可以将随机I/O变为顺序I/O 但是：索引并不总是最好的解决方案。总的来说，只有当索引帮助索引引擎快速查找到记录带来的好处大于其带来的额外工作时，索引才是有效的。对于非常小的表，大部分情况下简单的全表扫描更加高效。对于中到大型的表，索引就非常有效。但是对于特大型的表，建立和使用索引的代价将随之增长 - 这种情况下需要分区 - 直接区分查询需要的一组数据。 如果表的数量特别多，可以建立一个元数据信息表。用来查询需要用到的某些特性。例如执行哪些需要聚合多个应用分布在多个表的数据的查询，则需要记录“哪个用户的信息储存在哪个表中”的元数据，这样在查询时候就可以直接忽略哪些不包含指定用户信息的表。。对于TB级别的数据，定位单条记录的意义不大，所以经常使用块级别元数据技术替代索引。 当一个查询慢了怎么办：当一个表中, 存储了 N 多数据的时候, 操作会变得很慢, 怎么解决？实际实例：http://moon-walker.iteye.com/blog/2377643 建索引的几大原则1.最左前缀匹配原则非常重要的原则，MySQL会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配.比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 如下图people表的索引，该索引显示了该索引是如何组织数据的存储的，索引中包含了last_name, first_name和dob的值。123456create table people( last_name varchar(50) not null, first_name varchar(50) not null, dob date not null, key(last_name, first_name, dob)) 2.=和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式 3.尽量选择区分度高的列作为索引,区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录 4.索引列不能参与计算，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’);5.尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可 6.索引列不能是表达式的一部分，也不能是函数的参数。 7. 前缀索引当需要索引的列字符很长情况下需要使用前缀索引。如果直接索引这个很长的列，会让索引变得大且慢。前缀索引索引列开始的部分字符。对于BLOB,TEXT或者很长的VARCHAR类型的列，必须使用前缀索引。 如何选择合适的前缀长度呢？关注区分度。可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。123456789select count(distinct email) as L from SUser;//算出列上有多少个不同的值； // 然后依次选取不同长度的前缀来看这个值mysql&gt; select count(distinct left(email,4)）as L4, count(distinct left(email,5)）as L5, count(distinct left(email,6)）as L6, count(distinct left(email,7)）as L7,from SUser; 增加前缀索引1alter table xxx add key(city(7)) 前缀索引的缺点：1、MySQL无法使用前缀索引做ORDER BY和Group BY.2、可能造成额外的扫描。因此使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多查询成本。3、使用前索引将无法使用覆盖索引。 那对于很长的字符串，怎么既节省空间的存储，又能使用到索引，又能避免前缀索引的缺点？ 直接创建完整索引，这样可能比较占用空间； 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引； 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题； 创建hash字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。 8.索引合并 Index Merge - 通过使用表中的多个单列索引来定位指定的行。SQL查询语句where可能有多个条件涉及到多个字段，它们之间进行AND或者OR，那么此时就有可能会用到Index Merge技术。Index Merge技术简单的说，就是对同一个表使用多个索引分别进行条件扫描，然后将它们各自的结果进行合并 （intersect / union).虽然索引合并策略是一种优化的结果，但是实际上更多说明表上的索引建的糟糕：当出现服务器对多个索引做and、or操作时候，通常需要一个多列索引，而不是单个独立的单列索引。更重要的是，优化器不会把这些计算到查询成本中，优化器只会关心随机页面的读取，这回使得查询成本被低估，导致该执行计划还不如直接走权标扫描。 因此如果explain 中看到有Index Merge，应该好好检查一下查询 和表的结构，看是不是已经是最优的。也可以通过参数optimizer_switch 来关闭索引合并功能。 9.聚簇索引聚簇索引并不是一种单独的索引类型，而是一种数据存储方式,具体细节依赖于不同的引擎实现方式。InnoDB的聚簇索引在同一个结构中保存了B+树的索引和数据行（存储数据的顺序和索引的顺序一致）。当表有聚簇索引时候，它的数据行实际上存放在索引的叶子页（leaf page）中。 术语“聚簇”表示数据行和相邻的键值建凑的存储在一起。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。 InnoDB只能通过主键聚集数据 - 即聚簇索引只能是主键索引。一般情况下主键会默认创建聚簇索引，且一张表只允许存在一个聚簇索引。InnoDB的主键采用聚簇索引索引存储，使用B+树作为索引结构，但是叶子节点存储的是索引值和数据本身。 Innodb的二级索引不使用聚簇索引，叶子节点存储的是KEY字段+主键值（因为不存储指针，索引不需要更新索引的指标 - 这减小了移动数据或者数据页面分裂时维护二级索引的开销）。因此，通过二级索引查询首先查到的是主键值，然后Innodb再根据查到的主键值通过朱建索引找到数据块。 Innodb 主键索引存储数据方式： 聚簇索引的每个叶子节点都包含了主键值，事务ID,用于事务和MVCC的回滚指针以及所有的剩余列。如果主键是一个列前缀索引，InnoDB也会包含完整的主键列和剩下的其他列。 下图展示了聚簇索引的记录是如何存放的，leaf page存放了行的全部数据，但是节点页只包含了索引列。下图展示了二级索引的叶子节点是如何存放数据的，叶子节点存放索引key和主键值。 下图描述了InnoDB和MyISAM如何存放表的抽象图。 聚簇索引 Pros &amp; Cons聚簇索引优点 聚集索引表记录的排列顺序与索引的排列顺序一致，优点是查询速度快，因为一旦具有第一个索引值的记录被找到，具有连续索引值的记录页一定物理上紧跟其后。可以把相关数据保存在一起，减少磁盘IO. 数据访问更快 - 索引和数据保存在同一个Btree，因此获得数据比非聚簇索引快 使用聚簇索引扫描的索引可以直接使用页节点的主键值。 聚簇索引缺点 最大限度地提高了I/O密集型应用的性能，但如果数据全部都存放在内存中，则访问的顺序就没那么重要了。 插入速度严重依赖插入顺序。按照主键的顺序插入是加载数据到InnoDB表中速度最快的方式，反思如果不是按照主键顺序加载数据，那么在加载完成后最后使用optimize table命令重新组织一下表。插入数据时速度要慢（时间花费在 “物理存储的排序” 上，也就是首先要找到位置然后插入）。 更新聚簇索引列的代价很高，因为会强制INNODB将每个被更新的行移动到新的位置。 基于聚簇索引的表插入新行，或者被更新需要移动行的时候，可能面临“页分裂 page split”问题。当行的主键值要求必须将这一行插入到某个已满的页中，存储引擎会将该页分裂成两个页面来容纳该行，这就是一次page split. page split 会导致表占用更多的磁盘空间。 二级索引访问需要使用两次查找。 聚集索引V.S.非聚集索引聚集索引和非聚集索引的根本区别是表记录的排列顺序和索引的排列顺序是否一致。简单点来说，就是聚集索引: 物理存储按照索引排序；非聚集索引: 物理存储不按照索引排序。 聚集索引叶节点是数据，非聚集索引叶节点是指向索引叶节点的指针。 10.覆盖索引。如果索引包含所有满足查询需要的数据，那么索引即覆盖索引(Covering Index), 也就是不需要回表操作。覆盖索引是非常有用的工具，它使用索引就可以直接获取列数据，这样就不需要再回表查询，能够极大地提高性能。 覆盖索引是一种非常强大的工具，能大大提高查询性能，只需要读取索引而不用读取数据有以下一些优点:1、索引项通常比记录要小，所以MySQL访问更少的数据2、索引都按值的大小顺序存储，相对于随机访问记录，需要更少的I/O3、大多数据引擎能更好的缓存索引，比如MyISAM只缓存索引4、覆盖索引对于InnoDB表尤其有用，因为InnoDB使用聚集索引组织数据，如果二级索引中包含查询所需的数据，就不再需要在聚集索引中查找了 判断标准： 使用explain, 通过输出的extra来判断，对于一个索引覆盖查询，显示为using index。 MySQL查询优化器在执行查询前会决定是否有索引覆盖查询。 注意：如果使用覆盖索引，一定要注意select列表值取出需要的列，不可以是select *. 因为如果将所有字段一起做索引会导致索引文件过大，查询性能下降。 11.使用索引扫描来做排序MySQL有两种方式可以生成有序的结果，通过排序操作或者按照索引顺序扫描，如果explain的type列的值为index，则说明mysql使用了索引扫描来做排序（不要和extra列的Using index搞混了，那个是使用了覆盖索引查询）。扫描索引本身是很快的，因为只需要从一条索引记录移动到紧接着的下一条记录，但如果索引不能覆盖查询所需的全部列，那就不得不扫描一条索引记录就回表查询一次对应的整行，这基本上都是随机IO，因此按索引顺序读取数据的速度通常要比顺序地全表扫描慢，尤其是在IO密集型的工作负载时。 MySQL索引数据结构剖析B+-tree mysql可以使用同一个索引既满足排序，又用于查找行，因此，如果可能，设计索引时应该尽可能地同时满足这两种任务，这样是最好的。只有当索引的列顺序和order by子句的顺序完全一致，并且所有列的排序方向（倒序或升序，创建索引时可以指定ASC或DESC）都一样时，mysql才能使用索引来对结果做排序，如果查询需要关联多张表，则只有当order by子句引用的字段全部为第一个表时，才能使用索引做排序，order by子句和查找型查询的限制是一样的，需要满足索引的最左前缀的要求，否则mysql都需要执行排序操作，而无法使用索引排序。 MySQL的查询优化器会通过两个API来了解存储引擎的索引值的分布信息，以决定如何使用索引。第一个API是records_in_range()，通过向存储引擎传入两个边界值获取在这个范围大概有多少条记录。对于MYISAM该接口返回精确值；INNODB，该接口返回估算值。第二个是info()，该接口返回各种类型的数据，包括索引的基数 - 每个key有多少条记录。 cardinlity: 索引列的基数，显示了存储引擎估算索引列有多少不同的取值。","link":"/2018/05/21/mysql-reading-notes/"},{"title":"RocketMQ源码分析2--NameServer","text":"本文是RocketMQ源码分析系列之二，如有疑问或者技术探讨，可以email me,欢迎探讨. NameServer是类似于Zookeeper的分布式服务治理可单独部署的模块。主要负责管理集群的路由信息，包括Topic队列信息和Broker地址信息。 先来讲讲什么是分布式系统的服务治理？ 服务治理在分布式系统的构建中是类似血液一样的存在。随着分布式系统服务的越来越多，而每个服务都可以横向扩展出多个机器，因此服务的各种状况都可能出现。服务可能出现某台机器宕机，如果用户的请求刚好打到宕机的机器，就会造成服务不可用；当提供服务的机器发生变化，也需要将机器的调用IP通知服务的调用方。另外当服务的主机有多台的时候，如何负载均衡地分发请求也是需要考虑的。 服务治理具体为服务注册和服务发现。它作为一个管理中心一样的存在，解耦provider和consumer，需要分布式强一致性的存储服务的IP地址以及端口。目前业界常见的产品为Zookeeper，Zookeeper使用Paxos保证强一致性。 在同一个分布式集群中的进程或服务，互相感知并建立连接，这就是服务发现。服务发现最明显的优点就是零配置，不用使用硬编码的网络地址，只需服务的名字，就能使用服务。在现代体系架构中，单个服务实力的启动和销毁很常见，所以应该做到无需了解整个架构的部署拓扑，就能找到整个实例。复杂的服务发现组件要提供更多的功能，例如服务元数据存储，监控监控，多种查询和实时更新等。 在服务治理之前，RPC调用使用点对点方式，完全通过人为进行配置操作决定，运维成本高且容易出错，且整个系统运行期间的服务稳定性也无法很好感知。因此需要设计出包含基本功能服务的自动注册，客户端的自动发现，变更下发的服务治理。 NameServer作用当多Broker Master部署时，Master之间是不需要知道彼此的，这样的设计降低了Broker实现的复杂性。这样在分布式环境下，某台Master宕机或上线，不会对其他Master造成任何影响。那么怎样才能知道网络中有多少台Master和Slave？ NameServer作用1: Broker在启动的时候会去NameServer注册，Nameserver会维护Broker的状态和地址。 NameServer作用2：Nameserver还维护了一份Topic和Topic对应队列的地址列表，broker每次发送心跳过来的时候都会把Topic信息带上。客户端依靠Nameserver决定去获取对应topic的路由信息，从而决定对那些Broker做连接。 NameServer作用3：接收Producer和Consumer的请求，根据某个topic获取到对应的broker的路由信息，即实现服务发现功能。 Nameserver作用4：用来保存所有broker的Filter列表。 Nameserver启动NameServer的启动主要是初始化一个通信组件中的NettyRemotingServer实例，用来进行网络通信。另外NameServer还启动定时任务，用于broker存活检测。 RouteInfoManager 是NameServer核心类，它管理broker的路由信息，topic队列信息。其数据结构如下： 123456789 RouteInfo{ ... private final HashMap&lt;String/* topic */, List&lt;QueueData&gt;&gt; topicQueueTable; // topic队列表，存储每个topic包含的队列数据 private final HashMap&lt;String/* brokerName */, BrokerData&gt; brokerAddrTable; // broker的地址表 private final HashMap&lt;String/* clusterName */, Set&lt;String/* brokerName */&gt;&gt; clusterAddrTable;//集群主备信息表 private final HashMap&lt;String/* brokerAddr */, BrokerLiveInfo&gt; brokerLiveTable; // BrokerLiveInfo 存储了broker的版本号，channel以及最近心跳时间等信息 private final HashMap&lt;String/* brokerAddr */, List&lt;String&gt;/* Filter Server */&gt; filterServerTable; // 记录了每个broker的filter信息 ...} NameServer初始化步骤： 路由信息的管理（Topic &amp; Broker）Broker启动时候/topic配置变更/每隔30s（Broker启动时候的定时任务），broker会发起register到namserver的动作，把broker自己的基础数据以及维护的topic 一起提交给nameserver进行管理，同时把broker注册到NameServer 这些信息最后组成RouteInfo路由信息，由生产者和消费者来使用。 123456789RegisterBrokerResult registerBrokerResult = this.brokerOuterAPI.registerBrokerAll(// this.brokerConfig.getBrokerClusterName(), // this.getBrokerAddr(), //broker地址 this.brokerConfig.getBrokerName(), //broker名称 this.brokerConfig.getBrokerId(), //broker id this.getHAServerAddr(), // topicConfigWrapper,//所有topic this.filterServerManager.buildNewFilterServerList(),// oneway); 心跳检查NameServer启动初始化过程，会异步启动定时任务线程，定时跑2个任务，监听broker的存活情况。第一个：每隔10分钟扫描出不活动的broker,然后从routeinfo删除并关闭broker和nameserver通信的channel。第二个：每隔10分钟定时打印configTable的信息 1234567this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { NamesrvController.this.routeInfoManager.scanNotActiveBroker(); }}, 5, 10, TimeUnit.SECONDS); 1234567this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { NamesrvController.this.kvConfigManager.printAllPeriodically(); }}, 1, 10, TimeUnit.MINUTES); 通过定时扫描操作实现了服务治理中，如果出现机器宕机，可以自动摘除挂掉的机器，仍然保证服务可用。 服务发现接收Producer和Consumer的请求，根据某个topic获取到对应的broker的路由信息，即实现服务发现功能当Broker收到Producer发送的消息并判断1RequestCode = GET_ROUTINEINFO_BY_TOPIC 之后，会调用默认注册的请求处理器（DefaultRequestProcessor）的getRouteInfoByTopic方法，该方法根据Topic获取对应的broker路由信息.1TopicRouteData topicRouteData = this.namesrvController.getRouteInfoManager().pickupTopicRouteData(requestHeader.getTopic()); pickupTopicRouteData关于获取Topic BrokerRouteInfo核心代码：123456789...this.lock.readLock().lockInterruptibly();List&lt;QueueData&gt; queueDataList = this.topicQueueTable.get(topic);if (queueDataList != null) { topicRouteData.setQueueDatas(queueDataList);...}this.lock.readLock().unlock(); 具体代码流程如下：RouteInfoMananger的topicQueueTable记录了topic名称与broker队列[broker名称]的映射关系，而brokerAddrTable记录了brokerName与broker地址的映射关系，因此基于topicQueueTable和brokerAddrTable这两个关键属性即可以根据topic获取对应的broker路由地址。 另一个角度看NameServer通过看源码NameServer一共有8类线程： 线程名称 作用 核心方法 ServerHouseKeepingService:type Timer NSScheduledThread1:type scheduledExecutorService 定时任务线程，定时跑2个任务。监听broker的存活 第一个：每隔10分钟扫描出不活动的broker,然后从routeinfo删除 第二个：每隔10分钟定时打印configTable的信息 第一个：scanNotActiveBroker 每10秒扫描一次brokerLiveTable.怎么判断broker是不活动的呢？brokerLiveInfo上次发送过来【注册broker】的更新时间(lastUpdateTimestamp)+设置的broker超时时间 &lt; 系统当前时间，则说明此broker不活动了。第二个：每隔10秒日志打印KVConfig. EventLoopGroupBoss:type EventLoopGroup Netty的boss线程（accept线程)，负责处理客户端的TCP连接请求。 EventLoopGroupSelector:type EventLoopGroup Netty的worker线程 是真正负责I/O读写操作的线程组。通过ServerBootstrap的group方法进行设置，用于后续的channel绑定 NettyEventExecuter 一个单独的线程，监听NettyChannel状态变化通知ChannelEventListener做出响应的动作 defaultEventExecutorGroup Netty编解码 NettyServerPublicExecutor MQ消息根据RequestCode被分成了很多种类，每一个种类都对应一个处理器。每个处理器在处理消息时候需要在一个独立的线程池里执行 (线程模型问题，可能消息处理会设计比较复杂的操作，不适合放在与channel绑定的单线程里操作，防止阻塞)。如果调用方没有提供，就使用remotingserver自带的这个线程池。 RemotingExecutorThread_ Nameserver自己的processor在此线程池里执行 快速问答 NameServer怎么知道有多少broker机器？RouteInfoManager 有一个brokerLiveInfo列表保存当前存货的broker机器，可以从这里get到。 为什么不用Zookeeper而自己开发了一个NameServer?引用自首先，ZooKeeper可以提供Master选举功能，比如Kafka用来给每个分区选一个broker作为leader推荐看此文，但对于RocketMQ来说，topic的数据在每个Master上是对等的，没有哪个Master上有topic上的全部数据，所以这里选举leader没有意义； 其次，RockeqMQ集群中，需要有构件来处理一些通用数据，比如broker列表，broker刷新时间，虽然ZooKeeper也能存放数据，并有一致性保证，但处理数据之间的一些逻辑关系却比较麻烦，而且数据的逻辑解析操作得交给ZooKeeper客户端来做，如果有多种角色的客户端存在，自己解析多级数据确实是个麻烦事情； 既然RocketMQ集群中没有用到ZooKeeper的一些重量级的功能，只是使用ZooKeeper的数据一致性和发布订阅的话，与其依赖重量级的ZooKeeper，还不如写个轻量级的NameServer，NameServer也可以集群部署，NameServer与NameServer之间无任何信息同步，只有一千多行代码的NameServer稳定性肯定高于ZooKeeper，占用的系统资源也可以忽略不计，何乐而不为？ 参考 https://www.jianshu.com/p/3e025cf69a6a http://blog.csdn.net/earthhour/article/details/78718064","link":"/2018/02/23/nameserver/"},{"title":"Nginx高性能和高扩展性背后的设计原理[译]","text":"原文 : [https://www.nginx.com/blog/inside-Nginx-how-we-designed-for-performance-scale/] Nginx 以其作为高性能高并发web服务器著称，而其高性能的表现皆依赖于其软件的架构设计。尽管有众多web服务器和应用服务器使用了单一进程/线程模型，Nginx 以其成熟的事件驱动模型脱颖而出。依赖其事件驱动模型，在现在的硬件条件下Nginx 能够支持大量的并发连接。 这篇文章将详细讲解Nginx 是如何工作的。 Nginx 进程模型为了理解Nginx 的架构设计，你应该了解Nginx 谁如何运行的。Nginx 有一个主进程（master process）和 若干工作进程（working processes）. 主进程拥有其他进程没有的权限，例如读取配置文件，绑定端口等。123456789101112&gt;# service Nginx restart* Restarting Nginx# ps -ef --forest | grep Nginxroot 32475 1 0 13:36 ? 00:00:00 Nginx: master process /usr/sbin/Nginx \\-c /etc/Nginx/Nginx.confNginx 32476 32475 0 13:36 ? 00:00:00 \\_Nginx: worker processNginx 32477 32475 0 13:36 ? 00:00:00 \\_Nginx: worker processNginx 32479 32475 0 13:36 ? 00:00:00 \\_Nginx: worker processNginx 32480 32475 0 13:36 ? 00:00:00 \\_Nginx: worker processNginx 32481 32475 0 13:36 ? 00:00:00 \\_Nginx: cache manager processNginx 32482 32475 0 13:36 ? 00:00:00 \\_Nginx: cache loader process 在四核机器上，Nginx 主进程创建四个工作进程和两个用来管理硬盘内容缓存的辅助进程。 为什么Nginx架构如此重要？Unix 应用的基础是进程／线程(从linux OS的角度，进程和线程没有什么区别，他们的区别最大程度就说多大程度上他们共享存储空间)。一个进程或者线程是运行在cpu内核上的指令容器实例。大多数复杂的应用出于以下两个原因的考虑，会以多线程或者多进程的方式运行： 应用可以同时使用更多计算机内核计算。 可以很简单地实现并行操作（例如，同时处理大量连接） 然而，进程和线程使用并且消耗大量内存和操作系统其它资源。虽然现代操作系统可以同时处理大量体量小的进程/线程, 但是随之而来的是一旦内存不够带来的性能急剧下降，以及进程频繁切换导致的cpu资源浪费。 在设计网络应用时候，最普遍的方法是为每个连接分配一个线程/进程。 这种结构简单且容易实现，但是难以支持高并发连接。 Nginx是如何工作的？Nginx使用可以根据硬件资源来调节的进程模型。 master process 一个master 进程，只有它能够读配置，绑定端口等。master进程能够创建其他子进程为他干活。 cache loader process 在启动时候把硬盘缓存读进内存，它的资源消耗很低。 cache manager process 阶段性运行，管理缓存。 worker processes 实际干活的进程. 工作进程处理网络连接，从硬盘读写内容，和上游服务器通信。 推荐Nginx 配置: worker processes的数量应该和计算机CPU数量相同，以此来最大程度利用计算机硬件。你可以如下配置实现： 1worker_process auto; 当一个Nginx 服务器是活跃状态时候，只有工作进程们是忙碌的。每个工作进程以非阻塞形式处理大量的连接，以此减少多进程造成的上下文切换资源浪费。每个工作进程都是独立运行的单线程进程，可以处理大量新的网络连接。工作进程之间通过共享内存通信共享缓存数据，会话持续数据，以及其它共享资源。 master进程会根据Nginx配置创建工作进程，并且分配给工作进程大量的监听套接字(listen sockets). Nginx工作进程当等到监听套接字上的事件(accept_mutex和kernel socketsharding)时候会开始工作。新的连接到来时候会创建一个事件。这些连接被会分配给状态机（stat emachine）——HTTP状态机是最常用的，但Nginx还为流（原生TCP）和大量的邮件协议（SMTP，IMAP和POP3）实现了状态机。 状态机本质上是一组告知Nginx如何处理请求的指令，大多数和Nginx具有相同的web服务器也使用类似的状态机，只是实现不同罢了。 调度状态机我们可以将状态机想象成国际象棋的规则。每个HTTP事务都是一局象棋比赛：棋盘的一端是web服务器（一个可以迅速做出决定的象棋大师），另一端是通过浏览器和较慢的网络进行操作的客户端（普通象棋选手）。然而游戏的规则是非常复杂的。例如，web服务器可能需要与各方沟通(代理一个上游的应用程序)，或者和认证服务器交流。web服务器的第三方模块甚至可以拓展比赛规则。 阻塞状态机回忆一下我们之前对进程/线程的描述：是一组操作系统可调度的、运行在CPU内核上的指令集容器实例。大多数web服务器和web应用都使用进程(线程)连接一对一的模型去参与象棋比赛。每个进程或线程都包含一个将比赛玩到最后的指令。在这个过程中，进程是由服务器来运行的，它的大部分时间都花在“阻塞（blocked）”上，等待客户端完成其下一个动作： web服务器进程在监听套接字上监听新的连接（新的连接可以理解为客户端发起的新比赛）。 一局新的比赛发起后，服务器进程就开始工作，每一步棋下完后都进入阻塞状态，等待客户端回复（客户端走下一步棋）。 一旦比赛结束，web服务器进程会看看客户是否想开始新的比赛（这相当于一个存活的连接）。如果连接被关闭（客户端离开或者超时），web服务器进程会回到监听状态，等待下一句的比赛。 最重要的一点是每个http连接都需要一个专门的进程。这样的架构虽然简单且容易实现，但是这会造成一个巨大的失衡：一个以文件描述符和少量内存为代表的轻量级HTTP连接，被映射到一个单独的进程或线程——它们是非常重量级的操作系统对象。这在编程上是方便的，但它造成了巨大的浪费。 Nginx是真正的大师也许你听说过车轮表演赛，在比赛中一个大师级选手同时和多个对手比赛 — 这也是Nginx 工作进程如何“下棋”。每个工作进程都是一个象棋大师，可以同时处理大量的连接。 工作进程等待监听套接字和连接套接字上的事件； 套接字上发生事件，工作进程会去处理这些事件。- 监听socket上的事件意味着棋手发起了一轮新的象棋比赛，工作进程会创建一个新的连接套接字 - 连接socket上的事件意味着棋手下了一步棋。工作进程会立刻回复。 在网络中工作进程永远不会因为去等待对手下棋而阻塞。车轮战中，象棋大师跟一个对手下了一步棋后马上会继续和其他对手下棋。 为什么nginx这种单路复用的架构优于多进程阻塞架构？Nginx的架构可以让每个工作进程支持几十万的连接。每个连接创建一个文件描述符，消耗少量的工作进程的内存。可以说每个进程上新增加一个连接额外的代价是非常小的。Nginx的工作进程可以保持固定的CPU占用率，并不浪费CPU资源。进程上下文切换也并不频繁。 在连接和进程一对一的阻塞架构上，每个连接需要大量的附加资源和开销，上下文切换是十分频繁的。 通过适当的系统调优，NGINX能大规模地处理每个工作进程数十万并发的HTTP连接，并且能在流量高峰期间不丢失任何信息。 配置更新和升级nginx通过使用少量的工作进程，Nginx的进程模型使得配置、甚至是二进制文件本身的更新都非常高效。 更新Nginx配置是很简单，轻量级并且可靠的操作。可以通过1nginx -s reload 更新加载Nginx的配置，该命令会检查磁盘上的配置，并给主进程发送一个SIGHUP信号。 当主进程接收到一个SIGHUP信号，主进程会做： 重启配置，fork一套新的工作进程。这些新的工作进程会立即开始接受连接和处理流量（使用新的配置）。2.给旧的那套工作进程发信号。工作进程停止接收新的网络连接。只要它们处理的HTTP请求结束了，它们就会干净地关闭连接。一旦所有的连接都被关闭，工作进程也就退出了。 这个过程会导致CPU占用率和内存使用的一个小高峰，但相比于从活动的连接中加载资源，这个小高峰可忽略不计。你可以在一秒内重新加载配置多次。极少情况下，一代又一代工作进程等待连接关闭时会出现问题，但即便出现问题，它们也会被立即解决掉。 结论Inside NGINX infographic总结了Nginx是如何运转的，看似简单，背后是针对Nginx数十年的创新和优化，这才造就了Nginx在多种硬件上表现出良好的性能，同时还具备现代web应用所需要的安全性和可靠性。","link":"/2017/10/29/nginx-principle/"},{"title":"从数据库连接池想到的","text":"Merry Christmas and 本文特别鸣谢Master Eric刘. 长连接 VS 短连接先澄清个概念，我们通常说的长连接和短连接其实是TCP连接。因为HTTP是请求/响应模式，只要服务端给了响应，本次HTTP连接就结束了。而TCP连接是一个双向的通道，它是可以保持一段时间不关闭的，因此TCP连接才有真正的长连接和短连接一说。 所以“HTTP连接”这个词就不应该出现，更准确的是HTTP请求和HTTP响应，而HTTP请求和HTTP响应都是通过TCP连接这个通道来回传输的1。 那什么是长连接和短连接呢？ 短连接是指通信双方接收完数据后立刻断开连接； 长连接是指通信双方接收完数据后不断开连接，而是保持一段时间连接，可以继续传输数据。 为什么要有长连接？建立连接代价太大，长连接意味着连接会被复用。长连接可以使多个HTTP连接复用同一个TCP连接，这就节省了很多TCP连接建立和断开消耗。 比如你请求了博客园的一个网页，这个网页里肯定还包含了CSS、JS等等一系列资源，如果你是短连接（也就是每次都要重新建立TCP连接）的话，那你每打开一个网页，基本要建立几个甚至几十个TCP连接，这得浪费了多少资源… 但如果是长连接的话，那么这么多次HTTP请求（这些请求包括请求网页内容，CSS文件，JS文件，图片等等），其实使用的都是一个TCP连接，很显然是可以节省很多消耗的。1。 另外，长连接并不是永久连接的。如果一段时间内（具体的时间长短，是可以在header当中设置的，即设置超时时间）这个连接没有HTTP请求发出的话，那么这个连接就会被断掉。 怎么设置长连接？WEB类请求：客户端HTTP请求header设置支持Connection=keep-alive支持长连接。服务端Nginx设置支持：123http{keepalive_timeout 60} TCP类请求：可以使用Netty建立长连接。 连接池对于共享资源，有一个很著名的设计模式：资源池。该模式正是为了解决资源频繁分配、释放所造成的问题的。把该模式应用到数据库连接管理领域，就是建立一个数据库连接池；（类似还有线程池以及Redis连接池），提供一套高效的连接分配，使用策略，最终目标是实现连接的高效、安全的复用2。 数据库连接池数据库连接池的基本原理是在内部对象池中维护一定数量的数据库连接，并对外暴露数据库连接获取和返回方法（注意是返回不是关闭）。外部使用者可以通过getConnection方法获取连接，使用完毕后再通过realeaseConnection方法将连接返回，注意此时连接并没有关闭，二手由连接池管理器回收，并为下一次使用做好准备。 数据库连接池技术带来的优势：1.资源重用 - 避免频繁创建、释放连接引起的大量性能开销。2.更快的系统响应速度。3.统一的连接管理，避免无尽的连接导致数据库连接泄露。4.更为重要是我们可以通过连接池的管理机制监视数据库的连接数量，使用情况。 在较为完备的数据库连接实现中，可根据预先的连接占用超时设定，强制回收被占用连接。从而避免常规数据库连接中可能出现的资源泄露。 实践 Question: Java项目中是怎么使用数据库连接池的？怎么取，怎么返回？ Answer: 如果不用连接池，最原始的版本是手动建立连接，手动释放（生产环境不这么用）123456789101112131415161718192021222324252627282930313233343536373839404142public static void simple() throws SQLException, ClassNotFoundException { Class.forName(\"com.mysql.jdbc.Driver\"); System.out.println(\"成功加载驱动\"); PreparedStatement psmt = null; Connection connection = null; ResultSet resultSet = null; RedisProperties.Jedis jedis=new RedisProperties.Jedis(); jedis.getPool(); try { String url = \"jdbc:mysql://xx.xx.xx.xx:3306/xxxx?characterEncoding=UTF-8&amp;useSSL=false\"; connection = DriverManager.getConnection(url, \"root\", \"123456\"); System.out.println(\"成功获取连接\"); psmt = connection.prepareStatement(\"select 1\"); resultSet = psmt.executeQuery(); while (resultSet.next()) { System.out.println(resultSet.getString(1)); } System.out.println(\"成功操作数据库\"); } catch (Throwable t) { // TODO 处理异常 t.printStackTrace(); } finally { if (resultSet != null) { resultSet.close(); } if (psmt != null) connection.close();{ psmt.close(); } if (connection != null) { connection.close(); } System.out.println(\"成功关闭资源\"); } } 这样的缺点很显然, 每次都要客户端手动建立连接到数据库，再手动释放连接。如果忘记释放，将会连接越来越多，造成内存泄露。而且每次建立连接代价很大(时间长)，每次db操作的性能都很差。 生产环境用法：使用数据库连接池（例如下例Druid）, 连接池指定以下参数管理数据库连接： timeBetweenEvictionRunsMillis： 间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒。数据库连接池建立的连接都是长连接，但是在连接池在做管理的时候，会回收不活跃的连接。 initialSize：初始化时建立物理连接的个数。连接池初始化大小。 maxActive 最大连接池数量 minIdle 最小连接池数量 而每次获取连接和返回连接，是由MyBatis去实现。。 更多参数可以参考Druid参数官方文档3 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;bean id=\"dataSource\" class=\"com.alibaba.druid.pool.DruidDataSource\" init-method=\"init\" destroy-method=\"close\"&gt; &lt;!-- 基本属性 url、user、password --&gt; &lt;property name=\"url\" value=\"${jdbc.url}\"/&gt; &lt;property name=\"username\" value=\"${jdbc.user}\"/&gt; &lt;property name=\"password\" value=\"${jdbc.password}\"/&gt; &lt;!-- 配置初始化大小、最小、最大 --&gt; &lt;property name=\"initialSize\" value=\"30\"/&gt; &lt;property name=\"minIdle\" value=\"1\"/&gt; &lt;property name=\"maxActive\" value=\"500\"/&gt; &lt;!-- 配置获取连接等待超时的时间 --&gt; &lt;property name=\"maxWait\" value=\"6000\"/&gt; &lt;!-- 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒 --&gt; &lt;property name=\"timeBetweenEvictionRunsMillis\" value=\"6000\"/&gt; &lt;!-- 配置一个连接在池中最小生存的时间，单位是毫秒 --&gt; &lt;property name=\"minEvictableIdleTimeMillis\" value=\"30000\"/&gt; &lt;property name=\"validationQuery\" value=\"SELECT 'x'\"/&gt; &lt;property name=\"testWhileIdle\" value=\"true\"/&gt; &lt;property name=\"testOnBorrow\" value=\"false\"/&gt; &lt;property name=\"testOnReturn\" value=\"false\"/&gt; &lt;!-- 打开PSCache，并且指定每个连接上PSCache的大小 --&gt; &lt;property name=\"poolPreparedStatements\" value=\"true\"/&gt; &lt;property name=\"maxPoolPreparedStatementPerConnectionSize\" value=\"20\"/&gt; &lt;/bean&gt; &lt;bean id=\"sqlSessionFactory\" class=\"com.baomidou.mybatisplus.spring.MybatisSqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;!-- 自动扫描Mapping.xml文件 --&gt; &lt;property name=\"mapperLocations\" value=\"classpath:mybatis/mapper/*.xml\"/&gt; &lt;property name=\"configLocation\" value=\"classpath:mybatis/mybatis-config.xml\"/&gt; &lt;property name=\"typeAliasesPackage\" value=\"com.xiaojukeji.sec.data.*\"/&gt; &lt;property name=\"plugins\"&gt; &lt;array&gt; &lt;!-- 分页插件配置 --&gt; &lt;bean id=\"paginationInterceptor\" class=\"com.baomidou.mybatisplus.plugins.PaginationInterceptor\"&gt; &lt;property name=\"dialectType\" value=\"mysql\"/&gt; &lt;/bean&gt; &lt;/array&gt; &lt;/property&gt; &lt;!-- 全局配置注入 --&gt; &lt;property name=\"globalConfig\" ref=\"globalConfiguration\"/&gt; &lt;/bean&gt; &lt;bean id=\"txManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;/bean&gt; &lt;tx:annotation-driven transaction-manager=\"txManager\"/&gt; 参考关于长连接/短连接 各种数据库连接池对比 数据库连接泄露的问题","link":"/2018/12/25/pool/"},{"title":"Netty 是怎么做内存管理--PoolArena","text":"内存结构顾名思义，PoolArena负责缓存池的全局调度，它负责在上层组织和管理所有的chunk以及subpage单元。为了减少多线程请求内存池时的同步处理，Netty默认提供了cpu核数*2个PoolArena示例。 We use 2 available processors by default to reduce contention as we use 2 available processors for the number of EventLoops in NIO and EPOLL as well. If we choose a smaller number we will run into hot spots as allocation and de-allocation needs to be synchronized on the PoolArena. 1final int defaultMinNumArena = NettyRuntime.availableProcessors() * 2; PoolArena 内部对chunk与subpage的内存组织方式如下图： 每个PoolArena管理的所有chunk根据内存使用率的不同被划分为6种类型，以双向链表ChunkList的方式组织，并在不断的内存分配过程中根据chunk的使用率，对chunk的类型进行调整，放入合适的链表中。 在文章Netty 是怎么做内存管理-PoolSubpage中，我们已经了解到，subpage是针对请求内存小于一个页面大小时的内存划分。根据请求内存大小，subpage有可以被分为，smallpage(&gt;=512 btyes)和tinypage(&lt;512 bytes)两种类型。在内存分配的过程中，为了保持subpage中内存的连续性，避免内存碎片，并方便根据内存偏移量进行定位，每个页面内分配的内存段应该具有统一的规格(PoolSubpage中的elemSize)。 如上图所示，类似的chunk，PoolArena把subpage以数组的方式组织起来，把相同elemSize的subpage组成一个链表，放入数组中。由于subpage最小的内存段被限定为16bytes，所以tinysubpages共占据512/15=32个数组位置，而smallsubpage则在512的基础上依次翻倍，直到一整个页面大小，所以smallsubpages共有 log(pageSize/512)=pageShifts-9 个数组元素。 内存分配整体流程PoolArena分配的入口代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556private void allocate(PoolThreadCache cache, PooledByteBuf&lt;T&gt; buf, final int reqCapacity) { final int normCapacity = normalizeCapacity(reqCapacity); // 请求内存规整化 if (isTinyOrSmall(normCapacity)) { // capacity &lt; pageSize 页内分配 int tableIdx; PoolSubpage&lt;T&gt;[] table; boolean tiny = isTiny(normCapacity); if (tiny) { // &lt; 512 if (cache.allocateTiny(this, buf, reqCapacity, normCapacity)) { // was able to allocate out of the cache so move on return; } tableIdx = tinyIdx(normCapacity); table = tinySubpagePools; } else { if (cache.allocateSmall(this, buf, reqCapacity, normCapacity)) { // was able to allocate out of the cache so move on return; } tableIdx = smallIdx(normCapacity); table = smallSubpagePools; } final PoolSubpage&lt;T&gt; head = table[tableIdx]; synchronized (head) { final PoolSubpage&lt;T&gt; s = head.next; if (s != head) { //存在可分配的页面 assert s.doNotDestroy &amp;&amp; s.elemSize == normCapacity; long handle = s.allocate(); //调用PoolSubpage的分配 assert handle &gt;= 0; s.chunk.initBufWithSubpage(buf, handle, reqCapacity); incTinySmallAllocation(tiny); return; } } synchronized (this) { //在chunk中寻找新的subpage并分配 allocateNormal(buf, reqCapacity, normCapacity); } incTinySmallAllocation(tiny); return; } if (normCapacity &lt;= chunkSize) { //chunk内分配 if (cache.allocateNormal(this, buf, reqCapacity, normCapacity)) { // was able to allocate out of the cache so move on return; } synchronized (this) { allocateNormal(buf, reqCapacity, normCapacity); ++allocationsNormal; } } else { // Huge allocations are never served via the cache so just call allocateHuge allocateHuge(buf, reqCapacity); }} 代码的流程比较清晰： 对请求的内存大小做规整化处理：大于chunkSize时调整为保持内存对齐即可；大于512时调整为大于等于请求值的2的最小幂；小于512调整为大于等于请求值的16的倍数 当请求内存小于pageSize时，在页内分配：请求内存小于512在tinysubpages数组中定位并执行分配，大于512时在smallpages数组中定位并执行分配，当数组中没有可用的内存页时，在chunk中寻找新的内存页进行分配。 请求内存大于pageSize小于chunkSize，在chunk内执行分配 请求内存大于chunkSize，直接调用allocateHuge分配，不在内存池中管理。 当然除了上述整体流程，还有一些细节上的实现需要我们深挖：如何根据请求内存的大小在subpages数组中定位？当需要在chunk中执行分配时，在chunkList中选择哪个chunk？分配完成后，如何使用返回的内存偏移量句柄对Bytebuf进行初始化？依次来看： 页面定位页面定位主要解决的是根据请求内存的大小，如何选择合适的subpage来进行分配的问题。首先来看tinysubpages： 123static int tinyIdx(int normCapacity) { return normCapacity &gt;&gt;&gt; 4;} 我们已经知道，tinysubpages管理的内存段自16byte开始，以16byte依次递增自512，共映射到32个数组元素中，所以按照请求内存的大小除以16即可完成定位。 123456789static int smallIdx(int normCapacity) { int tableIdx = 0; int i = normCapacity &gt;&gt;&gt; 10; while (i != 0) { i &gt;&gt;&gt;= 1; tableIdx ++; } return tableIdx;} smallsubpages的定位也不难理解，计算请求内存相对于1024的倍数即为相应的数组下标。 Chunk定位回顾前述的内存结构，PoolArena根据内存使用率的情况把Chunk分成了6种类型：qInit,q000,q025,q50,q075,q100, 他们所对应的chunk使用率如下表： Type Usage qInit [Integer.MIN_VALUE, 25) q000 [1, 50) q025 [25, 75) q050 [50,100) q075 [75,100) q100 [100,Integer.MAX_VALUE) 这里有一点需要注意的就是，相邻的ChunkList之间在使用率上存在一定的重叠区域，即一个chunk的使用率为35的chunk可能存在于q000中，也可能存在于q025中。这主要是为了防止，由于使用率不断变化，导致某个chunk在两个List中不停来回跳动的情况，加了这么一段重叠的缓存区域，可以减少跳动的次数。详细可以参考链接中的分析。 1234567891011121314private void allocateNormal(PooledByteBuf&lt;T&gt; buf, int reqCapacity, int normCapacity) { if (q050.allocate(buf, reqCapacity, normCapacity) || q025.allocate(buf, reqCapacity, normCapacity) || q000.allocate(buf, reqCapacity, normCapacity) || qInit.allocate(buf, reqCapacity, normCapacity) || q075.allocate(buf, reqCapacity, normCapacity)) { return; } // Add a new chunk. PoolChunk&lt;T&gt; c = newChunk(pageSize, maxOrder, pageShifts, chunkSize); long handle = c.allocate(normCapacity); assert handle &gt; 0; c.initBuf(buf, handle, reqCapacity); qInit.add(c);} 首先尝试在已经发生分配的ChunkList中，进行分配，如果有可用的chunk则直接返回，否则新建一个chunk，执行分配并初始化ByteBuf。新创建的Chunk将就被加入qInit链表中。在qxxx中执行分配的过程比较简单，此处不再附上代码，其流程可以描述为：从ChunkList的链表头开始遍历，找到第一个可以分配的Chunk，并初始化ByteBuf，最后依据Chunk的使用率，判断是否需要将Chunk加入下一个ChunkList中。 对于Netty选择ChunkList时候的顺序，摘抄文章中的下面一段分析： 分配内存时，为什么不从内存使用率较低的q000开始？在chunkList中，我们知道一个chunk随着内存的释放，会往当前chunklist的前一个节点移动。q000存在的目的是什么？q000是用来保存内存利用率在1%-50%的chunk，那么这里为什么不包括0%的chunk？直接弄清楚这些，才好理解为什么不从q000开始分配。q000中的chunk，当内存利用率为0时，就从链表中删除，直接释放物理内存，避免越来越多的chunk导致内存被占满。想象一个场景，当应用在实际运行过程中，碰到访问高峰，这时需要分配的内存是平时的好几倍，当然也需要创建好几倍的chunk，如果先从q0000开始，这些在高峰期创建的chunk被回收的概率会大大降低，延缓了内存的回收进度，造成内存使用的浪费。那么为什么选择从q050开始？1、q050保存的是内存利用率50%~100%的chunk，这应该是个折中的选择！这样大部分情况下，chunk的利用率都会保持在一个较高水平，提高整个应用的内存利用率；2、qinit的chunk利用率低，但不会被回收；3、q075和q100由于内存利用率太高，导致内存分配的成功率大大降低，因此放到最后； ByteBuf初始化对ByteBuf的初始化就是告诉ByteBuf，他可以使用的内存起点位置offset，请求内存的位置length和最大可用内存的位置maxLength。 123456789101112void initBuf(PooledByteBuf&lt;T&gt; buf, long handle, int reqCapacity) { int memoryMapIdx = memoryMapIdx(handle); int bitmapIdx = bitmapIdx(handle); if (bitmapIdx == 0) { // 基于chunk的分配 byte val = value(memoryMapIdx); assert val == unusable : String.valueOf(val); buf.init(this, handle, runOffset(memoryMapIdx) + offset, reqCapacity, runLength(memoryMapIdx), arena.parent.threadCache()); } else { // 基于subpage的分配 initBufWithSubpage(buf, handle, bitmapIdx, reqCapacity); }} 1234567private static int memoryMapIdx(long handle) { return (int) handle;}private static int bitmapIdx(long handle) { return (int) (handle &gt;&gt;&gt; Integer.SIZE);} 在PoolChunk或者PoolSubpage完成分配后，都会返回一个内存偏移量句柄handle作为标识。在执行初始化时，就是依据handle逆向初始化的过程。PoolChunk返回的handle就是分配的内存在memoryMap中的节点编号，而PoolSubpage返回的handle中，低32表示当前page在chunk中的编号，高32包含了分配的内存段在bitmap中的索引。所以，在handle中取低32位就能计算得到分配的内存在chunk中的节点编号，而取高32位信息就能计算得到分配的内存段在subpage中位图的索引位置。 首先来看一下基于chunk的分配(buf.init)，ByteBuf三个内存参数的计算方式: offsetoffset = runOffset(memoryMapIdx)+offset 最后一个offset是初始化chunk时预留的offset，默认是0。重点看runOffet函数：12345678910private int runOffset(int id) { // represents the 0-based offset in #bytes from start of the byte-array chunk int shift = id ^ 1 &lt;&lt; depth(id); return shift * runLength(id);}private int runLength(int id) { // represents the size in #bytes supported by node &apos;id&apos; in the tree return 1 &lt;&lt; log2ChunkSize - depth(id);} shift的计算方式与Netty内存管理-PoolSubpage一文中，计算叶子节点相对位置的方式相同。简单回顾一下，如图： 由完全二叉树的性质，每一层首节点的编号为2depth, 而当前层其他节点的编号则从左到右依次加1，所以当前层任意节点相对于首节点的位置，可以通过节点编号的异或运算去除高位的1计算得到。所以shift表示了，当前节点相对于其所在层的首节点的相对位置(从0开始计算)。 runLength表示了当前节点所在层里，每个节点拥有的内存大小值，所以shift*runLength即为当前节点在chunk中的相对位置偏移量。 lengthlength即为用户请求的内存大小：reqCapacity。 maxLengthmaxLength = runLength(memoryMapIdx), 由于把当前节点都分配给了ByteBuf的最大可用内存就是一个节点所拥有的内存大小。 类似的，看一下基于subpage的分配：1234567891011121314private void initBufWithSubpage(PooledByteBuf&lt;T&gt; buf, long handle, int bitmapIdx, int reqCapacity) { assert bitmapIdx != 0; int memoryMapIdx = memoryMapIdx(handle); PoolSubpage&lt;T&gt; subpage = subpages[subpageIdx(memoryMapIdx)]; assert subpage.doNotDestroy; assert reqCapacity &lt;= subpage.elemSize; buf.init( this, handle, runOffset(memoryMapIdx) + (bitmapIdx &amp; 0x3FFFFFFF) * subpage.elemSize + offset, reqCapacity, subpage.elemSize, arena.parent.threadCache());} 由于subpage是chunk的叶子节点，所以首先需要根据memoryMapIdx定位到chunk中具体的哪一页：123private int subpageIdx(int memoryMapIdx) { return memoryMapIdx ^ maxSubpageAllocs; // remove highest set bit, to get offset} maxSubpageAllocs表示chunk中最多有多少个页面，由于完全二叉树的性质，其值与首个叶子节点的编号一致，所以可以计算出当前叶子节点的相对位置。 offsetrunOffset(memoryMapIdx) + (bitmapIdx &amp; 0x3FFFFFFF) * subpage.elemSize + offset runOffset与前面基于chunk的分配含义相同，计算得到当前叶子节点在chunk中的内存偏移；(bitmapIdx &amp; 0x3FFFFFFF) * subpage.elemSize 利用位图索引bitmapIdx计算分配的内存段在位图中的实际位置，此处与0x3FFFFFFF按位与是为了去除在构造handler时添加的高位1。用页的内存偏移加上页内偏移地址，即可得到分配的内存段全局的相对内存位置。 lengthlength即为用户请求的内存大小：reqCapacity。 maxLength当前页面内存段的大小：elemSize 自此，基于内存池的内存分配逻辑以全部梳理完成。","link":"/2017/09/02/poolarena/"},{"title":"Netty 是怎么做内存管理--PoolChunk","text":"Preface我们将PoolChunk上的内存分配视为一个算法来分析： 输入：指定的连续内存空间大小 输出：如果分配成功，返回一个包含目标空闲内存地址信息的句柄，否则返回失败 这里强调以下，Netty内存池分配出来的内存空间不是Client申请size的大小，而是大于size的最小2的幂次方（size &gt; 512）或者是16的倍数。比如Client申请100byte的内存，那么返回的将是128byte。Netty会在入口处对申请的大小统一做规整化的处理，来保证分配出来的内存都是2的幂次方，这样做有两点好处：内存保持对齐，不会有很零散的内存碎片，这点和操作系统的内存管理类似；其次可以基于2的幂次方在二进制上的特性，大量运用位运算提升效率。后面的详细流程中我们将会看到。 内存存储单元在分析原理之前，我们先看以下PoolChunk中一些默认参数： 1.内存块Chunk，Netty向操作系统申请资源的最小单位，chunk是page单元的集合 chunk默认大小16M，可调节，根据pageSize和maxOrder计算得到 MaxChunkSize, chunk最大大小为1G DefaultMaxOrder = 11, 一个chunk默认由211个页面构成，因为一个page 8k，所以默认完全二叉树11层。 2.内存页Page，当请求的内存小于页大小时，可继续划分为更小的内存段，使用位图标记各使用情况Page大小的默认值为8K,可调节，必须为2的幂： Data structureChunk基于一个完全平衡的二叉树来管理它拥有的Pages，每个叶子节点表示一个Page。参考前述的默认参数：ChunkSize = pageSize*2maxOrder, 也就是Page的个数是2的幂次方个，那么以此为叶子节点的二叉树一定是完全平衡的二叉树。二叉树中的父节点包含所有子节点的内存，也就是说父节点可以使用和分配以其为根的所有子节点表示的内存空间。其在内存分配的过程中（Page级别的分配），总是在此二叉树上（从左到右）寻找最先最小满足条件的Pages。 表示方式Netty使用了一个数组memoryMap来表示这个完全二叉树，数组元素的语义与二叉树拓扑结构的对应关系如下图: 详细的来说，数组下标表示二叉树中各节点的编号id，数组元素内容表示当前节点可分配内存的子节点（即未分配）在二叉树中的深度。根据i节点在memoryMap中的取值不同，它可以有一下三种语义： memoryMap[i] = depth_of_i 当前节点及其所有子节点都可以用来分配 memoryMap[i] = x &gt; depth_of_i 至少有一个子节点被分配，无法直接使用此节点，但其在第x层的子节点中有可分配的内存 memoryMap[i] = maxOrder + 1 当前节点的所有子节点均已被分配 以上图节点3为例，当memoryMap[3]=1时，表示该节点及其子节点均可分配，memoryMap[3]=2时，表示节点6和7中至少有一个已经被分配，并且在这两个节点中还能找到未分配的空间，memoryMap[3]=4时，表示该节点下的所有空间均已经被分配，无法再使用。 Procedure先看代码：12345678910/** * 向PoolChunk申请一段内存 * /long allocate(int normCapacity) { if ((normCapacity &amp; subpageOverflowMask) != 0) { // &gt;= pageSize return allocateRun(normCapacity); // 当要分配的内存大于pageSize时候，使用allocateRun在chunk内分配 } else { return allocateSubpage(normCapacity); //否则使用向PoolChunk申请一段内存在page内分配 }} 123456789101112131415/** * Allocate a run of pages (&gt;=1) * * @param normCapacity normalized capacity * @return index in memoryMap */private long allocateRun(int normCapacity) { int d = maxOrder - (log2(normCapacity) - pageShifts); int id = allocateNode(d); if (id &lt; 0) { return id; } freeBytes -= runLength(id); return id;} 首先根据请求内存的大小，选择采取合适的分配策略，这里详细讨论分配大于一个页面大小的情况，页内分配请移步Netty 是怎么做内存管理-PoolSubpage 再根据请求内存的大小，定位其在二叉树中的深度：int d = maxOrder - (log2(normCapacity) - pageShifts)。参考前述二叉树的图，可以有两种理解方式： 自底向上：父节点的内存是子节点的二倍，比子节点高一层；父节点的内存是孙子节点的四倍，比孙子节点高两层，所以拥有normalCapacity内存的节点应该比叶子节点高：log2(normalCapacity/pagesie) = log2(normalCapacity)-pageShifts层，也就是说它在树中的深度应该为maxOrder-(log2(normalCapacity)-pageShifts). 自顶向下：观察上图右侧说明信息的第三列，根节点拥有整个chunk的内存，任意d层节点拥有的内存Capacity=chunksize／2d, 两边取对数可得拥有normalCapacity内存的节点应该在log2(chunksize/normalCapacity)这一层上。 下面看最核心的一段：如何利用memoryMap在d层上寻找第一个可用内存节点。1234567891011121314151617181920212223242526272829/** * Algorithm to allocate an index in memoryMap when we query for a free node * at depth d * * @param d depth * @return index in memoryMap */private int allocateNode(int d) { int id = 1; // 从根节点开始计算 int initial = - (1 &lt;&lt; d); // has last d bits = 0 and rest all = 1 byte val = value(id); if (val &gt; d) { // 根节点的容量不足以满足要分配的内存大小 return -1; } while (val &lt; d || (id &amp; initial) == 0) { // id &amp; initial == 1 &lt;&lt; d for all ids at depth d, for &lt; d it is 0 id &lt;&lt;= 1; //取左子节点 val = value(id); if (val &gt; d) { id ^= 1; //取邻居节点（2-&gt;3, 3-&gt;2） val = value(id); } } byte value = value(id); assert value == d &amp;&amp; (id &amp; initial) == 1 &lt;&lt; d : String.format(&quot;val = %d, id &amp; initial = %d, d = %d&quot;, value, id &amp; initial, d); setValue(id, unusable); // mark as unusable updateParentsAlloc(id); return id;} 123private byte value(int id) { return memoryMap[id];} 这一段的核心思想是： 从根节点开始，如果根节点已经被分配，并且其可分配内存的子节点深度大于d，表示已没有足够且连续的内存空间用来分配，返回-1； 如果左子节点上的内存可分配则在左子节点上分配，否则尝试右子节点，依次迭代。 这里主要注意下循环的迭代条件： 如果当前节点可分配节点的深度小于目标深度（相应的内存也就大于请求的内存），说明子节点就能够满足条件，进入下一层； 当一个节点(id)可分配节点的深度与目标深度相等时，只要当前节点(id)的深度小于目标深度（id &amp; initial == 0），就应该进入下一层迭代到目标深度那一层，寻找可用的节点。 更直观的来看，如下图: 假设现在经过内存的分配，我们待分配的内存应该在第二层d=2上寻找，首先从根节点1开始，memoryMap[1]=1 &lt; d，符合迭代条件1，进入左子节点2； memoryMap[2]=2=d，说明节点2在第二层的子节点上有内存可以分配，虽然此时已不满足迭代条件1，但节点2的深度还是要小于d的，满足迭代条件2，进入左子节点4，由于4的叶子节点都被分配，已经被标记为不可用（val = maxorder+1），所以尝试邻居节点5; 由于memoryMap[5]=2 并且 5 &amp; initial != 0, 无法再迭代，推出循环，找到目标节点5. 分配完成后需要将当前节点标记为不可用，并更改将父节点可分配节点的情况，防止重复分配。具体的步骤如下：123456789101112131415161718/** * Update method used by allocate * This is triggered only when a successor is allocated and all its predecessors * need to update their state * The minimal depth at which subtree rooted at id has some free space * * @param id id */private void updateParentsAlloc(int id) { while (id &gt; 1) { int parentId = id &gt;&gt;&gt; 1; byte val1 = value(id); byte val2 = value(id ^ 1); byte val = val1 &lt; val2 ? val1 : val2; setValue(parentId, val); id = parentId; }} 过程比较简单：从当前节点开始向上回溯，以当前两个子节点memoryMap中取值较小的那个，来更新父节点的值，这样就维持了节点memoryMap[id]的语义：在第x层的子节点中有可分配的内存。","link":"/2017/08/27/poolchunk/"},{"title":"指令重排序","text":"BackgroundIntel 从8086系列芯片起到目前的Core i系列芯片，每一次提速与架构升级都会引入一些新的技术.指令重排序(instruction reordering)的概念大约出现在Pentium Pro系列上。在说明这项技术之前，先简单看一下CPU内部的组件与指令流水线等相关概念。 现代的CPU内部组织极其紧密与复杂，为了简单起见(虽然简单，但是真实存在并且足以阐述问题)，以最基础的80286为例，其内部逻辑抽象如下图所示：其中各部件的功能描述如下： AU 地址部件，主要负责根据寻址请求，生成物理地址 BU 总线部件，主要负责内存物理寻址，完成数据传输 EU 执行部件，负责执行指令要求的功能，包括运算器，微程序控制器等 IU 指令部件，包含指令译码器和指令队列 任何用高级编程语言写成的程序都要最终由编译器，解释器等翻译成为CPU可以理解执行的指令。了解冯诺依曼体系结构的人都知道，无论我们的指令还是指令操作的数据，都是做为数据存在存储器里，在计算时输入计算器进行运算。所以，结合上图，完成一条指令就可以分为以下几个执行阶段，我们把完成一条指令所需要的时间称为：指令周期。 取指FE：发起取指请求，经物理地址转换和一次或者多次总线周期(视指令字长和总线位数而定)，提取内存中的指令代码 译码DE：对指令代码进行译码 执行EX：由控制器根据指令编码，发送信号给执行部件进行计算(根据指令类型的差异，还可能需要多个总线周期读取操作数) 回写WB：将计算结果写到指定的目标地址上 这里插入一点关于指令周期，总线周期和时钟周期的知识: 指令周期：CPU完成一条指令所需要的时间 总线周期：CPU通过总线完成一次内存或者I/O访问的时间 时钟周期：时钟频率的倒数，也就是我们买电脑时熟知的主频的倒数 在多核技术出现以前，CPU通过不断提高主频，压缩时钟周期，让一条指令完成的时间更短，让我们用起来越来越快，但是受限于工艺，主频不会无限制的增长，但是CPU提速的需求没变，所以引入了多核，这也给编程人员如何充分利用这项技术，提升自己程序的性能带来了挑战。有兴趣的可以阅读The Free Lunch Is Over，中文版自行百度。 Pipeline我们已经知道，我们的线程可以并行，数据可以并行，那么我们指令的执行为什么不能并行呢？在串行的指令执行方式下，一个指令周期只能执行一条指令，太慢了！为什么不能在对第一条指令译码的时候，就取第二条指令呢，在执行第一条指令的时候，再对第二条指令进行译码呢？假设，每个执行阶段都花费一个总线周期，在完美的条件下，那么我们的指令就可以以下图的方式运行起来，这就是指令流水线技术（大约在Intel 386里开始出现）。 在指令流水线的技术下，一条指令被拆分为多个独立的运行阶段，做个简单的计算，以前8个总线周期才能完成两条指令，现在我们8个总线周期就已经执行了五条指令了，效率提升了很多。 Instruction reordering注意到，我们前面介绍流水线时提到了：完美条件。事实上，由于受到指令之间三种依赖的限制，我们的流水都没法完美的并行下去： 数据相关：后面的指令需要使用先前指令的计算结果 名相关：两条指令使用了相同的寄存器或者存储单元, 但他们之间又没有数据流动 控制相关：由分支指令引起的, 程序流向需根据分支指令执行的结果来确定。 指令重排序就可以用来解决数据相关，举个简单的例子，有如下三条指令: ADD AX, BX; (1) AX + BX --&gt; AX INC AX; (2) AX + 1 --&gt; AX MOV CX, DX; (3) DX --&gt; CX 按照前面介绍的指令流水线技术，我们会得到如下的执行效果图： 需要注意的是，上图为Pentium系列的5级指令流水线机制，PF 为指令预取周期，D1为指令译码周期，D2为地址生成周期，虽然指令周期的划分有变化，但不影响对流水线机制理解。由于指令(2)的执行阶段依赖指令(1)的执行结果，所以会出现一个时钟周期的空等待。但是指令(3)并不依赖指令(1)(2)的执行结果，所以可以通过对指令进行从排序，消除空等待，提升流水线的效率。下图是重排序后的指令流水线，不难看出指令的执行效率得到了提升。 当然，对于除了CPU可以对指令进行重排序之外，编译器也会进行适当的重排优化，并且编译器能进行的优化程度要高于CPU，因为CPU只能在局部指令范围内进行重排，而编译器可以全局的分析我们的程序。编译器进行指令重排也场景和规则比较复杂，有兴趣的可以研究一下。 Relevant things指令重排序对程序开发人员来说，最大影响的就是多线程并发同步的问题。如果程序没有得到合适的同步，那么程序的运行结果将无法预测。 从硬件的角度来看，CPU与内存之间存在着数量级上的速度差异，为了充分利用资源，防止CPU空置，都会在二者之间加入片内缓存，毕竟访问片内缓存的速度要比通过总线访问内存的速度高很多。这给我们程序带来的影响就是我们可以充分利用缓存，提升效率，而当面临数据一致性问题时，就需要多加小心了。为了满足数据一致性的需求，CPU也提供了一些规则(内存屏障)和指令来支持强读内存(保证程序读取到的数值都是先前被修改过的值，而不是自己缓存的值)和强刷内存(保证程序对内存变量修改的全局可见性)。 多线程程序中，如果每个线程之间不需要交互，大家各自为政，互不干涉，自然相安无事，怕就怕有的线程之间存在共享的数据，由于存在重排序、内存分级，如果没有进行有效的管理，读写操作访问的位置和执行先后顺序的不确定性会严重影响程序最终运行结果的可解释性。 Java语言规范中通过内存模型限定了多线程并发时编译器和处理器的执行规范(顺序一致性原则和happens-before原则等)，保证了在程序通过sychronized,volatile等关键字正确同步的情况下，多线程程序的全局一致性和各线程内的按照程序顺序执行。所以，只要程序的最终运行结果满足此规范，无论虚拟机开发商以及处理器使用什么机制对程序和指令进行排序都是合法的。 关于Java内存模型更多的知识可以参考：Threads and locks, JSR133, JSR133FAQ, InfoQ博客 A memory model describes, given a program and an execution trace of that program, whether the execution trace is a legal execution of the program. The Java Memory Model describes what behaviors are legal in multithreaded code, and how threads may interact through memory. At the processor level: a memory model defines necessary and sufficient conditions for knowing that writes to memory by other processors are visible to the current processor, and writes by the current processor are visible to other processors. The compiler, runtime, and hardware are supposed to conspire to create the illusion of as-if-serial semantics, which means that in a single-threaded program, the program should not be able to observe the effects of reorderings. Most of the time, one thread doesn’t care what the other is doing. But when it does, that’s what synchronization is for. Synchronization ensures that memory writes by a thread before or during a synchronized block are made visible in a predictable manner to other threads which synchronize on the same monitor. After we exit a synchronized block, we release the monitor, which has the effect of flushing the cache to main memory, so that writes made by this thread can be visible to other threads. Before we can enter a synchronized block, we acquire the monitor, which has the effect of invalidating the local processor cache so that variables will be reloaded from main memory.You have to set up a happens-before relationship for one thread to see the results of another.","link":"/2016/11/17/reorder/"},{"title":"RocketMQ源码分析4--Broker模块","text":"本文是RocketMQ源码分析系列之四，如有疑问或者技术探讨，可以email me,欢迎探讨. Broker模块是RocketMQ的核心组件，消息中转角色：负责消息存储和消息转发，和Producer对接 - 接收Producer发送的消息并转存（调用store模块持久化到磁盘）；和Consumer对接 - 消息订阅拉消息;另外Broker解决了消息堆积，并通过Master-Slave横向扩展实现了HA。 对于RocketMQ，Broker是逻辑概念。一台机器或者为Master或者为Slave的一个实例。1One Broker = One Master + n * Slaves 一个Master可以对应多个Slave, Master与Slave的对应关系通过指定相同的BrokerName关联起来，BrokerId=0的实例为Master,非0的实例为Slave。在横向扩展情况下，Master可以部署为多实例。 每个Broker与NameServer集群中的所有节点建立长连接 - 每个Broker每隔30s向NameServer更新自身Topic和Broker路由信息.1234567891011this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { try { BrokerController.this.registerBrokerAll(true, false); } catch (Exception e) { log.error(\"registerBrokerAll Exception\", e); } } }, 1000 * 10, 1000 * 30, TimeUnit.MILLISECONDS); start 启动Broker启动的时候主要启动一些辅助线程服务，例如CQ和CommitLog的刷盘线程服务，启动用于构建indexfile和CQ的服务的ReputService服务，启动BrokerOutAPI(Broker和其他模块通信类), 并创建消息发送线程池，消息拉取线程池，admin管理线程池和client管理线程池。以及把broker注册到NameServer等等。另外Broker启动的时候还将已经持久化到硬盘的topic,，consumerOffset， subscriptionGroup, consumerFilter到内存。 另外Broker启动的时候还会注册Netty的处理器(BrokerController.initialize)，用于和NameServer之间通信的处理。注册Processor会为每个消息的注册码指定一个请求处理器。例如负责处理生产者发送消息时，NameServer与Broker通信的处理器SendMessageProcessor, 负责处理消费者消费消息时，NameServer与Broker通信的处理器PullMessageProcessor等等。 参见代码：1234567891011public class BrokerStartup { public static Properties properties = null; public static CommandLine commandLine = null; public static String configFile = null; public static Logger log; public static void main(String[] args) { start(createBrokerController(args)); } ...} 消息的发送当消息的生产者向Broker发送消息时候，实际上消息先到达NameServer, NameServer将消息发送给Broker进行消息的落地。 这里引用kyghkgyh的broker接收消息的图： Broker通过提供SendMessageProcessor与NameServer进行通信获取消息，并调用store模块提供的写commitLog的接口，将消息持久化到commitlog文件，最后落地到磁盘上。 Broker与Topic的关系对于消息来说，topic是消息的逻辑分类单元，queue是消息的物理存储单元。一个topic下可以有多个queue。逻辑上：当生产者生产消息时候需要为消息指定topic，topic创建时需要指定1个或者多个broker。topic与broker是多对多的关系，一个topic分布在多个broker上，一个broker可以配置多个topic.物理上：上缠着生产消息发送给broker时候需要指定发送到哪个queue上。默认情况生产者会轮询将消息发送到每个queue，顺序随机。 NameServer就是通过broker与topic的映射关系来做路由。 消息的堆积MQ的一个很重要的一个功能是挡住并缓冲数据洪峰，削峰填谷，从而保证后端系统的稳定性。因此RocketMQ的broker端需要具备一定的消息堆积能力（官方数据是支持亿级消息堆积）。 Broker在接收到消息后，会将其持久化到本地磁盘的文件中。之所以没有选择持久化到远程DB或者KV数据库，个人认为可以减少网络开销，还可以避免因为带宽原因可能影响到消息的发送和消费的TPS。Broker通过使用Linux的零拷贝技术保证了提高了文件高并发读写。具体实现为：Broker通过Java的MappedByteBuffer(CommitLog, CQ等的源码都使用到了MappedByteBuffer)使用mmap技术, 将文件直接映射到用户态的内存地址, Broker可以像操作内存一样操作文件 - 直接操作Linux操作系统的PageCache，这样就可以直接操作内存中的数据而不需要每次都通过IO去物理磁盘写文件。因此可以RocketMQ存储得以实现亿级消息堆积，并且保持了低写入延迟。 Broker响应Consumer请求 - 消息的接收关于RocketMQ的两种消费模式Push与Pull，具体参见。 RocketMQ在具体实现时，Push和Pull模式都是采用消费者主动Pull的方式 - Consumer长轮询从broker拉取消息。这样可以保证在消息不堆积情况下，消息到达Broker后，能立刻到达Consumer - Consumer先根据对应的Topic+queueId去ConsumeQueue拿到该消息在CommitLog中的offset, 接着通过offset到CommitLog中拿到具体消息。这样实现了消息的实时性不低于Push. BrokerController启动时候，会通过initialize()注册PullMessageProcessor来处理拉消息的请求。BrokerControler123456789public void registerProcessor() { ... /** * PullMessageProcessor */ this.remotingServer.registerProcessor(RequestCode.PULL_MESSAGE, this.pullMessageProcessor, this.pullMessageExecutor); this.pullMessageProcessor.registerConsumeMessageHook(consumeMessageHookList); ...} PullMessageProcessor处理拉消息请求的逻辑：1234567private RemotingCommand processRequest(final Channel channel, RemotingCommand request, boolean brokerAllowSuspend){ ... final GetMessageResult getMessageResult = this.brokerController.getMessageStore().getMessage(requestHeader.getConsumerGroup(), requestHeader.getTopic(), requestHeader.getQueueId(), requestHeader.getQueueOffset(), requestHeader.getMaxMsgNums(), messageFilter); ...} 类似于Producer发送消息，PullMessageProcessor解析（用户 -》 NameServer -》Broker）消息，调用store模块的DefaultMessageStore提供的方法, 从topic的某个CQ指定offset开始拉消息，一次最多maxMagNums条，并且使用指定的订阅表达式进行过滤。 123GetMessageResult getMessage(final String group, final String topic, final int queueId, final long offset, final int maxMsgNums, final MessageFilter messageFilter) { ....} Broker 消息过滤RocketMQ是在订阅时做过滤(PullMessageProcessor - DefaultMessageStore.getMessage() - ExpressionMessageFilter)。1public boolean isMatchedByConsumeQueue(Long tagsCode, ConsumeQueueExt.CqExtUnit cqExtUnit) (1). 在 Broker 端进行 Message Tag 比对，先遍历 Consume Queue，如果存储的 Message Tag 与订阅的 Message Tag 不符合，则跳过，继续比对下一个，符合则传输给 Consumer。注意:Message Tag 是字符串形式，Consume Queue 中存储的是其对应的 hashcode，比对时也是比对 hashcode。(2). Consumer 收到过滤后的消息后，同样也要执行在 Broker 端的操作，但是比对的是真实的 Message Tag 字符串，而不是 Hashcode。 为什么过滤要这样做?(1). Message Tag 存储 Hashcode，是为了在 Consume Queue 定长方式存储，节约空间。(2). 过滤过程中不会访问 Commit Log 数据，可以保证堆积情况下也能高效过滤。(3). 即使存在Hash冲突，也可以在Consumer端进行修正，保证万无一失。 Broker 处理消息重复 ： At least Once or Exactly Only Once[引自官方Doc: RocketMQ 原理简介] At least Once: 是指每个消息必须投递一次。RocketMQ Consumer 先 pull 消息到本地，消费完成后，才向服务器返回 ack，如果没有消费一定不会 ack 消息，所以 RocketMQ 可以很好的支持此特性。 Exactly Only Once(1). 发送消息阶段，不允许发送重复的消息。(2). 消费消息阶段，不允许消费重复的消息。只有以上两个条件都满足情况下，才能认为消息是“Exactly Only Once”，而要实现以上两点，在分布式系统环境下，不可避免要产生巨大的开销。所以 RocketMQ 为了追求高性能，并不保证此特性，要求在业务上进行去重，也就是说消费消息要做到幂等性。RocketMQ 虽然不能严格保证不重复，但是正常情况下很少会出现重复发送、消费情况，只有网络异常，Consumer启停等异常情况下会出现消息重复。 此问题的本质原因是网络调用存在不确定性，即既不成功也不失败的第三种状态，所以才产生了消息重复性问 题。 Broker 和Name Server的心跳实现Broker启动时,会在定时线程池中每30秒注册信息至Name Server 1234567891011this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { try { BrokerController.this.registerBrokerAll(true, false); } catch (Throwable e) { log.error(\"registerBrokerAll Exception\", e); } } }, 1000 * 10, 1000 * 30, TimeUnit.MILLISECONDS); Broker 的主从同步BrokerController在启动的时候，会通过initialize()判断Broker的角色，如果角色是Slave的话，会启动一个定时线程任务，每隔60s调用SlaveSynchronize.syncAll()进行主从同步: Topic配置同步 消费进度信息同步 延迟消费进度信息同步 订阅关系同步 如果角色是Master的话，打印主备之间commitlog同步位点差异。 1234567891011121314151617181920212223242526272829303132if (BrokerRole.SLAVE == this.messageStoreConfig.getBrokerRole()) { if (this.messageStoreConfig.getHaMasterAddress() != null &amp;&amp; this.messageStoreConfig.getHaMasterAddress().length() &gt;= 6) { this.messageStore.updateHaMasterAddress(this.messageStoreConfig.getHaMasterAddress()); this.updateMasterHAServerAddrPeriodically = false; } else { this.updateMasterHAServerAddrPeriodically = true; } this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { try { BrokerController.this.slaveSynchronize.syncAll(); } catch (Throwable e) { log.error(\"ScheduledTask syncAll slave exception\", e); } } }, 1000 * 10, 1000 * 60, TimeUnit.MILLISECONDS); } else { this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { try { BrokerController.this.printMasterAndSlaveDiff(); } catch (Throwable e) { log.error(\"schedule printMasterAndSlaveDiff error.\", e); } } }, 1000 * 10, 1000 * 60, TimeUnit.MILLISECONDS); } 123456public void syncAll() { this.syncTopicConfig(); // Topic配置同步 - 同步topic.json文件 this.syncConsumerOffset(); // 消费进度信息同步 - 同步consumerOffset.json文件 this.syncDelayOffset(); // 延迟消费进度信息同步 - 同步delayOffset.json文件 this.syncSubscriptionGroupConfig(); // 订阅关系同步 - 同步subscriptionGroup.json文件} 参考 https://rocketmq.apache.org/docs/rmq-arc/ 实际问题解决 -http://dbaplus.cn/news-21-1123-1.html RocketMQ-HA高可用,作者meilong_whpu Linux 内核的文件 Cache 管理机制介绍","link":"/2018/03/26/rocketmq-broker/"},{"title":"RocketMQ源码分析6--关于RocketMQ你想知道的Questions","text":"既见树木，又见森林。1.RocketMQ如何保证严格的消息顺序？ 消息顺序分为：普通顺序消息和严格顺序消息。 普通顺序消息在正常情况下可以保证完全的顺序消息，但是一旦发生通信异常，Broker重启，由于队列总数发生变化，hash取模后定位的队列会变化，产生短暂的消息顺序不一致。如果业务能容忍集群在异常情况下（如某个Broker宕机或者重启）下，消息短暂的乱序，使用普通顺序方式比较合适。 严格顺序消息在无论正常异常情况下都能保证顺序，但是牺牲了分布式Failover特性，即Broker集群中只要有一台机器不可用，整个集群都不可用，服务可用性大大降低。目前已知的应用只有数据库binlog同步强依赖严格顺序消息，其他应用绝大部分都可以容忍短暂乱序。推荐使用普通的顺序消息。 顺序消息是RocketMQ功能特性上的一个卖点。在RocketMQ中，主要指的是局部顺序，即一类消息为满足顺序性，必须由Producer单线程顺序发送，且发送到同一个队列，这样Consumer就可以按照Produer发送的顺序去消费消息。 从源码角度分析RocketMQ怎么实现发送顺序消息。 RocketMQ通过轮询所有队列的方式来确定消息被发送到哪一个队列（负载均衡策略）。比如下面的示例中，订单号相同的消息会被先后发送到同一个队列中：1234567891011// RocketMQ通过MessageQueueSelector中实现的算法来确定消息发送到哪一个队列上// RocketMQ默认提供了两种MessageQueueSelector实现：随机/Hash// 当然你可以根据业务实现自己的MessageQueueSelector来决定消息按照何种策略发送到消息队列中SendResult sendResult = producer.send(msg, new MessageQueueSelector() { @Override public MessageQueue select(List&lt;MessageQueue&gt; mqs, Message msg, Object arg) { Integer id = (Integer) arg; int index = id % mqs.size(); return mqs.get(index); }}, orderId); 在获取到路由信息以后，会根据MessageQueueSelector实现的算法来选择一个队列，同一个OrderId获取到的肯定是同一个队列。12345678910111213private SendResult send() { // 获取topic路由信息 TopicPublishInfo topicPublishInfo = this.tryToFindTopicPublishInfo(msg.getTopic()); if (topicPublishInfo != null &amp;&amp; topicPublishInfo.ok()) { MessageQueue mq = null; // 根据我们的算法，选择一个发送队列 // 这里的arg = orderId mq = selector.select(topicPublishInfo.getMessageQueueList(), msg, arg); if (mq != null) { return this.sendKernelImpl(msg, mq, communicationMode, sendCallback, timeout); } }} 2.RocketMQ保证消息不重复吗？ MQ的消息不重复指无论是发送阶段还是消费消息阶段，都不允许发送重复的消息。先说结论，RocketMQ不能严格保证不重复，但是正常情况下很少会出现重复发送or消费。只有网络异常，consumer启停的是可能会出现。 重复消息的本质是网络调用的不确定性。只要网络交换数据，就无法避免这个问题，所以只能绕过这个问题以姐姐。那么问题就变成了：如果消费端收到两条一样的消息，应该怎么处理？ 消费端自己处理：消费端处理消息的业务逻辑保持幂等 保证每条消息都有唯一编号且保证消息处理成功与去重的日志同事出现 - 利用一张日志表来记录已经处理成功的消息的ID，如果新到的消息ID已经在日志表中，那么就不再处理这条消息。 第1条解决方案，很明显应该在消费端实现，不属于消息系统要实现的功能。第2条可以消息系统实现，也可以业务端实现。正常情况下出现重复消息的概率其实很小，如果由消息系统来实现的话，肯定会对消息系统的吞吐量和高可用有影响，所以最好还是由业务端自己处理消息重复的问题，这也是RocketMQ不解决消息重复的问题的原因。 RocketMQ不保证消息不重复，如果你的业务需要保证严格的不重复消息，需要你自己在业务端去重 关于消息顺序&amp;重复，更多推荐阅读： travi’s blog CHEN川‘s简书 3.RocketMQ为什么不采用Zookeeper而自己开发了NameServer? 首先，ZooKeeper可以提供Master/Slave选举功能，比如Kafka一个topic由多个partition，每个partition有1个master+多个slave，Kafka使用ZK给每个分区选一个机器作为Master。这里Master/Slave是动态的，Master挂了会有1个Slave切换成Master. 但对于RocketMQ来说，不需要选举，Master/Slave各是一台机器，角色固定。当一个Master挂了，可以写到其他Master上，但是不会Slave切换成Master. 这种简化，使RocketMQ可以不依赖ZK就很好的管理了Topic和Queue以及物理机器的映射，也实现了高可用。 其次，RockeqMQ集群中，需要有构件来处理一些通用数据，比如broker列表，broker刷新时间，虽然ZooKeeper也能存放数据，并有一致性保证，但处理数据之间的一些逻辑关系却比较麻烦，而且数据的逻辑解析操作得交给ZooKeeper客户端来做，如果有多种角色的客户端存在，自己解析多级数据确实是个麻烦事情； 既然RocketMQ集群中没有用到ZooKeeper的一些重量级的功能，只是使用ZooKeeper的数据一致性和发布订阅的话，与其依赖重量级的ZooKeeper，还不如写个轻量级的NameServer，NameServer也可以集群部署，NameServer与NameServer之间无任何信息同步，只有一千多行代码的NameServer稳定性肯定高于ZooKeeper，占用的系统资源也可以忽略不计，何乐而不为？ 4.RocketMQ怎么处理亿级消息的堆积的？在保证了堆积亿级消息后，怎么保持写入低延迟？ MQ的一个很重要的一个功能是挡住并缓冲数据洪峰，削峰填谷，从而保证后端系统的稳定性。因此RocketMQ的broker端需要具备一定的消息堆积能力（官方数据是支持亿级消息堆积）。 Broker在接收到消息后，会将其持久化到本地磁盘的文件中。之所以没有选择持久化到远程DB或者KV数据库，个人认为可以减少网络开销，还可以避免因为带宽原因可能影响到消息的发送和消费的TPS。Broker通过使用Linux的零拷贝技术保证了提高了文件高并发读写。具体实现为：Broker通过Java的MappedByteBuffer(CommitLog, CQ等的源码都使用到了MappedByteBuffer)使用mmap技术, 将文件直接映射到用户态的内存地址, Broker可以像操作内存一样操作文件 - 直接操作Linux操作系统的PageCache，这样就可以直接操作内存中的数据而不需要每次都通过IO去物理磁盘写文件。因此可以RocketMQ存储得以实现亿级消息堆积，并且保持了低写入延迟。 5.RocketMQ消息订阅模式是什么？ 两种消息读取模式 ： Push or Pull。实际上，在RocketMQ中无论是Push还是Pull, 底层都是通过Consumer从Broker拉消息实现的。为了做到能够实时接收消息，RocketMQ使用长轮询方式，保证消息实时性和Push方式一致。这种长轮询类似Web QQ收发消息机制。 6.RocketMQ对于负载均衡有哪些设计？关于RocketMQ的负载均衡讨论，需要分为Broker端，Producer端，Consumer端三处来看是如何支持横向扩展和负载均衡的。 Broker端： 在RocketMQ中，一个Broker是由一台Master机器+一台或者多台Slave机器组成的逻辑概念。Master和Slave之间的数据同步或者为同步双写，或者为异步复制（可配）。当Broker需要横向扩展时，只需要增加Broker，然后新增该Broker和topic的路由关系，这样对于Broker的负载由更多的Broker分担。 消息的topic路由信息通过NameServer暴露给客户端，客户端通过NameServer可以获取topic对应的多个分布在多个broker上的message queue。客户端会把请求分摊到不同的CQ上，进而分摊到不同的Broker上，这样消息的存储和转发都得到了负载均衡。 Producer端 Producer端会通过轮询RoundRobin的方式写入消息到服务端中的某个CQ(CQ个数固定，默认配置)，从而达到消息均匀地生产到不同的CQ上。 而每个CQ分布在不同的broker上。所以对于Producer来说，消息会轮询均匀发送到不同的broker上。 Consumer端： 在集群消费模式下，每条消息需要投递到订阅这个topic的ConsumerGroup下的一个Consumer实例即可。由于Consumer在消费消息时候是根据topic到CQ上查找消息在CommitLog的Offset。所以问题可以转换为：Consumer多台实例怎么到这个topic的多个ConsumeQueue上消费。默认的算法是AllocateMessageQueueAveragely：即每个消费者实例可以拿到相同数量的CQ。另外，如果想要水平扩展消费能力的话，可以增加consumer实例。 另外集群模式下，每个CQ只能分配给一个实例。这是由于如果多个实例同时消费到一个queue，会导致同一个消息被重复消费。 7.RocketMQ是怎么做消息失败重试机制的？ https://my.oschina.net/xinxingegeya/blog/1584617 8.RocketMQ是怎么设计事务机制的？ 分布式事务涉及到两阶段提交问题。RocketMQ通过offset方式实现分布式事务。RocketMQ把消息的发送分成了两个阶段：Prepare阶段和确认阶段。（1） 发送Prepared消息（2） updateDB（3) 根据updateDB结果成功或者失败，确认或者取消Prepare消息。如果前两步执行成功了，最后一步失败了。由于RocketMQ会定期扫描所有的Prepared消息，询问发送方，到底是要确认这条消息发出去了，还是取消这条消息。 参考Travis‘s blog 9.RocketMQ是怎么“天然分布式的”？ producer, consumer, broker, nameserver都可以分布式，集群部署，消除单点故障。 [推荐jaskey文章总结][http://jaskey.github.io/blog/2016/12/19/rocketmq-rebalance/]https://www.jianshu.com/p/453c6e7ff81c http://mp.weixin.qq.com/s/6PmcXJZVyWYZPssveeNkIw","link":"/2018/05/01/rocketmq-questions/"},{"title":"秒杀系统设计","text":"秒杀这一业务场景已经发展多年，有套路可循。另外，秒杀属于极端大流量场景，它的应对经验对Web大流量应对方案有很好的借鉴意义。 秒杀系统本质秒杀正常的业务流程：查询商品 -&gt; 创建订单 -&gt; 减库存 -&gt; 更新订单 -&gt; 付款 -&gt; 卖家发货。 而业务特性是：（1）低廉价格；（2）大幅推广；（3）瞬时售空；（4）一般是定时上架；（5）时间短、瞬时并发量高。 从技术角度看秒杀系统本质上是一个满足大并发、高性能和高可用的分布式系统。秒杀其实主要解决两个问题，一个是并发读，一个是并发写。并发读的核心优化理念是尽量减少用户到服务端来“读”数据，或者让他们读更少的数据；并发写的处理原则也一样，它要求我们在数据库层面独立出来一个库，做特殊的处理。另外，我们还要针对秒杀系统做一些保护，针对意料之外的情况设计兜底方案，以防止最坏的情况发生。 总的来说，架构是一种平衡的艺术，而最好的架构一旦脱离了它所适应的场景，一切都将是空谈。具体设计应该参照秒杀预估流量： QPS 小于1W：只需要把商品购买页面增加一个定时上架功能，仅在秒杀开始时才让用户看到购买按钮，当商品库存卖完了也就结束了。 随着请求量加大（QPS 1W/s -&gt; 10W/s），这个简单的架构就很快遇到了瓶颈，因此需要做架构改造来提升系统性能。 QPS 100W/s 以上 怎样设计降低服务压力一、架构设计原则1.数据要尽量少 用户请求的数据能少就少。具体包含上传给系统的数据和系统返回给用户的数据。原因是首先这些数据在网络传输需要时间，其数据传输都需要服务器做压缩和字符编码，都非常消耗CPU，所以减少传输的数据量可以显著减少CPU的使用。 系统依赖的数据能少就少。依赖的路径越多会增加CPU处理时间（序列化和反序列化），同样会增加延时。 常见设计手段为：动静分离。 动静分离具体为变刷新整个页面为只点击“秒杀”按钮就够了。动静分离后，客户端大幅度减少了请求的数据量。分离改造核心：分离出动态数据。 如url唯一化，分离浏览者相关因素，分离时间因素，异步化地域因素，去掉cookie等。 对静态数据缓存：1. 静态数据缓存到离用户最近的地方。浏览器、CDN、服务端Cache。2.静态化改造直接缓存HTTP连接 3. Web服务器流入Nginx缓存静态数据优于Tomcat。对动态数据缓存:1.ESI（edge side includes）服务端拼接动静态内容，组装一起返回，服务端性能有影响，但是客户端体验好2.CSI（client side include）客户端发起异步js请求，服务端性能好，客户端可能会有延时，体验稍差. 部署架构：需要解决（失效问题，命中率问题，发布更新问题），其他细节：浏览器缓存和cdn缓存差别很大；合并是否用gzip压缩。 2.请求数要尽量少用户请求的页面返回后，浏览器渲染这个页面还包含其他的额外请求。例如页面依赖的CSS/JS， 图片， Ajax请求等都被定义为“额外请求”，这些额外请求应该尽量少。因为上述每个资源请求都能增加连接（需要做三次握手），可能造成资源串行加载，不同域名还有DNS解析。解决办法：合并CSS/JS文件。 常见设计手段为：流量削峰。 流量削峰本质上：延缓用户请求的发出。让服务处理更加平稳，节省服务器成本。削峰基本思路如下： 排队：用MQ来缓冲瞬时流量，把同步的直接调用转换成异步的间接推送，中间通过一个队列在一段承接瞬时的流量洪峰，在另一端平滑的将消息推送出去。 除了利用MQ,还可以使用线程池加锁方式实现排队，FIFO内存排队。这样就会存在异步返回结果问题：解决方案有两种1. 客户端轮询，例如支付页面，每秒轮询一次； 2.服务端push结果。需要C/S保持长连接。 答题：防作弊，延缓请求。 分层过滤：对请求进行分层过滤，讲请求尽量拦截在系统上游。传统秒杀系统之所以挂，请求都压到在后端数据库层，数据读写锁冲突严重，并发响应高，几乎所有请求都超时。流量虽大，下单成功的有效流量甚小。分层过滤其实就是采用“漏斗式”设计来处理请求。核心思想为：在不同层次尽量过滤掉无效请求[根据库存判断无法抢到商品的人]，让“漏斗”最末端的才是有效请求。 读系统尽量减少一致性校验的瓶颈，但尽量将不影响性能的检查条件提前 写系统主要对写数据进行一致性检查 3.路径尽量短路径：用户发出请求到返回数据这个过程中，需要经过的中间节点数。 这是因为每增加一个连接都会增加新的不确定性。从概率统计上说，假如一个请求经过5个节点，每个节点可用性是99.9%的话，那么整个请求的可用性是：99.9的5次方，约等于99.5%。缩短路径不仅可以增加可用性，同样可以有效提升性能（减少中间节点可以减少数据的序列化和反序列化），并减少延时。 有一种缩短访问路径办法: 多个相互强依赖的应用合并部署在一起，把远程调用RPC 变成JVM内部之间的方法调用。 4.依赖要尽量少，系统分级展示秒杀页面，这个页面必须强依赖商品信息、用户信息，还有其他如优惠券、成交列表等这些对秒杀不是非要不可的信息（弱依赖），这些弱依赖在紧急情况下就可以去掉。要减少依赖就必须对系统进行分级。0级系统要尽量减少对1级系统的强依赖，防止重要的系统被不重要的系统拖垮。在极端情况下可以把1级系统例如优惠券系统降级。 5.不要有单点无单点的重点是避免将服务的状态和机器绑定，即把服务无状态化。 应用无状态化是有效避免单点的一种方式，但是像存储服务本身很难无状态话，因为数据要存储在磁盘上，本身就要和机器绑定，那么这种场景只能通过冗余多个备份来解决单点问题。 二、热点数据处理为什么要处理热点数据？热点请求会大量占用服务器处理资源，虽然这个热点可能只占请求总量的亿分之一，然而却可能抢占90%的服务器资源。 什么是热点：热点分为热点操作和热点数据。 热点操作，例如大量的刷新页面、大量的添加购物车、双十一零点大量的下单。对系统来说，这些操作可以抽象为“读请求”和“写请求”。热点操作中的写操作将下面单独一节讲解。 热点数据：用户的热点请求对应的数据。热点数据分为“静态热点数据”和“动态热点数据”。 静态热点数据：可以提前预测的热点数据。业务场景，通过卖家报名来打标。还可以通过数据分析历史成交记录，用户购物车记录分析出热点商品。 动态热点数据：不能被提前预测的热点数据，系统在运行过程中临时产生的热点。例如上家临时做了广告导致的热点数据。解决方案：构建数据动态发现系统，分析热点Key，数据上报统计。 处理热点数据：一、优化 ： 缓存。热点数据动静分离。二、限制 ： 热点数据限制到一个请求队列里，防止热点数据占用太多服务器资源导致其他请求无法处理。三、隔离 系统隔离：为避免对现有网站业务的冲击：分组部署，将热点描述请求分到单独的集群。秒杀系统只是一个短时的促销活动，具有时间短、访问量高的特点。如果模块与原业务系统部署在一起，将对现有的业务造成冲击。因此，应当把秒杀模块迁移出去，独立部署。 数据隔离：热点秒杀数据启用单独的Cache/MySQL集群。 业务隔离：卖家报名秒杀提前感知热点，做数据预热。 三、 性能优化核心：降低CPU消耗。 衡量指标1总QPS = （1000ms / RT） * 线程数量 其中线程数量一般默认配置为 2*CPU核数 + 1。 优化方法 减少编码 减少序列化 服务优化（如nginx返回静态数据，框架定制优化） 并发读优化：应用层的LocalCache，在秒杀系统的单机上缓存商品相关的数据. 静态数据（秒杀前全机推静态cache数据） 动态数据（类似库存，一般缓存几秒，被动失效，允许一定的脏数据） 流程：发现数据，减少短板，数据分级，减少中间环节，做好应用基线（性能基线，成本基线，链路基线）不断调整 四、并发写-减库存秒杀系统设计除了上述的并发读的问题，还有一个难点是如何解决并发写 – 多个用户在同时抢一件商品，也就是并发很高，但集中在同一商品上，造成实质为串行操作。因为在数据库这层本质执行的是对同一件商品扣库存 – 需要合理的减库存。用户的购买过程一般分两步：下单和付款。123BEGIN UPDATE stock SET count = count - 1 WHERE skuId = ?COMMIT 减库存一般有三种方式: 下单减库存。下单减库存最简单也是控制最精确的一种，下单时直接通过数据库的事务机制控制商品库存，这样一定不会出现超卖情况。缺点是：恶意下单（有些人下单完不一定会付款，但是库存已经扣了，会影响商家。） 付款减库存。等到用户付款完后才真正减库存，否则库存一直保留给其他买家。缺点是出现买家下单后无法付款。缺点：可能超卖。 预扣库存。 买家下单后库存为其保留一段时间，超过这个时间，库存会自动释放。释放后其他买家继续购买。在买家付款前，系统会校验该订单的库存是否还有保留，有保留则尝试预扣，如果预扣失败，则不允许付款；如果预扣成功，则完成付款减库存。缺点：也可能恶意下单（只能结合安全和反作弊，标示用户并限制操作） 。 一般情况下秒杀减库存逻辑复杂，存在SKU库存和总库存联动关系，需要使用MySQL事务. 由于同一数据在数据库里肯定是一行存储，因此会有大量线程来竞争InnoDB行锁，而并发度越高等待线程会越多，TPS会下降，响应事件RT会上升，数据库的吞吐量就会严重手影响。这就会发生单个热点商品影响整个数据库性能，导致0.01%商品影响99.99的商品的售卖。 解决并发锁的问题： 乐观锁/悲观锁 悲观锁：可能会造成大量线程抢锁等待，结果可能会瞬间增大响应时间，造成系统连接数耗尽。 乐观锁：根据版本号的思路，可能会操作操作失败次数增多，需要上层业务重试，或者交给用户重试。12select * from tab1 where id = ?udpate tab1 set col1 = ? where id = ? and version = ? 缺点：在高并发下可能更新失败，所以要通过重试来提高更新效率。 FIFO队列排队：并行强制改成串行，单机内存队列，如果生产远高于生产可能造成内存爆掉。即使内存没问题，如果消费过慢用户响应时间也会长。 redis watch如果可以把数据放到内存数据库中，可以考虑redis watch机制，采用乐观锁方式更新。123456WATCH mykey val = GET mykey val = val + 1 MULTI SET mykey $val EXEC 五、高可用建设 参考 极客时间-如何设计一个秒杀系统 秒杀系统架构分析与实战 MySQL的并发更新","link":"/2019/02/05/secondkill/"},{"title":"接口超时引起系统雪崩原因反思","text":"笔者亲身经历的一次线上服务雪崩，可谓刻骨铭心…经过此次故障，不断反思，不断复盘，成长颇多。 背景10.15 下午16：55开始，UPM - API 服务接口延时&amp;报错数报警猛增（错误数多为500，502，499，504），一度UPM-API无法对外提供服务。通过排查定位到接口/user/get/permissions调用量增加了十倍（20QPS），因此给/user/get/permissions接口加了限流策略，18:17 UPM-API故障解除。 故障点错误数监控：可以看出故障点499错误数（还有500，502）猛增。499错误码一般发生在客户端主动断开连接，当服务端处理慢了，超过了客户端的超时时间，就会抛出499。 故障点99分位接口延时监控:可以发现故障点接口服务几乎整体都受到了影响，接口延迟最差的时候到了5s左右。 故障点CPU Idle:17:00左右出故障的时候进一步下跌。因此排查思路是“什么导致了CPU IDLE异常下跌。”通过看20天的CPU波动图，可以看出故障点附近CPU IDLE异常下跌（机器3*48core 物理机）。看故障点CPU IDLE，可以看出16：30 CPU IDLE 83.736%, /user/get/permissions QPS情况:再看/user/get/permissions流量在17点左右流量较上周/前一天均增加了百倍。QPS达到23（1412/60s）左右系统难以支持，引发UPM API所有服务雪崩。 /user/get/permissions 问题复现问题复现的最好办法是就是找没有正式流量的机器进行压测，由于我们YS机房刚好没有正式线上流量，并且已经部署完毕（4 core / 8Gi /10台docker soa-serivce服务）。于是选择对YS机房进行压测。 不过为了使压测准确，需要先看下引发故障的这个接口的代码逻辑，模拟出尽量真实的压测接口参数 代码逻辑:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374@GetMapping(\"/get/permissions\")public ResponseEntity&lt;PermisssionResponseDto&gt; getPermissions(@SessionAttribute(ApiConstants.REQUEST_APPID) Long appId, UserParamDto userParamDto) { --- 隐去不重要代码 --- UserDto userDto = getUserByName(userParamDto); PermisssionResponseDto response = new PermisssionResponseDto(); response.setUser(BeanUtils.map(UserResponseDto.class, userDto)); // 获取用户的所有角色 - 有缓存 List&lt;RoleDto&gt; roleDtoList = userService.selectRoleByUserId(appId, userDto.getId()); response.setRoles(BeanUtils.mapList(RoleResponseDto.class, roleDtoList)); List&lt;Long&gt; roleIds = new ArrayList&lt;&gt;(); if (roleDtoList != null) { roleIds = roleDtoList.stream().map(RoleDto::getId).collect(Collectors.toList()); } // 获取用户的flag - 查DB,无缓存，但是有索引 List&lt;FlagDto&gt; flagList = userService.selectFlagByUserId(appId, userDto.getId(), roleIds); response.setFlags(BeanUtils.mapList(FlagResponseDto.class, flagList)); // 获取用户地区 - 猜测是他引起的故障 List&lt;AreaDto&gt; appUserAreaDto = appUserAreaService.selectUserAreaBusinessList(userDto.getId(), appId, null, true, false); response.setAreas(BeanUtils.mapList(AreaResponseDto.class, appUserAreaDto)); return ResponseEntity.success(response);}public List&lt;AreaDto&gt; selectUserAreaBusinessList(Long userId, Long appId, List&lt;Long&gt; businessIds, boolean needChild, boolean childTypeIsParent) { // 获取用户绑定的地区 List&lt;AppUserArea&gt; appUserAreaList = appUserAreaMapper.selectUserAreaBusinessList(userId, appId, businessIds, null); // 获取businessIds绑定的地区 - businessId =null导致返回了全表数据 Map&lt;Long, AppUserArea&gt; appUserAreaMap = appUserAreaList.stream() .collect(Collectors.toMap(o -&gt; o.getAreaId(), v -&gt; v, (o1, o2) -&gt; o2)); appUserAreaList.addAll(getRoleArea(appId, userId, appUserAreaMap, businessIds)); Wrapper&lt;Area&gt; areaWrapper = new EntityWrapper&lt;&gt;(); // 致命所在！businessIds传递的是null, mybatis-plus in的输入参数为null时候默认为全部 areaWrapper.in(\"business_id\", businessIds); areaWrapper.eq(\"is_delete\", IsDeleteEnums.NODELETE.getValue()); List&lt;Area&gt; areaList = areaMapper.selectList(areaWrapper); List&lt;Area&gt; res; if (childTypeIsParent) { res = mergeChildWithParentId(appUserAreaList, areaList, needChild); } else { res = mergeChildWithLeftRight(appUserAreaList, areaList, needChild); } return BeanUtils.mapList(AreaDto.class, res);} private List&lt;Area&gt; mergeChildWithLeftRight(List&lt;AppUserArea&gt; appUserAreaList, List&lt;Area&gt; areaList, boolean needChild) { Map&lt;Long, Area&gt; areaMap = areaList.stream().collect(Collectors.toMap(Area::getId, s -&gt; s, (e, n) -&gt; e)); List&lt;Area&gt; response = new ArrayList&lt;&gt;(); // 二重循环计算量巨大 for (AppUserArea au : appUserAreaList) { Area area = areaMap.get(au.getAreaId()); response.add(area); if (needChild) { for (Area a : areaList) { if (a.getLft() &gt; area.getLft() &amp;&amp; a.getRght() &lt; area.getRght() &amp;&amp; a.getBusinessId() .equals(area.getBusinessId())) { response.add(a); } } } } return response;} 通过看selectUserAreaBusinessList代码可以猜出问题所在：MyBatis-Plus是 Mybatis 的增强工具, 可以方便做数据层的开发操作。但是 areaWrapper.in(“business_id”, businessIds); 一旦businessIds为null的时候，操作为搜索全表所有数据。因此其算法复杂度为最差O（n*m）,平均O(n)，最好O(n),其中n为upm_area表的未被删除的记录数（目前38000+，并且还会持续增大)，m为用户绑定的地区数。返回全表地区数据创建的这个List areaList对象消耗性能实在是太大了，会占用大量的连接以及JVM内存。《深入理解Java虚拟机》中曾指出，占据大量连续内存空间的大对象（典型的大对象就是那种很长的字符串以及数组），对虚拟机的内存分配来说，是一件坏消息，特别是一群“朝生夕灭”的“短命大对象”，容易引发Full GC。所以猜测，流量高起来的时候，List的频繁分配容易触发高频的FGC，进而导致服务雪崩。 单接口压测验证通过单接口压测，发现当QPS=60的时候，CPU掉底为0。 通过top查看消耗CPU的进程pid为1557, 使用JVM 的jstack工具打印 1557进程的所有线程当前时刻正在执行的方法堆栈追踪情况，即线程快照。通过快照可以定位出线程长时间出现停顿的原因。 jstack 1557 &gt; 1557.log VM Thread”就是CPU消耗较高的线程。VM Thread是JVM层面的一个线程，主要工作是对其他线程的创建，分配和对象的清理等工作的。通过Jstack 出来的日志可以看出JVM正在进行大量的GC工作。 进一步通过jstat查看什么导致的大量GC.可怕的事情发现了：60s内FGC竟然触发了3次 – 即20s一次FGC的频率。要知道FGC号称stop-the-world, 10s一次FGC是对于线上服务来说是十分可怕的。频繁的出现FGC肯定是出现大对象占用了内存，YGC无法销毁堆积到了老年代，老年代不断被占满才导致FGC。 进一步通过jmap看看JVM内存中创建的对象情况： 可以发现Area这个实例创建的非常多，有2883677个;总共占用内存184555326bytes, 即176M; 而单接口瓶颈值60QPS，即可导致每秒后端60个请求，也就是每秒从DB获取38000个area * 60qps = 2280000个Area对象，而不断的产生的Area对象最终导致了老年代被占满。com.mysql.jdbc.ByteArrayRow的实例也非常多3661658，无论是实例个数还是占用内存最多的是字节数组[B, 实例42727219个，占用内存1G多，这都是不正常的。再回看涉及到Area 从DB获取的代码：123456Wrapper&lt;Area&gt; areaWrapper = new EntityWrapper&lt;&gt;();// 致命所在！businessIds传递的是null, mybatis-plus in的输入参数为null时候默认为全部areaWrapper.in(\"business_id\", businessIds);areaWrapper.eq(\"is_delete\", IsDeleteEnums.NODELETE.getValue());List&lt;Area&gt; areaList = areaMapper.selectList(areaWrapper);select 语句返回了全表38000+条数据生成了大对象List&lt;Area&gt;areaList，导致com.mysql.jdbc.ByteArrayRow猛增，ByteArrayRow是缓存数据库结果的类，会产生大量的byte数组，这也解释了字节数组[B大量生成的原因。由此证明：从DB获取大量数据的操作占用了大量内存，触发了频繁的FGC. 再看此服务的内存消耗： pmap -d 1557 实际内存占用近5G 超过了我们设置的JVM内存大小 4G （jvm_args=-Xmx4g -Xms4g）总结此次故障原因为：/user/get/permissions（由于历史原因是一个一直性能不高但是没有调用量的接口，一直未受到重视）此次接口QPS陡增了10倍，即增加到20qps左右，服务内部执行了大量的返回全Area表数据操作，触发了频繁的FGC会致使对内计算延迟，对外请求积压，导致服务整体雪崩。 反思 慎用mybatis-plus的in(null)，一定要校验入参是否是null, 返回全表数据是十分消耗性能的操作。 CPU idle 迅速掉很有可能是某个接口当调用量大的时候性能变差，触发了频繁的FGC. 限流一定要加到所有接口上，木桶原理。 服务一定要加FGC频繁的报警，当触发频繁FGC，一定是服务性能达到瓶颈了。","link":"/2018/10/15/接口超时引起系统雪崩原因反思/"},{"title":"MySQL的锁","text":"时不时就要来翻翻看。 锁的使用场景在多用户环境中，同一时间可能会有多个用户同时更新相同记录，这就会产生冲突 - 并发性问题。典型的冲突有： 丢失更新：一个事务的更新覆盖了其他事务的更新结果，就是所谓的丢失更新：例如用户A把值从6改为2， 用户B把值从2改为6，则用户A丢失了他的更新。 脏读，脏读又称无效数据的读出，是指在数据库访问中，事务T1将某一值修改，然后事务T2读取该值，此后T1因为某种原因撤销对该值的修改，这就导致了T2所读取到的数据是无效的。 脏读就是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。 为了解决并发的冲突问题，需要引入并发控制机制。而并发控制机制最常用的办法就是加锁。当一个用户锁住数据库中的某个对象时，其他用户就不能再访问该对象。加锁对并发访问的影响体现在锁的粒度上。 全局锁一句话特点：InnoDB不用，MyISAM才用。 MySQL提供的针对数据库级别的对数据加读锁的功能:Flush tables with read lock（FTWRL）。之后针对这个数据库的增删改、DDL、事物提交语句都会被堵塞住。他主要的用途是用于数据库全库的逻辑备份. 【优点】全局锁是数据库级别的，所有表引擎都支持，在数据的导出对库实例加锁，保持导出数据逻辑的一致性。和设置数据库只读（set global readonly=true）相比，全局锁在当前链接异常或者中断的情况下可以自动释放，而设置数据库只读不能。注：对于Innodb引擎，我们推荐用使用mysql自带的mysqldump工具，并且使用参数–single-transaction.( 导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中，数据是可以正常更新的。这样就不需要全局锁，但是只适用于InnoDb引擎)。如果表不支持MVCC，我们还是只能利用全局锁。 表级锁MySQL的表级锁分俩种，一种是表锁，一种是元数据锁（meta data lock MDL） 表锁语法是 lock tables … read/write。可以用unlock主动释放锁。也可以在链接中断时候自动释放。表锁对于自己和其他线程读写操作的限制如下：如果对一个表进行read/write锁。其他线程在写or读/写时都会堵塞；同时本线程也只能读or读写该表，其他表无法访问。 缺陷：锁的粒度太大导致影响面大。 使用场景：在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式（MyISAM）。而对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令控制并发。 元数据锁（MDL）该锁不会显式的使用，在访问表的时候会自动加上，他用来保护数据字典元数据，保证在并发情况下，结构变更的一致性。 123456结构变更的一致性 ：读写正确性是针对DDL操作来说的，保证多线程读写DB的操作中，任何线程都能读到最新的表结构的数据。举个例子来说：如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定不行的。 原理上MDL锁的加锁模式是：mutex + condition + queue来实现类似公平锁的并发阻塞唤醒机制。DDL语句发起时候，如果无法获取排他锁，那么DDL将进入阻塞状态，但是由于阻塞队列的设计，就阻塞了后面所有的DML和SELECT操作，在高并发系统上，有可能引起雪崩。 他有如下特点： 对一个表的数据CRUD时候，加MDL读锁；当修改表结构时候，加MDL写锁 读锁是不互斥的，因为这些操作不会改表结构，所以可以多个线程同时对表做CRUD； 读写锁之间，写锁之间是互斥的，也就是说当对表进行CRUD时候，为了保证返回数据的读写的正确性，DDL操作是堵塞的。 这也是为什么公司RDS不允许高峰期提工单执行DDL的原因之一。 另外由于上面MDL的读写锁机制，就会有下面这种情况，修改一个访问量很高的小表，会导致整个库挂掉：比如: 大量的select语句，加了MDL读锁，这时候是不会堵塞的;这时候有一条alter表的语句需要执行，加了MDL写锁，开始block。由于select量很大，alter会一直堵塞，这时候后续的select也会堵塞，很块连接池就被用完了。 解决方法：1.首先解决长事务，事务不提交就会一直占着MDL锁。在MySQL中找到正在执行的事务，kill掉长事务。2.如果是变更的表是热点表，虽然数据量不大，但是请求频繁，在加字段的时候处理办法只能为：给alter table语句里面设置等待时间，如果能在指定等待时间内拿到MDL写锁最好，拿不到也不阻塞后面语句，先放弃。MariaDB&amp;AliSQL解决了这个问题。 行锁MySQL不显示指定行锁，都是InnoDB自动加的。MyISAM没有行锁，这也是逐渐MyISAM被互联网抛弃的原因。 Innodb的行锁定分为两种类型，共享锁-S锁和排他锁-X锁。而在锁定机制的实现过程中，为了让行级锁定和表级锁定共存，InnoDB也同样使用了意向锁（表级锁定）的概念，也就有了意向共享锁和意向排他锁两种。 另外，对于insert,update,delete, Innodb会自动给涉及的数据加X锁；对于一般的select语句，innodb不会加任何锁，事务可以通过语句给显式加共享锁或者排他锁；对于alter操作会锁表： 行锁的两阶段性为了加强数据库的并发度引入的锁的机制，他有如下几个特点： 行锁是由MySQL的引擎决定，Innodb支持行锁，MyISAM不支持行锁。 Innodb的行锁是两阶段锁：即数据需要锁的时候对数据涉及到的行加锁；在事务结束后才释放行锁； 为了减少锁的冲突，我们在事务中要把会引起锁冲突的语句往后放。因为事务是原子性的所以，在一个事务中，要么都成功要么都失败，所以我们在一个事务中根据业务需求可以把一些没有锁冲突的操作或者语句放在事务的前面先只是，可能会造成锁冲突的语句放在后面，减少锁冲突加大并发。 先说个非MySQL内部机制，但是可以通过语句实现的乐观锁。 乐观锁【解决问题】多线程并发场景对共享资源控制，避免数据冲突。例如售卖场景。【具体技术】每次读数据的时候认为没有冲突不会上锁，只是在提交更新的时候去判断数据是否已经被其他事务修改。乐观锁的实现上一版使用判断版本号Version的方式：为数据增加一个版本标识，一版是通过为数据库表增加一个数字类型的version。当读取数据时候，将version字段一并读出，数据每更新一次，对此version值加1. 当提交更新的时候，判断数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据。【适用场景】乐观锁假定事务之间的数据竞争概率是比较小的，只有提交的时候才校验，所以不会产生实际上的任何锁和死锁。乐观锁适用于读多写少-冲突较小的场景，加锁时间短，这样可以提高吞吐量。【缺点】乐观锁不能解决脏读问题。并且增加读数据的次数，例如如果第二个用户恰好在第一个用户提交更改之前读取了该对象，那么当他完成了自己的更改进行提交时，数据库就会发现该对象已经变化了，这样，第二个用户不得不重新读取该对象并作出更改。这说明在乐观锁环境中，会增加并发用户读取对象的次数。 例子:电商下单减库存避免超卖。这个过程需要保证数据库一致性：某人点击秒杀后系统中查出来的库存量和实际扣减库存量一致123456789CREATE TABLE `tb_product_stock` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;自增ID&apos;, `product_id` bigint(32) NOT NULL COMMENT &apos;商品ID&apos;, `number` INT(8) NOT NULL DEFAULT 0 COMMENT &apos;库存数量&apos;, `create_time` DATETIME NOT NULL COMMENT &apos;创建时间&apos;, `modify_time` DATETIME NOT NULL COMMENT &apos;更新时间&apos;, PRIMARY KEY (`id`), UNIQUE KEY `index_pid` (`product_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&apos;商品库存表&apos;; 不考虑并发：12345678910Public boolean updateStockRaw(Long productId) { ProductStock product = query(“select * from tb_product_stock where product_id = #{productId}”, productId); if (product.getNumber() &gt; 0) { int updateCnt = update(\"UPDATE tb_product_stock SET number=number-1 WHERE product_id=#{productId}\", productId); // 扣库存 if(updateCnt &gt; 0){ //更新库存成功 return true; } } return false;} 上述代码在多线程并发情况下会存在超卖可能。 乐观锁:1234567891011121314151617181920/** * 下单减库存 * @param productId * @return */public boolean updateStock(Long productId){ int updateCnt = 0; while (updateCnt == 0) { ProductStock product = query(\"SELECT * FROM tb_product_stock WHERE product_id=#{productId}\", productId); if (product.getNumber() &gt; 0) { updateCnt = update(\"UPDATE tb_product_stock SET number=number-1 WHERE product_id=#{productId} AND number=#{number}\", productId, product.getNumber()); if(updateCnt &gt; 0){ //更新库存成功 return true; } } else { //卖完啦 return false; } } return false;} 使用乐观锁更新库存的时候不加锁，当提及更新时需要判断依据是否已经被修改（AND number = #{number}），只有在number 等于上一次查询到的number时才提交更新。 悲观锁【解决问题】多线程并发场景对共享资源控制，避免数据冲突。例如售卖场景。【具体技术】假定会发生冲突，屏蔽一切可能违反数据完整性的操作。因此每次读数据的时候都会上锁，直接block其他想要读数据的线程。Java synchronized就属于一种悲观锁，每次线程要修改稿数据时候都要先获得锁，保证同一时刻只有一个线程能操作数据，否则就会被block.【适用场景】对并发控制严格的场景。【缺点】加锁时间长，并发粒度小，可能会长时间限制其他用户的访问。12345678910111213141516/** * 更新库存(使用悲观锁) * @param productId * @return */ public boolean updateStock(Long productId){ //先锁定商品库存记录 ProductStock product = query(&quot;SELECT * FROM tb_product_stock WHERE product_id=#{productId} FOR UPDATE&quot;, productId); if (product.getNumber() &gt; 0) { int updateCnt = update(&quot;UPDATE tb_product_stock SET number=number-1 WHERE product_id=#{productId}&quot;, productId); if(updateCnt &gt; 0){ //更新库存成功 return true; } } return false; } 悲观锁在MySQL,Java有广泛的使用： MySQL的读锁，写锁，行锁等。 Java的synchronized关键字 共享锁 / 读锁共享锁(Shared Lock)又称读锁(Select Lock)，S锁， 是读取操作创建的锁。其他事务可以并发读取数据，但是任何事务都不能对数据进行修改（获取数据上的排他锁）。因为加上共享锁后，对于update,insert,delete语句会自动加排它锁。,因此如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁。获得共享锁的事务只能读数据，不能修改数据。 加了共享锁后，MySQL会对查询结果中的每行都加共享锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请共享锁，否则会被阻塞。其他线程也可以读取使用了共享锁的表，而且这些线程读取的是同一个版本的数据。1select ... lock in share mode 【使用场景】业务开发一般事务中并不用它。DBA会在主-从表的这种情况, 比如想在从表insert一条记录, 需要先将主表相关的数据加S锁锁定, 然后再insert从表, 来实现主从表数据一致性, 即有可能其他session会再此时delete主表的这条数据而造成只有从表有数据而主表无数据的数据不一致结果。【例子】然后在另一个查询窗口模拟另一个事务，对id=1的商品进行扣库存处理。会发现操作界面进入卡顿，然后超时报错。【缺点】容易造成死锁。即session1在数据加S锁, 然后session2在相同数据也加S锁, 然后同时update, 必然会导致其中一个session的事务监测到deadlock,而终止事务。 排它锁排它锁(Exclusive Lock)又称写锁(Update Lock)，X锁。若事务 1 对数据对象A加上X锁，事务 1 可以读A也可以修改A，其他事务不能再对A加任何锁，直到事物 1 释放A上的锁。这保证了其他事务在事物 1 释放A上的锁之前不能再读取和修改A。排它锁会阻塞所有的排它锁和共享锁。 读取为什么要加读锁呢：防止数据在被读取的时候被别的线程加上写锁,出现死锁。1select ... for update 行锁与事务设想一个情景，如果一个事务A要更新一行，如果刚好有另外一个事务B有这一行的行锁，那么事务A会被锁住，进入等待状态。问题是，既然进入了等待状态，那么等到这个事务自己获取到行锁要更新数据的时候，它读到的值是什么？ 一句话重点：MySQL可重复读的隔离级别下，更新是当前读（具体说为更新数据都是先读后写，而这个读当前的值，称为“当前读” ），查询是一致性读。 另外，如果select语句不加锁，很可能出现也是典型的“丢失更新”问题：一个事务的更新操作会被另一个事务的更新操作覆盖。解决办法，在读取的时候设置一个读锁，保证当前读（读到最新数据的版本号）。例如： 加读锁（S锁，共享锁）：select k from t where id = 1 lock in share mode for update. 加写锁（X锁，排他锁）：select k from t where id=1 for update. 幻读与间隙锁(Gap Lock)幻读是指一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行的行。 事务在RR隔离级别下，普通的查询是一致性读，是不会看到事务插入的数据的。因此，幻读是在“当前读”下才出现–而这也是幻读出现的原因。 幻读专指“新插入的行”。幻读的危害：1.破坏了业务的语义；2.违反了数据的一致性。 为什么需要间隙锁？ 当数据库的隔离级别是RR的情况下，由于MySQL的行锁在为数据上锁的时候需要数据在表中，也就是说对于新插入的数据MySQL是无法上锁的。这样就会出现幻读的情况。InnoDB为了解决幻读引入了间隙锁。 间隙锁对于加锁的实体，可以是数据行。而数据行之间的空隙，也可以是加锁的实体。间隙锁有如下特性： 当一个涉及到为数据加锁的语句执行时候，会为涉及到的数据加行锁，同时为整个表的数据之间加间隙锁（gap lock）,这个行锁+间隙锁也叫next-key lock， InnoDB通过next-key lock解决了幻读的问题； 间隙锁开区间锁，next-key lock是个左开，右闭的空间； 间隙锁只会锁插入语句，其他的操作是不互斥的。与行锁的读写锁冲突不同，跟间隙锁存在冲突关系的，是“往这个间隙中插入一条记录”这个操作。间隙锁之间不存在冲突关系。 具体例子说明：表t(id, c(有索引))有数据(3，3),(5，5),(7，7)。有如下3个事务： t1 ,t2锁不冲突， t3会被间隙锁block。next-key-lock会把表分为这几个区间：(-∞,3],(3,5],(5,7],(7,suprenum]，其中suprenum是一个不存在的最大值。1234567891011#t1begin transaction;select * from t where c=5 for update;---会锁(3,5](5,7) 普通索引的等值查询从左往右直到第一个不满足条件的值（5，7）降为间隙锁,（3,5]是next-key lock#t2begin transaction;update t set c=1 where c=5 for share mode;---S锁不冲突#t3begin transaction;insert into t values(4,4);---因为间隙锁被block住 间隙锁的缺点间隙锁也并不是万能的，因为间隙锁导致同样的语句锁住更大的范围，其实影响了并发度，因此在某些情况下会造成死锁。比如下面的情况： 判断断C=5是否存在； 不存在就insert,存在就update； 在并发的情况下，且c=5不存在，行锁无效，触发间隙锁，这时候事物2的insert需要等待事物1commit之后释放间隙锁。 但是事物1，因为也insert导致了，不能释放锁就触发了死锁。这种方式的解决方案，在满足业务的前提下，将事物的隔离级别改为RC，并且将binlog置为row。 InnoDB加锁规则以下所述InnoDB加锁规则适用于MySQL 5.x系列&lt;= 5.7.24, 8.0系列&lt;=8.0.13， 且隔离级别为可重复读隔离级别。且遵守两阶段锁协议，即所有加锁的资源，都是在事务提交或者回滚的时候才释放。 原则1： 加锁的基本单位是next-key lock, next-key lock是前开后闭区间。原则2：查找过程中访问到的对象才会加锁。优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值查询的时候，next-key lock退化为间隙锁。一个bug: 唯一索引上的范围查询会访问到不满足条件的第一个值为止。 另外，锁是加在索引上的。例如select id from t where c=5 lock in share mode (c 有索引)，因为只有访问到的对象才加锁，而这个查询使用到了覆盖索引，并不需要访问主键索引，所以主键索引上没有任何锁。因此update t set d = d+1 where id=5也可以执行。 另外，非唯一索引的范围锁没有优化规则，不会蜕变为行锁，因此锁的范围更大，例如select * from t where c&gt;=10 and c &lt; 11 for update,索引c上的next-key lock为 （5，10] 和 （10， 15]. 另外，删除数据时候尽量加limit。这样不仅可以控制删除数据的条数，让操作更安全，还可以减小加锁的范围。 死锁死锁和死锁检测出现原因：循环的资源等待。举个例子：俩个事务，事务1:用户uid=1点赞,事务2：用户uid=1取消赞。如下：这时候在并发时候可能会造成死锁当事务1、事务2都执行完第一条语句时候，这时候事务1等待事务2执行完第二条语句释放，这时候事务2执行完第二条语句等待事务一释放12345678910#事务1 点赞BEGIN TRANSACTION;UPDATE reply SET like_ammount=like_ammount+1 WHERE reply_id=1 ;UPDATE user_sum SET like_sum=like_sum+1 WHERE uid=1;COMMIT;#事务2取消赞BEGIN TRANSACTION;UPDATE user_sum SET like_sum=like_sum-1 WHERE uid=1;UPDATE reply SET like_ammount=like_ammount-1 WHERE reply_id=1 ;COMMIT; 如何避免死锁 按同一顺序访问对象。 避免事务中的用户交互。 保持事务简短并在一个批处理中。 使用低隔离级别。 使用绑定连接 死锁处理办法 等待锁超时，用innodb_lock_wait_timeout来设置；（默认50s）用死锁检查的方式innodb_deadlock_detect设置为on，表示开启，一旦发生死锁回滚死锁链条中的某个事务让其他事务可以进行； 死锁检查真的很美好么？死锁检查是一个O(n*n)的时间复杂度的操作，因为每一个被堵塞的线程都会检查是否是自己导致了死锁。导致会有大量的线程做无谓的死锁检查，最终现象是CPU很高，但是并发度很低。 参考 https://juejin.im/post/5b5ea32351882519f6477c5c#heading-6 丁奇 https://time.geekbang.org/column/article/77083","link":"/2019/03/01/db-lock/"},{"title":"一条MySQL是怎么执行的？ - 《高性能MySQL》读书笔记","text":"前面一篇总结介绍了如何建立最好的索引，这些对于高性能来说必不可少。但是这些还不够，还需要合理的设计查询。如果查询写的很糟糕，即使库表结构再合理，索引再合理，也无法实现高性能。 这篇总结关注查询设计的一些基本原则，介绍一些更深的查询优化技巧，以及介绍MySQL优化器内部机制，也解答了MySQL是如何执行查询的和如何执行关联查询的。当向MySQL发送一个请求的时候，MySQL到底做了什么？ 客户端发送一条查询给服务器 服务器先检查查询缓存，如果命中了缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段。 服务器端进行SQL解析、预处理、再由优化器生成对应的执行计划。 MySQL根据优化器生成执行计划，调用存储引擎API来执行查询。 将结果返回给客户端。 为什么查询速度会慢对于查询来说，真正重要的是响应时间。如果把查询看做是一个任务，那么它由一系列的子任务组成，每个子任务都会消耗一点时间，如果要优化查询，实际上要优化子任务，要么消除其中一些子任务，要么减少子任务的执行次数，要么让子任务运行的更快。 查询的生命周期：客户端 -&gt; 服务端 -&gt; 在服务器上解析 -&gt; 生成执行计划 -&gt; 执行 -&gt; 返回结果给客户端。其中执行可以认为是整个生命周期中最重要的阶段，这其中包括了大量为了检索数据到存储引擎的调用以及调用后的数据处理，包括排序分组。 在完成这些任务的时候，查询需要在不同的地方花费时间，包括网络，CPU计算，生成统计信息和执行计划，锁等待（互斥等待）等操作，尤其是向底层存储引擎检索数据的调用操作，这些调用需要在内存操作，CPU操作和内存不足时导致的I/O操作上消耗时间。根据存储引擎不同，可能还会产生大量的上下文切换以及系统调用。 优化数据访问查询性能低下的最基本的原因是访问的数据太多。大部分性能低下的查询都可以通过减少访问的数据量的方式进行优化。对于低效的查询，我们发现通过下面两个步骤来分析总是很有效： 确认应用程序是否在检索大量超过需要的数据。这通常意味着访问了太多的行，有时候也是发访问了太多的列。 确认MySQL服务器层是否在分析大量超过需要的数据行。 是否向MySql请求了不需要的数据 避免多余行以及多余列。 select *：需要使用的时候确认是否需要真的返回全部列？select *取出全部列会让优化器无法完成索引覆盖扫描这些优化，还会给服务器带来额外的IO，内存和CPU消耗。 MySQL是否在扫描额外的记录衡量查询开销的三个指标： 响应时间响应时间 = 服务时间+排队时间。服务时间是指数据库处理这个查询真正花费了多场时间。排队时间是指服务器因为等待某些资源而没有真正执行查询的时间 - 等IO,等行锁等等。 返回的行数 扫描的行数理想情况是扫描的行数=访问的行数。但是例如在做关联查询，服务器必须要扫描多行才能生成结果集中的一行。扫描的行数对于返回的行数比率通常很小，一般在1：1和10：1之间。在评估查询开销的时候，需要考虑从表中找到某一行数据的成本。有些访问类型可能需要扫描很多行才能返回一行数据，有些可能无需扫描。 访问类型（从慢到快）：全表扫描 ALL -&gt; 索引扫描 -&gt;范围扫描 -&gt;唯一索引查询 -&gt; 常数引用 explain的type列反应了访问类型，例如下图id为主键索引： 这三个指标都会记录到MySQL的慢日志中，所以检查慢日志记录是找出扫描行数过多的查询的好办法。 一般MySQL能够使用如下三种方式应用WHERE条件，从好到坏依次为： 在索引中使用where条件来过滤不匹配的记录。这是在存储引擎层完成的。 使用索引覆盖扫描(在Extra列出现了Using index)来返回记录，直接从索引中过滤不需要的记录并返回命中的结果。这是在MySQL服务器层完成的，但无须回表查询记录。 从数据表中返回数据，然后过滤不满足条件的记录（在extra列中出现using where）.这在MySQL服务器层完成。MySQL需要先从数据表独处记录然后过滤。 重构查询的方式“一个复杂的查询还是多个简单查询” - 在传统实现中，总是强调需要数据库层完成尽可能多的工作，这样做的逻辑在于以前总是认为网络通信，查询解析和优化是一件代价很高的事情。但是这样的想法对于MySQL并不适用，MySQL从设计上让连接和端口连接都很轻量级，在返回一个小的查询结果方面都很高效。现代的网络速度比以前要快得多，无论是带宽还是延迟。在某些版本的SQL，也能够运行每秒超过10w的查询。所以运行多个小查询已经不是大问题了。 MySQL内部每秒能够扫描内存中上百万行数据，相比之下，MySQL响应数据给客户端就慢很多。在其他条件都相同的情况，使用尽可能少的查询当然是更好的。 很多高性能应用都会对关联查询进行分解。简单地，可以对每一个表进行一次单表查询，然后将结果在应用程序中进行关联。为什么？ 让缓存效率更高。许多应用程序可以方便地缓存单表查询对应的结果对象。如果关联中的某个表发生了变化，那么就无法使用查询缓存了，而拆分后，如果某个表很少改变，那么基于该表的查询就可以重复利用查询缓存结果了。 关联多表查询不利于写操作。执行读操作的时候，会锁住被读的数据，阻塞其他业务对该部分数据的更新操作。如果涉及多个聚合函数，相当于同时锁住多个表，不能进行读写，直接影响其他业务，影响了系统的整体性能。因此将查询分解后，执行单个查询可以减少锁的竞争。 不利于维护。业务发生表动时，比如join一张表改了，可能导致系统原有的SQL不可用，最终导致基于该SQL执行结果的上层显式失败。因此，在应用层做关联，可以更容易对数据库进行拆分，更容易做到高性能和可扩展。 查询本身效率也可能会有所提升。使用in代替关联查询，可以让mysql按照id顺序进行查询，这可能比随机关联更高效。 可以减少冗余记录的查询。在应用层做关联查询，意味着对于某条记录应用只需要查询一次，而在数据库中关联查询，则可能需要重复地访问一部分数据。从这点看，这样的重构还可能会减少网络和内存消耗。 更进一步，这样做相当于在应用中实现了哈希关联，而不是使用MySQL的嵌套循环关联。 因此，在很多场景，通过重构查询将关联放到应用程序中将会更加高效。例如：当应用能够很方便地缓存单个查询的结果的时候，当可以将数据分布到不同的MYSQL服务器上的时候，当能够使用IN（）方式代替关联查询到时候。 查询执行的基础1.连接MySQL的通信是半双工的。 查询状态：对于一个MYSQL连接或者说一个线程，任何时刻都有一个状态，该状态表示了MySQL当前正在做什么。1SHOW FULL PROCESSLIST //查看所有MYSQL TCP/IP连接 在一个查询的生命周期中，状态会变化很多次： Sleep： 线程正在等待客户端发送新的请求 Query：线程正在执行查询或正在将结果发送给客户端 Locked: 在MySQL服务器层，该线程正在等待表锁。在存储引擎级别实现的锁，例如InnoDB行锁，并不会体现在线程状态中。 Anlyzing and statistics:线程正在收集存储引擎的统计信息，并生成查询的执行计划 Copy to tmp table [on disk] 线程正在执行查询，并且将结果集都复制到一个临时表，这种状态一般要么是group by操作，要么是文件排序操作，或者是UNION操作。如果这个状态后面还有on disk,那标识Mysql正在将一个临时表存储在磁盘上 Sorting result 线程正在对结果集排序 Sending data 这表示多种情况：线程可能在多个状态之间传送数据，或者在生成结果集，或者在向客户端返回数据。了解这些状态的含义非常有用，这可以让你很快地了解当前“谁正在持球”。在一个繁忙的服务器上，可能会看到大量的不正常的状态，例如sttaistics 正在占用大量的时间。这通常标识，某个地方有异常了。具体参考http://www.ywnds.com/?p=9337 2.查询缓存在解析一个查询语句之前，如果查询缓存是打开的，那么MySQL会优先检查这个查询是否命中查询缓存中的数据。这个检查是通过一个对大小写敏感的哈希查找实现的。关于MySQL的缓存： 执行计划缓存。 很多DB产品都能缓存查询的执行计划，对于相同类型的SQL就可以跳过SQL解析和执行计划生成阶段。 查询缓存：缓存完成的SELECT查询结果。当查询命中该缓存，MySQL会立刻返回结果，跳过了解析、优化和执行阶段。 查询缓存系统会跟踪查询中涉及的每个表，如果这些表发生变化，那么和这个表相关的所有缓存数据都将失效。 3.查询优化处理如果查询没有命中MySQL缓存，查询的生命周期的下一步是将一个SQL转换成一个执行计划，MySQL再依照这个执行计划和存储引擎交互。 这包括多个子阶段：解析SQL，预处理，优化SQL执行计划。 语法解析器和预处理MySQL通过关键字将SQL语句进行解析，并生成一棵对应的解析树。MySQL解析器将使用MySQL语法规则验证和解析查询。 预处理器则根据一些MySQL规则进一步检验解析树是否合法，检验表和列是否存在，别名是否有歧义。 查询优化器：现在语法树被认为是合法的了，并由优化器将其转换成执行计划。一条查询可以有很多种执行方式，最后都返回相同的结果。优化器的作用就是找到这其中最好的执行计划。 关于执行计划推荐此文：https://blog.csdn.net/wuseyukui/article/details/71512793 MySQL使用基于成本的优化器，它将尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。可以通过查询当前会话的last_query_cost值来得知MySQL计算的当前查询的成本。这个结果标识MySQL优化器认为大概需要做1040个数据页的随机查找才能完成上面的查询 - 这是根据一些列的统计信息来计算得到的：每个表或者索引的页面个数、索引的基数，索引和数据行的长度、索引分布情况。优化器在评估成本的时候并不考虑任何层面的缓存，它假设读取任何数据都需要一次磁盘IO. MySQL的查询优化器使用了静态优化策略和动态优化策略来生成一个最优的执行计划。MySQL可以处理的优化类型： 列表in的比较。MySQL将in()列表中的数据线进行排序，然后通过二分 查找的方式来确定列表中的值是否满足条件，这是一个O(logn)复杂度的操作。而不是像其他数据库那样的相当于or（），对于in（）列表中有大量取值的时候，MySQL的处理速度将会更快。 重新定义关联表的顺序 - 关联表的顺便并不总是按照在查询中指定的顺序进行。 将外链接转换成内连接 - 并不是所有OUTER JOIN语句都必须以外连接的方式执行。诸多因素，例如where条件，库表结构都可能会让外连接等价于一个内连接。MySQL能够识别这点并重写查询，调整关联顺序。 优化COUNT(),MIN()和MAX()要找到某一列最小值，只需要查询对应B树索引最左端的记录，MySQL可以直接获取索引的第一行记录 - 优化器生成执行计划时候就利用了这一点。如下图，可以看到select tables optimized way. 它表示优化器已经从执行计划中移除了该表，并以一个常数取而代之。没有任何where条件的count(*)查询通常也可以使用存储引擎提供的一些优化，例如MyIsam维护了一个变量来存放数据表的行数。 覆盖索引扫描 - 无须回表 提前终止查询：当发现已经满足查询需求的时候，MySQL总是能够立刻终止查询。 4. 优化与执行MySQL服务器层并没有任何统计信息，所以MySQL查询优化器在生成查询的执行计划时，需要调用引擎的API. MySQL会解析查询，并创建一个内部数据结构（解析树），然后对其进行各种优化。其中包括重写查询，决定查询的读表顺序，以及选择使用的索引等。用户可以通过特殊的关键字给优化器传递各种提示，影响它的决策过程。另外还可以请求服务器给出优化过程的各种说明，使用户可以知晓服务器是如何进行优化决策的，为用户提供一个参考基准。 优化器并不关心某个表使用哪种存储引擎，但是存储引擎对服务器的查询优化过程有影响。优化器会请求存储引擎，为某种具体操作提供性能与开销方面的信息，以及表内数据的统计信息。 MySQL是如何执行关联查询的对于连接查询，MySQL先将一系列的单个查询结果放到一个临时表中，然后再重新读出临时表数据来完成UNION查询。 MySQL关联执行的策略很简单： MySQL对任何关联都执行嵌套循环关联操作，即MySQL先在一个表中循环取出单条数据，然后再循环到下一个表中寻找匹配的行；依次下去，直到找到所有表中匹配的行为止。然后根据各个表匹配的行，返回查询中需要的各个列。MySQL会尝试在最后一个关联表中找到所有匹配的行，如果最后一个关联表无法找到更多的行以后，MySQL返回到上一层次关联表，看是否能够找到更多的匹配记录，以此类推执行。 上面的执行计划对于单表查询和多表关联查询都适用，如果是一个单表查询，那么只需要完成上面外层的基本操作。对于外连接 从本质上说，MySQL对所有的类型的查询都以同样的方式运行。例如，MySQL在From子句中遇到子查询时，先执行子查询并将其结果放到一个临时表中，然后将这个临时表当作一个普通表对待。 MySQL优化器最重要的一部分就是关联查询优化，它决定了多个表关联时的顺序。通常多表关联的时候，可以有多重不同的关联顺序来获得相同的执行结果。关联查询优化器则通过评估不同顺序时的成本来选择一个代价最小的关联顺序。 不过糟糕的是，如果有超过n个表的关联，那么需要检查n！种关联顺序 - 所有可能的执行计划的搜索空间。 在关联查询的时候如果需要排序，MySQL会分两种情况进行排序。1.如果order by子句中所有列都来自关联的第一个表，那么MySQL在关联处理第一个表的时候就进行文件排序。在MySQL的explain结果中可以看到extra字段会有using filesort.2.其他情况，MySQL都会将先关联的结果存放到一个临时表中，然后在所有的关联都结束后，再进行排序。在MySQL的explain结果中可以看到extra字段会有using temporary;using filesort 最新版本的MySQL，当使用limit的时候，MySQL不再对所有结果进行排序，而是根据实际情况，选择抛弃不满足条件的结果，然后再排序。","link":"/2018/05/21/dive-into-mysql-execute/"},{"title":"ElasticSearch系列学习-1","text":"今天是一个要上班的愚人节 整理自ES.cn，并加入个人理解 What is Elastic Search exactly?ElasticSearch 是一个建立在全文搜索引擎Apache Lucene基础上的 实时分布式搜索引擎，而Lucene 是当今最先进，最高效的全功能开源搜索引擎框架。ElasticSearch降低了Lucene的学习和使用难度，用户可以使用ES统一的API即可进行全文检索，而不需了解Lucene背后原理。 ES 脸谱 基于Lucene，超越Lucene的搜索引擎 海量数据实时分析 分布式集群，易于扩展 近似于数据库的聚合功能 并非是一个全文检索系统.蜕变为一个完整的数据分析平台 Restful API Json over HTTP Basic ConceptsES节点 是一个运行中的ES 实例。 ES集群 包含一组节点，这些节点名字都是同一个cluster_name,他们一起工作并共享数据，还提供容错和可伸缩性。因此，ES可以横向扩展至数百甚至数千的服务器节点，同时可以处理PB级数据。可以说，ES天生就是分布式的，并且在设计时屏蔽了分布式的复杂性。 e.g.1234567891011121314http://localhost:9200/?pretty{ \"name\" : \"gsm-node\", \"cluster_name\" : \"gsm-cluster\", \"cluster_uuid\" : \"gVeY2ENwTsmGY4wQgvoxXQ\", \"version\" : { \"number\" : \"5.2.2\", \"build_hash\" : \"f9d9b74\", \"build_date\" : \"2017-02-24T17:26:45.835Z\", \"build_snapshot\" : false, \"lucene_version\" : \"6.4.1\" }, \"tagline\" : \"You Know, for Search\"} ES是 文档面向（file-oriented） 的，对象在ES中的存储形式都为文件，并且采用JSON 作为文档的序列化格式。 索引和类型：一个ES集群可以包含很多 索引， 每个索引可以包含多个 类型 ，这些不同的类型存储着多个 文档 ，每个文档又有 多个 属性 。类比于关系型数据库：1数据库/表/行/属性 == 索引/类型/文档/属性 当使用HTTP API 插入数据时候12345678910111213PUT /megacorp/employee/2{ &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 32, &quot;about&quot; : &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ]}the db is represented as megacorpthe table is represented as employee2 is the line of employeethe http body, which is json document includes all the properties of line 2. 索引和分片: Actually, 索引 实际上是指向一个或者多个物理 分片（shard） 的 逻辑命名空间。 一个 分片 是一个底层的 工作单元，是一个Lucene实例，它本身就是一个完整的搜索引擎。 在索引建立时候就已经确定了主分片数 why? ，但是副分片数随时可以修改。 ES 利用分片将数据分发到集群内各个节点. 分片是数据的容器，文档保存在分片中。当集群扩大或者缩小，ES会自动在各个节点迁移分片，使数据均匀分布在集群中。当PUT一个document的时候，Elasticsearch通过对docid进行hash来确定其放在哪个shard上面，然后在shard上面进行索引存储。 当进行查询是，如果提供了查询的DocID，Elasticsearch通过hash就知道Doc存在哪个shard上面，再通过routing table查询就知道located哪个node上面，然后去node上面去取就好了。如果不提供DocID,那么Elasticsearch会在该Index（indics）shards所在的所有node上执行搜索预警，然后返回搜索结果，由coordinating node gather之后返回给用户(哈希知分片。路由表知节点) 副分片是主分片的拷贝，用于灾备，搜索读服务。下图分别：“集群只有一个节点，一个主分片存储P0，P1，P2三个文档的情况”， “集群有两个节点，一个主分片存储P0，P1，P2三个文档，每个主分片有一个副本分片的情况”， “集群有三个节点，一个主分片存储P0，P1，P2三个文档，每个主分片有1个副本分片的情况”， Anyway, 分片是一个功能完整的搜索引擎，它拥有使用一个节点上的所有资源的能力。 我们这个拥有6个分片（3个主分片和3个副本分片）的索引可以最大扩容到6个节点，每个节点上存在一个分片，并且每个分片拥有所在节点的全部资源。 扩容：如果想扩容，使ES集群中的节点数增加，怎么办？ 一个索引能存储多少数据量在索引创建的时候就已经确定下来了 – 主分片数目在这时候已经确定。 但是，我们可以提高数据吞吐量，即增加replica shard的个数，因为replica shard 可以处理读操作，进行搜索并返回数据 。 But，当提高分片个数的时候，最好增加节点个数，否则每个分片从节点上获得的资源会更少。 更多副本分片处理索引 . 当有一个节点宕机，通过副本分片机制来容灾. ES单机多实例部署方法 Shards and replicas in ES Interact with ElasticSearch 使用Java API (9300) 分为 节点客户端 Node Client 和 传输客户端 Transport Client 使用Restful API with JSON over HTTP (9200) API 格式1curl -X&lt;VERB&gt; &apos;&lt;PROTOCOL&gt;://&lt;HOST&gt;:&lt;PORT&gt;/&lt;PATH&gt;?&lt;QUERY_STRING&gt;&apos; -d &apos;&lt;BODY&gt;&apos; How a document is indexed？一个 对象 是基于特定语言的内存的数据结构。 为了通过网络发送或者存储它，我们需要将它表示成某种标准的格式。 JSON 是一种以人可读的文本表示对象的方法。 它已经变成 NoSQL 世界交换数据的事实标准。当一个对象被序列化成为 JSON，它被称为一个 JSON 文档。 1object -&gt; JSON document 一个Employee JSON文档。1234567891011121314151617181920{ &quot;name&quot;: &quot;John Smith&quot;, &quot;age&quot;: 42, &quot;confirmed&quot;: true, &quot;join_date&quot;: &quot;2014-06-01&quot;, &quot;home&quot;: { &quot;lat&quot;: 51.5, &quot;lon&quot;: 0.1 }, &quot;accounts&quot;: [ { &quot;type&quot;: &quot;facebook&quot;, &quot;id&quot;: &quot;johnsmith&quot; }, { &quot;type&quot;: &quot;twitter&quot;, &quot;id&quot;: &quot;johnsmith&quot; } ]} 通过使用index API, 文档可以被索引 – 存储文档使之能够被搜索。123456PUT /website/blog/123{ &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Just trying this out...&quot;, &quot;date&quot;: &quot;2014/01/01&quot;} 这创建了低配版的索引，如果想自定义高配版（性能好）的索引，还需要自定义映射。 类似地，ES还提供GET, HEAD,UPDATE,DELETE等API. 当我们使用 index API 更新文档 ，可以一次性读取原始文档，做我们的修改，然后重新索引 整个文档 。 最近的索引请求将获胜：无论最后哪一个文档被索引，都将被唯一存储在 Elasticsearch 中。如果其他人同时更改这个文档，他们的更改将丢失。 在数据库领域中，有两种方法通常被用来确保并发更新时变更不会丢失。 悲观锁 这种方法被关系型数据库广泛采用。It assumes 变更冲突时常发生，因此阻塞访问资源以防止冲突。一个典型的例子是读取一行数据之前先将其锁住，确保只有放置锁的线程能够对这行数据进行修改。 乐观锁 Elasticsearch 中使用的这种方法假定冲突是不可能发生的，并且不会阻塞正在尝试的操作。 然而，如果源数据在读写当中被修改，更新将会失败。应用程序接下来将决定该如何解决冲突。 例如，可以重试更新、使用新的数据、或者将相关情况报告给用户。 ES通过version采用乐观锁的方式处理并发冲突。 ES 是分布式的。当文档创建、更新或删除 – 【写操作】时， 新版本的文档必须复制到集群中的其他节点。Elasticsearch 也是异步和并发的，这意味着这些复制请求被并行发送，并且到达目的地时也许 顺序是乱的 – 也解释了bulk 的api 格式。 ES 需要一种方法确保文档的旧版本不会覆盖新的版本。这也是version的作用：确保应用中相互冲突的变更不会导致数据丢失。通过制定想要修改文档的version号来达到目的，如果该版本不是当前版本号，请求会失败。 所有文档的更新或删除 API，都可以接受 version 参数，这允许你在代码中使用乐观的并发控制，这是一种明智的做法。12345PUT /website/blog/1?version=1{ &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Starting to get the hang of this...&quot;} 还有update API, update API 简单使用与之前描述相同的 检索-修改-重建索引 的处理过程。 区别在于这个过程发生在分片内部，这样就避免了多次请求的网络开销。通过减少检索和重建索引步骤之间的时间，我们也减少了其他进程的变更带来冲突的可能性。 Distributed document storage这一节讲的是上一章节的实现原理。 ES的主旨是随时可用和按需扩容。而扩容可以通过购买性能更强大（ 垂直扩容 ） 或者数量更多的服务器（水平扩容）来实现（垂直扩容有限，真正的扩容能力来自于水平扩容，并且将负载压力和稳定性分散到这些节点中）。ElastiSearch天生就是 分布式 的 ，它知道如何通过管理多节点来提高扩容性和可用性。 如果想要实现分布式，需要回答下面两个问题：1231. 文档是如何被索引，进而物理上被存储在一个主分片中的？2. ES是如何知道一个文档存在了哪个分片中的？ 这个过程根据下面公式决定： shard = hash(routing) % number_of_primary_shards其中routing是一个变值，默认是文档的_id. 这就解释了为什么我们要在创建索引的时候就确定好主分片的数量 并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。 所有文档的API(get, index, delete, bulk, update, mget)都接受一个叫做routing的路由参数，通过路由参数我们可以自定义文档到分片的映射。routing确保了相关文档（所有隶属于同一个用户的文档）都被存储到同一个分片中。 写操作 CRUD新建、索引和删除请求都是写操作。 Preliminaries: 每个节点都知道集群中任一文档位置[计算Hash查询，routing表]，所以可以直接将请求转发到需要的节点上。 以下是在主副分片和任何副本分片上面 成功新建，索引和删除文档所需要的步骤顺序： 1.客户端向 Node 1 发送新建、索引或者删除请求。 2.节点使用文档的 id 确定文档属于分片 0。请求会被转发到Node 3，因为分片 0 的主分片目前被分配在Node 3 上。 3.Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1 和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功，协调节点向客户端报告成功。 读操作 (已知doc Id的读，并非搜索的场景)可以从master shard or replica shard检索文档。 以下是从主分片或者副本分片检索文档的步骤顺序： 1、客户端向 Node 1 发送获取请求。 2、节点使用文档的 id 来确定文档属于分片 0 。分片 0 的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到 Node 2 。 3、Node 2 将文档返回给 Node 1 ，然后将文档返回给客户端。 为了读取请求，协调节点在每次请求的时候将选择不同的副本分片来达到负载均衡；通过轮询所有的副本分片。 批量读and写mget 步骤: client send mget to some node, such as Node1. Node1 calculdate the shards where the ducuments in mget stored. Node1 为每个分片构建多文档请求，并将这些请求并行发送到每个参与的节点。（一次发送多个文件，并行发送多个node） once getting all the response, Node1 will return them to client. bulk 步骤: client send bulk to some node, such as Node1. Node1 calculdate the shards where the ducuments in mget stored. Node1为每个节点创建一个批量请求，并将这些请求并发转发到包含主分片的节点主机 主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。 一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端。主分片操作完，转发给副分片 搜索一个CRUD操作只对单个文档进行处理，文档的唯一性由 _index, _type, 和 routing values （通常默认是该文档的 _id ）的组合来确定。 这表示我们确切的知道集群中哪个分片含有此文档。搜索需要一种更加复杂的执行模型，因为我们不知道查询会命中哪些文档（并不知道文档ID）:这些文档有可能在集群的任何分片上。一个 搜索请求 必须询问我们关注的索引（index or indices）的所有分片的副本来确定它们是否含有任何匹配的文档。 搜索查询步骤： step1: 客户端发送一个 search 请求到 Node 3(成为了 协调节点 ) ， Node 3 会创建一个大小为 from + size 的空优先队列 step2: Node 3 将查询请求转发到索引的每个主分片和副本分片中（ 广播 到每个分片copy, 这也是为什么增加分片copy可以增加搜索吞吐率）。每个分片在本地执行查询并添加结果到大小为 from + size 的本地有序优先队列中 step3：每个分片返回各自优先队列的轻量级结果列表：仅包含结果文档的 ID 和排序值_score 给协调节点，也就是 Node 3 ，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表 step4 协调节点辨别出哪些文档需要被取回(例如限制了from 90， size 10，那么最初90个会被丢弃)并向相关的分片提交多个 GET 请求（mget）。 step5 每个分片加载并 丰富 文档(加载文档的_source字段，如果有需要用highlight丰富结果文档。)，如果有需要的话，接着返回文档给协调节点。 step6 一旦所有的文档都被取回了，协调节点返回结果给客户端。 In a word, 当一个搜索请求被发送到某个节点时，这个节点就变成了协调节点。 这个节点的任务是广播查询请求到所有相关分片并将它们的响应整合成全局排序后的结果集合，这个结果集合会返回给客户端。 影响搜索过程的查询参数1. 偏好 preferencepreference parameter allows you to set some determined shards or nodes to process query by yourself. 通过设置preference为一个特定任意值，比如用户会话ID，让用一个用户始终使用同一个分片，可以解决bouncing results（场景：查询结果按照timestamp排序，如果多个分片处理，每次查询不同分片的时间戳都不同，排序结果都不同）. 2. 超时问题 timeout搜索过程的短板：搜索花费的时间由最慢的分片的处理时间决定。如果一个节点有问题，就会导致所有的响应缓慢。 参数timeout 告诉分片允许处理数据的最大时间，如果没有在足够时间处理完所有数据，返回的查询结果可以是部分的，甚至是空数据。 ... &quot;timed_out&quot;: true, ... 这个搜索请求超时了。 3. 路由 routing在搜索的时候，不用搜索索引的所有分片，而是通过指定几个 routing 值来限定只搜索几个相关的分片：1GET /_search?routing=user_1,user2 这个技术在设计大规模搜索系统时就会派上用场 4. 搜索类型 search type缺省的搜索类型是 query_then_fetch 。 在某些情况下，你可能想明确设置 search_type 为 dfs_query_then_fetch 来改善相关性精确度：1GET /_search?search_type=dfs_query_then_fetch 搜索类型 dfs_query_then_fetch 有预查询阶段，这个阶段可以从所有相关分片获取词频来计算全局词频。 High Performance 代价较小的批量操作：mget,bulk mget API 批量get多个文档 bulk API 批量多次create, index, update , delete. 单机多实例时, 配置主从分片在不同的机器; Shard不怕多，尽可能分散 避免深度分页的检索 优化映射，禁用 all 避免没有指定索引的检索和会差很多个索引的检索1整体高性能 = 每次查询的计算成本最小化 = 需要的节点尽量少 + 用到的Index尽量小","link":"/2017/04/01/es-note1/"},{"title":"Elastic Search 学习笔记-2 (搜索相关)","text":"整理自ES.cn，并加入个人理解 Search API一种是 “轻量的” 查询字符串版本 ，要求在查询字符串中传递所有的 参数。但是查询字符串参数需要URL编码 另一种是更完整的 请求体版本 ，要求使用 JSON 格式和更丰富的查询表达式作为搜索语言。（ES提供了DSL即领域特定语言） 轻量查询详见here，查询字符串版本更适合通过命令行做即席搜索。例如，查询在 tweet 类型中 tweet 字段包含 elasticsearch 单词的所有文档：1GET /_all/tweet/_search?q=tweet:elasticsearch _all查询： 这个简单搜索返回包含 mary 的所有文档(从不同的字段中)： 1GET /_search?q=mary ES是如何做到的呢？当索引一个文档的时候, ES取出所有字段的值拼接成一个大的字符串，作为 all 字段进行索引。 但是： 字符串搜索允许任何用户在索引的任意字段上执行可能较慢且重量级的查询，这可能会暴露隐私信息，甚至将集群拖垮。 因为这些原因，不推荐直接向用户暴露查询字符串搜索功能，除非对于集群和数据来说非常信任他们。 因此，在生产环境中更多地应该使用功能全面的请求体查询 request body query, API。 请求体查询请求体查询的大部分参数是通过HTTP请求传递，而非查询字符串传递。 请求体查询 —下文简称 查询—不仅可以处理自身的查询请求，还允许你对结果进行片段强调（高亮）、对所有或部分结果进行聚合分析，同时还可以给出 你是不是想找 的建议，这些建议可以引导使用者快速找到他想要的结果。 请求体查询允许我们使用DSL 特定领域语言来书写查询语句。DSL是一种灵活且有表现力的查询语言，可以使查询语句更加灵活，精确。 DSL 的形式：1234GET /_search{ &quot;query&quot;: YOUR_QUERY_HERE} YOUR_QUERY_HERE是查询语句，典型结构为：123456{ QUERY_NAME: { ARGUMENT: VALUE, ARGUMENT: VALUE,... }} 查询语句(Query clauses) 就像一些简单的组合块 ，这些组合块可以彼此之间合并组成更复杂的查询 – 复合语句。一条复合语句可以将多条语句 — 叶子语句和其它复合语句 — 合并成一个单一的查询语句。 通过使用bool查询来实现将多查询组合成单一查询的查询方法。它接收以下参数： must: 文档必须匹配这些条件才能被包含进来。 must_not: 文档 必须不 匹配这些条件才能被包含进来。 should: 如果满足这些语句中的任意语句，将增加 score ，否则，无任何影响。它们主要用于修正每个文档的相关性得分*。(全文匹配) filter:必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档。（精确匹配） e.g. 评分缓存12345678910111213下面的查询用于查找 title 字段匹配 how to make millions 并且不被标识为 spam 的文档。那些被标识为 starred 或在2014之后的文档，将比另外那些文档拥有更高的排名。如果 _两者_ 都满足，那么它排名将更高：{ &quot;bool&quot;: { &quot;must&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;how to make millions&quot; }}, &quot;must_not&quot;: { &quot;match&quot;: { &quot;tag&quot;: &quot;spam&quot; }}, &quot;should&quot;: [ { &quot;match&quot;: { &quot;tag&quot;: &quot;starred&quot; }}, { &quot;range&quot;: { &quot;date&quot;: { &quot;gte&quot;: &quot;2014-01-01&quot; }}} ] }} e.g. 不评分查询，有缓存123456789101112{ &quot;bool&quot;: { &quot;must&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;how to make millions&quot; }}, &quot;must_not&quot;: { &quot;match&quot;: { &quot;tag&quot;: &quot;spam&quot; }}, &quot;should&quot;: [ { &quot;match&quot;: { &quot;tag&quot;: &quot;starred&quot; }} ], &quot;filter&quot;: { &quot;range&quot;: { &quot;date&quot;: { &quot;gte&quot;: &quot;2014-01-01&quot; }} } }} By the way, validate-query API 可以用来验证查询是否合法, 并解释原因。12345678GET /gb/tweet/_validate/query?explain{ &quot;query&quot;: { &quot;tweet&quot; : { &quot;match&quot; : &quot;really powerful&quot; } }} ### 排序与相关性 排序计算_score的花销是巨大的，如果不使用相关性排序（例如filter操作）score为null. e.g. 通过date排序。123456789 GET /_search{ &quot;query&quot; : { &quot;bool&quot; : { &quot;filter&quot; : { &quot;term&quot; : { &quot;user_id&quot; : 1 }} } }, &quot;sort&quot;: { &quot;date&quot;: { &quot;order&quot;: &quot;desc&quot; }}} 查询结果_score : null 还可以实现多级排序。假定我们想要结合使用 date 和 score 进行查询，并且匹配的结果首先按照日期排序，然后按照相关性排序：}1234567891011121314GET /_search{ &quot;query&quot; : { &quot;bool&quot; : { &quot;must&quot;: { &quot;match&quot;: { &quot;tweet&quot;: &quot;manage text search&quot; }}, &quot;filter&quot; : { &quot;term&quot; : { &quot;user_id&quot; : 2 }} } }, &quot;sort&quot;: [ { &quot;date&quot;: { &quot;order&quot;: &quot;desc&quot; }}, { &quot;_score&quot;: { &quot;order&quot;: &quot;desc&quot; }} ]}} 字符串multifields – 排序解决方案：在映射中增加子字段field 计算相关性相关性得分计算方法： 每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来，bool查询就将 这些得分进行合并并且返回一个代表整个bool 操作的得分。 Elasticsearch 的相似度算法 被定义为检索词频率/反向文档频率， TF/IDF. TF/IDF (term frequency-inverse document frequency) 是一种用于信息检索和文本挖掘的常用加权技术。TF/IDF是一钟统计方法，用于评估一个word对于一个文件集或者一个语料库中其中一份文件的重要程度。 在ES中体现为：检索词的重要性随着它在该字段中出现的次数成正比增加，但同时会随着它在索引中出现的频率成反比下降, 并且与字段长度成反比。 relevance = tf * idf * fieldNorm 分页和 SQL 使用 LIMIT 关键字返回单个 page 结果的方法相同，Elasticsearch 接受 from 和 size 参数： size显示应该返回的结果数量，默认是 10 from显示应该跳过的初始结果数量，默认是 0 如果每页展示 5 条结果，可以用下面方式请求得到 1 到 3 页的结果：123GET /_search?size=5GET /_search?size=5&amp;from=5GET /_search?size=5&amp;from=10 ES search, something deep 12345678映射（Mapping） 描述数据在每个字段内如何存储(索引有哪些字段，字段什么数据类型，是否全文检索，用了哪个分析器)分析（Analysis） 全文是如何处理使之可以被搜索的(分词)领域特定查询语言（Query DSL） Elasticsearch 中强大灵活的查询语言 精确值，如它们听起来那样精确。例如日期或者用户 ID（在ES中 除了String类型的字段，都是精确查询的 ），但字符串也可以表示精确值，例如用户名或邮箱地址。对于精确值来讲，Foo 和 foo 是不同的，2014 和 2014-09-15 也是不同的。 精确值很容易查询。结果是二进制的：要么匹配查询，要么不匹配。这种查询很容易用 SQL 表示：123WHERE name = &quot;John Smith&quot; AND user_id = 2 AND date &gt; &quot;2014-09-15&quot; 全文 是指文本数据（通常以人类容易识别的语言书写），例如一个推文的内容或一封邮件的内容。全文通常是指非结构化数据。 查询全文数据要微妙的多。我们问的不只是“这个文档匹配查询吗”，而是“ 该文档匹配查询的程度有多大 ？”换句话说，该文档与给定查询的相关性如何 ？ 我们很少对全文类型的域做精确匹配。相反，我们希望在文本类型的域中搜索。不仅如此，我们还希望搜索能够理解我们的 意图 ：1234567&gt;搜索 UK ，会返回包含 United Kindom 的文档。&gt;搜索 jump ，会匹配 jumped ， jumps ， jumping ，甚至是 leap 。&gt;搜索 johnny walker 会匹配 Johnnie Walker ， johnnie depp 应该匹配 Johnny Depp 。&gt;fox news hunting 应该返回福克斯新闻（ Foxs News ）中关于狩猎的故事，同时， fox hunting news 应该返回关于猎狐的故事。 ES的搜索（search） 可以做到： 短语搜索，精准匹配一系列单词或者短语（match_phrase） 全文检索，找出所有匹配关键字的文档并按照相关性（relevance） 排序后返回结果。 高亮搜索功能 精确搜索VS全文搜索：索引存储的字段分为代表 精确值 的字段和代表 全文 的字段。这个区别非常重要——它将搜索引擎和所有其他数据库区别开来。 例如，ES 可以为String 类型的字段指定 映射，在映射中设置哪些字段可以analyzed，该字段就支持全文检索；映射中设置哪些字段not analyzed,就支持精确检索。 How full-text search works?keyword: 分析和倒排索引，还有映射123index flow: prepare document -&gt; analysis document to terms -&gt; build inverted index -&gt; save index to storesearch flow: prepare query string -&gt; analysis query to terms -&gt; match inverted index -&gt; return search result 为了促进全文搜索，Elasticsearch 首先 分析 文档，之后根据结果创建 倒排索引 。倒排索引 由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 倒排索引示例： 之所以要分析，是因为ES是使用Lucene进行搜索，Lucene使用Analyzer分析器进行分析，将文本转换成可索引/可搜索的tokens(tokens可以理解为将一段文本，一篇文章打碎，分出一片片易于索引的单词)进行处理，分析包含下面的过程： 首先，将一块文本分成适合于倒排索引的独立的 词条。 之后，将这些词条统一化为标准格式以提高它们的 可搜索性 分析器analyzer 执行分析工作。分词器由具体 首先，字符串按顺序通过每个 字符过滤器 char_filter。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 &amp; 转化成 and。 其次，字符串被分词器 tokenizer分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成 词条。 最后，词条按顺序通过每个 token 过滤器 filter。这个过程可能会改变词条（例如，小写化 Quick Lowercase Tokenizer），删除词条（例如， 像 a，and， the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词） 如图所示： 分词器Analyzer非常重要，因为他将document分成倒排索引(terms)，查询条件分成terms, 而直接影响了搜索命中率。 创建一个自定义过滤器： 格式1234567891011PUT /my_index{ &quot;settings&quot;: { &quot;analysis&quot;: { &quot;char_filter&quot;: { ... custom character filters ... }, &quot;tokenizer&quot;: { ... custom tokenizers ... }, &quot;filter&quot;: { ... custom token filters ... }, &quot;analyzer&quot;: { ... custom analyzers ... } } }} my_index索引创建example.12345678910111213141516171819202122PUT /my_index{ &quot;settings&quot;: { &quot;analysis&quot;: { &quot;char_filter&quot;: { &quot;&amp;_to_and&quot;: { &quot;type&quot;: &quot;mapping&quot;, &quot;mappings&quot;: [ &quot;&amp;=&gt; and &quot;] }}, &quot;filter&quot;: { &quot;my_stopwords&quot;: { &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: [ &quot;the&quot;, &quot;a&quot; ] }}, &quot;analyzer&quot;: { &quot;my_analyzer&quot;: { &quot;type&quot;: &quot;custom&quot;, &quot;char_filter&quot;: [ &quot;html_strip&quot;, &quot;&amp;_to_and&quot; ], &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;my_stopwords&quot; ] }}}}} 测试分析器12345GET /my_index/_analyze?{ &quot;analyzer&quot;:&quot;my_analyzer&quot; &quot;text&quot;:&quot;Text to analyze} 结果中每个元素代表一个单独的词条：123456789101112131415161718192021222324{ &quot;tokens&quot;: [ { &quot;token&quot;: &quot;text&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 }, { &quot;token&quot;: &quot;to&quot;, &quot;start_offset&quot;: 5, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 2 }, { &quot;token&quot;: &quot;analyze&quot;, &quot;start_offset&quot;: 8, &quot;end_offset&quot;: 15, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 3 } ] 根据全文检索和精确检索的原理，也可以理解下面的搜索结果1234GET /_search?q=2014-09-15 # 12 results !GET /_search?q=2014 # 12 resultsGET /_search?q=date:2014-09-15 # 1 resultGET /_search?q=date:2014-09-15 # 1 result 这是因为：当你查询一个 全文域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。但是当你查询一个 精确值域时，不会分析查询字符串，而是搜索你指定的精确值。 那ES内部是怎么确定哪些域（属性）是全文域，哪些是精确域呢？ ES将每个字段/域的数据类型保存在映射里。映射定义了索引的类型中有哪些字段，字段的数据类型，以及ES如何处理这些域，是属于全文域还是精确域。例如String类型的域被ES默认地设置为全文域，这样String类型的查询参数会用分析器分词。String也可以设置为not analysised, 这样就可以只进行精确匹配。因此，string 域映射的两个最重要 属性是 index 和 analyzer 。可以在index中指定是否被全文检索，在analyzer中指定分析器。 那么问题来了，映射是在什么时候进行配置的呢？– 创建索引的时候。 Important: Create and Manage your Index通过put 一个文档的方法可以创建索引，这种方式采用的是默认的配置。例如：12345678PUT /megacorp/employee/1{ &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]} 但是如果想要创建性能更高的索引，需要确保这个索引有数量适中的主分片，并且在索引任何数据之前，分析器和映射已经建立好。。 因此，需要手动创建索引。创建一个高配版的索引需要先创建好分词器和映射。 Types and mapping of an index类型在ES中表示一类相似的文档（类似sql的表）。类型由 名称 和 映射 组成。 其中，映射mapping是基于index 的，所以一个index对应了一个映射，一个映射配置了这个index的所有类型. mapping 不仅仅指定了每个字段的类型，还可以指定这个字段能不能被检索，要不要被 全文分析，如果要被全文分析的话，要使用什么 分析器。（分析器用于将全文字符串分析拆分，转换为适合全文检索的倒排索引）. 映射描述了文档可能1. 具有的字段、2.每个字段的数据类型（string,integer,date）3.以及Lucene是如何索引和存储这些字段的。 Configure a type对于String字段，两个最重要的映射参数是index和analyzer index参数控制字符串以何种方式被索引。它包含以下三个值当中的一个： 值 解释 analyzed 首先分析这个字符串，然后索引。换言之，以全文形式索引此字段。 not_analyzed 索引这个字段，使之可以被搜索，但是索引内容和指定值一样。不分析此字段。 no 不索引这个字段。这个字段不能为搜索到。 其他简单类型（long、double、date等等）也接受index参数，但相应的值只能是no和not_analyzed，它们的值不能被分析 analyzer字段指定哪一种分析器将在搜索和索引时候使用。默认ES使用standard分析器，也可以指定其他分析器。 举例：id字段被指定为精确匹配，name字段被指定为使用ngramAnalyzer分析器123456789&quot;id&quot;:{ &quot;type&quot;:&quot;keyword&quot;, &quot;index&quot;:&quot;not_analyzed&quot;},&quot;name&quot;:{ &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;:&quot;ngramAnalyzer&quot;, &quot;search_analyzer&quot;:&quot;whitespace&quot;}, root object of a mapping映射的最高一层被称为 根对象。它包含 一个properties节点，列出了document中可能包含的每个字段的映射。 各种元数据字段，都以一个下划线开头，type 、 id 和 source metadata 配置项,控制如何动态处理新的字段，例如analyzer、 dynamic_date_formats 和 dynamic_templates 其他设置，可以同时应用在根对象和其他 object 类型的字段上，例如 enabled 、dynamic 和 include_in_all 12345678910111213141516&quot;mappings&quot;: { &quot;staff&quot;:{ &quot;dynamic&quot;:&quot;strict&quot;, // 动态映射，遇到新的字段就跑出异常 &quot;_all&quot;: { &quot;analyzer&quot;: &quot;ngramAnalyzer&quot; }, &quot;properties&quot;:{ &quot;id&quot;:{ &quot;type&quot;:&quot;keyword&quot;, // type表示字段的数据类型 &quot;index&quot;:&quot;not_analyzed&quot; //index表示字段是否应当被当成全文检索analyzed 或被当成一个准确的值（ not_analyzed ），还是完全不可被搜索（ no ） }, &quot;name&quot;:{ &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;:&quot;ngramAnalyzer&quot;,// index_analyzer确定在索引和搜索时全文字段使用的分析器 &quot;search_analyzer&quot;:&quot;whitespace&quot; //对搜索条件的分析器 }, ... dynamic mapping当 Elasticsearch 遇到文档中以前 未遇到的字段，它用 dynamic mapping 来确定字段的数据类型并自动把新的字段添加到类型映射。 通过使用dynamic_templates动态模板,可以控制新检测生成字段的映射，甚至可以通过字段名称或者数据类型应用不同的映射。 模板按照顺序来检测；第一个匹配的模板会被启用。例如，我们给 string 类型字段定义两个模板： es ：以 es 结尾的字段名需要使用 spanish 分词器。en ：所有其他字段使用 english 分词器。我们将 es 模板放在第一位，因为它比匹配所有字符串字段的 en 模板更特殊：1234567891011121314151617181920212223PUT /my_index{ &quot;mappings&quot;: { &quot;my_type&quot;: { &quot;dynamic_templates&quot;: [ { &quot;es&quot;: { &quot;match&quot;: &quot;*_es&quot;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;spanish&quot; } }}, { &quot;en&quot;: { &quot;match&quot;: &quot;*&quot;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot; } }} ]}}} create an index123456789PUT /my_index{ &quot;settings&quot;: { ... any settings ... }, &quot;mappings&quot;: { &quot;type_one&quot;: { ... any mappings ... }, &quot;type_two&quot;: { ... any mappings ... }, ... }} settings 指定这个索引多少个master shard, 多少个replica；刷新的ttl，自定义的分词器。 mappings是映射,定义了该类型都有哪些字段，该字段是什么数据类型，该字段使用是否被分析–全文检索，如果被分析，那么使用什么分析器。 索引创建示例, 创建一个索引由2个主分片，每个分片有1个副本分片构成；分析器是由{自定义hypen，auto_complete过滤器，分词器ngram_tokenizer}组成的{orgFullNameAnalyzer，ngramAnalyzer，phoneAnalyzer，phoneAnalyzer}，字段有{id,name,title,picture,phone,tel, gender, jobinfo}12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394{ &quot;settings&quot;: { &quot;index&quot;: { &quot;number_of_shards&quot;:2, &quot;number_of_replicas&quot;:1, &quot;refresh_interval&quot;:&quot;1s&quot; }, &quot;analysis&quot;:{ &quot;filter&quot;:{ &quot;hypen&quot;:{ &quot;type&quot;:&quot;stop&quot;, &quot;stopwords&quot;:[&quot;-&quot;] }, &quot;auto_complete&quot;:{ &quot;type&quot;:&quot;edge_ngram&quot;, &quot;min_gram&quot;:3, &quot;max_gram&quot;:11 } }, &quot;tokenizer&quot;:{ &quot;ngram_tokenizer&quot;: { &quot;type&quot;:&quot;ngram&quot;, &quot;min_gram&quot;:3, &quot;max_gram&quot;:10, &quot;token_chars&quot;:[&quot;letter&quot;, &quot;digit&quot;] } }, &quot;analyzer&quot;: { &quot;orgFullNameAnalyzer&quot;: { &quot;type&quot;:&quot;custom&quot;, &quot;tokenizer&quot;:&quot;ik_max_word&quot;, &quot;filter&quot;:[&quot;lowercase&quot;, &quot;hypen&quot;] }, &quot;ngramAnalyzer&quot;: { &quot;type&quot;:&quot;custom&quot;, &quot;tokenizer&quot;:&quot;ngram_tokenizer&quot;, &quot;filter&quot;:&quot;lowercase&quot; }, &quot;phoneAnalyzer&quot;: { &quot;type&quot;:&quot;custom&quot;, &quot;tokenizer&quot;:&quot;standard&quot;, &quot;filter&quot;: &quot;auto_complete&quot; }, &quot;telAnalyzer&quot;: { &quot;type&quot;:&quot;custom&quot;, &quot;tokenizer&quot;:&quot;standard&quot;, &quot;filter&quot;:[&quot;auto_complete&quot;,&quot;hypen&quot;] } } } }, &quot;mappings&quot;: { &quot;staff&quot;:{ &quot;dynamic&quot;:&quot;strict&quot;, &quot;_all&quot;: { &quot;analyzer&quot;: &quot;ngramAnalyzer&quot; }, &quot;properties&quot;:{ &quot;id&quot;:{ &quot;type&quot;:&quot;keyword&quot;, &quot;index&quot;:&quot;not_analyzed&quot; }, &quot;name&quot;:{ &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;:&quot;ngramAnalyzer&quot;, &quot;search_analyzer&quot;:&quot;whitespace&quot; }, &quot;title&quot;:{ &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;:&quot;ik_max_word&quot;, &quot;search_analyzer&quot;:&quot;whitespace&quot; }, &quot;picture&quot;:{ &quot;type&quot;:&quot;keyword&quot;, &quot;index&quot;:&quot;no&quot; }, &quot;phone&quot;:{ &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;:&quot;phoneAnalyzer&quot; }, &quot;tel&quot;:{ &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;:&quot;telAnalyzer&quot; }, &quot;gender&quot;:{ &quot;type&quot;:&quot;keyword&quot;, &quot;index&quot;:&quot;no&quot; }, &quot;jobInfo&quot;:{ &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;:&quot;ik_max_word&quot; } } } }} refer to 中文分词","link":"/2017/04/08/es-note2/"},{"title":"Elastic Search学习笔记-4 (常见搜索类型实践)","text":"结构化查询 精确匹配 term 组合过滤器 bool 全文检索 match查询 组合过滤器 bool 多字段查询 best fields most fields cross fields 邻近匹配 best fields most fields cross fields 结构化查询精确值查找使用filter, 由于该类搜索有缓存，因此搜索速度相较于全文检索要更快。关于缓存：ES在运行精确查找的时候会执行多个操作：1. 查找匹配文档，term查询在倒排索引中查找满足条件的所有文档。2. 创建bitset(一个包含0和1的数组)，它描述了哪个文档包含该term. 匹配则置为1，[1,0,0,0]说明第一个文档匹配。3. 迭代bitsets. 一旦为每个查询生成了bitsets, ES会循环迭代bitsets 从而找到满足所有过滤条件的匹配文档集合。4. 增量使用技术。 ES会为每个索引跟踪保留查询使用的历史状态，如果查询在最近的256次查询中会被用到，那么它就会被缓存到内存中。当bitset被缓存后，缓存会在那些容量低于10000个文档的段中忽略。（因为这些容量小的端会消失，为他们分配缓存是一种浪费。）一旦缓存bitset, bitset可以复用任何已经使用过的相同过滤器，而无需再次计算整个过滤器。这是因为，属于查询组件的bitset是独立于它所属搜索请求其他部分的，这意味着，一旦被缓存，一个查询可以被用作多个搜索请求。精确匹配 termterm query finds documents that contain the exact term specified in the inverted index(精确匹配，term过滤器)。例外，term过滤器匹配过程是 包含 ，而非等值。如果我们有一个 term（词项）过滤器{ &quot;term&quot; : { &quot;tags&quot; : &quot;search&quot; } } ，它会与以下两个文档同时 匹配：12{ &quot;tags&quot; : [&quot;search&quot;] }{ &quot;tags&quot; : [&quot;search&quot;, &quot;open_source&quot;] }查询条件分为以下两种情况：- 非string字段 默认精确搜索- string字段 如果要使用精确搜索，需要在mapping - 字段index配置项指定“not_analyzed”示例：在倒排索引中查询user字段是否精确匹配12345678POST _search{&quot;query&quot;:{ &quot;term&quot;:{ &quot;user_id&quot;:200002 } }}A boost parameter can be specified to give this term query a higher relevance score than another query, for instance12345678910111213141516171819202122GET _search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;should&quot;: [ { &quot;term&quot;: { &quot;status&quot;: { &quot;value&quot;: &quot;urgent&quot;, &quot;boost&quot;: 2.0 } } }, { &quot;term&quot;: { &quot;status&quot;: &quot;normal&quot; } } ] } }}通常当查找一个精确值的时候，我们不希望对查询进行评分计算。只希望对文档进行包括或排除的计算，所以我们会使用 constant_score 查询以非评分模式来执行 term 查询并以一作为统一评分。term 更高效地写法：1234567891011{ &quot;query&quot; : { &quot;constant_score&quot; : { &quot;filter&quot; : { &quot;term&quot; : { &quot;category_name&quot; : &quot;服装&quot; } } } }}组合过滤器bool类似sql 的and or查询，要使用bool过滤器（复合过滤器）。格式：1234567{ &quot;bool&quot; : { &quot;must&quot; : [], &quot;should&quot; : [], &quot;must_not&quot; : [], }}1234567891011121314151617{ &quot;query&quot; : { &quot;filtered&quot; : { &quot;filter&quot; : { &quot;bool&quot; : { &quot;should&quot; : [ { &quot;term&quot; : {&quot;color_name&quot; : &quot;棕色&quot;}}, { &quot;term&quot; : {&quot;color_name&quot; : &quot;白色&quot;}} ], &quot;must_not&quot; : { &quot;term&quot; : {&quot;price&quot; : 30} } } } } }}等同于sql1where( color_name = &quot;棕色&quot; or color_name = &quot;白色&quot; ) and (price != 30)#### 嵌套布尔过滤器12345SELECT documentFROM productsWHERE productID = &quot;KDKE-B-9947-#kL5&quot; OR ( productID = &quot;JODL-X-1937-#pV7&quot; AND price = 30 )我们将其转换成一组嵌套的 bool 过滤器：1234567891011121314151617181920GET /my_store/products/_search{ &quot;query&quot; : { &quot;filtered&quot; : { &quot;filter&quot; : { &quot;bool&quot; : { &quot;should&quot; : [ { &quot;term&quot; : {&quot;productID&quot; : &quot;KDKE-B-9947-#kL5&quot;}}, { &quot;bool&quot; : { &quot;must&quot; : [ { &quot;term&quot; : {&quot;productID&quot; : &quot;JODL-X-1937-#pV7&quot;}}, { &quot;term&quot; : {&quot;price&quot; : 30}} ] }} ] } } } }}#### 查找多个精确值等价于where price = 20 or price =3012345{ &quot;terms&quot;:{ &quot;price&quot;:[20,30] }}123456789101112GET /my_store/products/_search{ &quot;query&quot; : { &quot;constant_score&quot; : { &quot;filter&quot; : { &quot;terms&quot; : { &quot;price&quot; : [20, 30] } } } }}#### 范围range 查询：查询处于某个范围的文档。可供组合的选项如下：- gt: &gt; 大于（greater than）- lt: &lt; 小于（less than）- gte: &gt;= 大于或等于（greater than or equal to）- lte: &lt;= 小于或等于（less than or equal to）123456789101112131415GET /my_store/products/_search{ &quot;query&quot; : { &quot;constant_score&quot; : { &quot;filter&quot; : { &quot;range&quot; : { &quot;price&quot; : { &quot;gte&quot; : 20, &quot;lt&quot; : 40 } } } } }}Note: 数字和日期字段的索引方式使高效地范围计算成为可能。 但字符串却并非如此，要想对其使用范围过滤，Elasticsearch 实际上是在为范围内的每个词项都执行 term 过滤器，这会比日期或数字的范围过滤慢许多。字符串范围在过滤 低基数（low cardinality） 字段（即只有少量唯一词项）时可以正常工作，但是唯一词项越多，字符串范围的计算会越慢。全文检索全文检索两个最重要的方面是：- 相关性 Relevance 它是评价查询与其结果间的相关程度，并根据这种相关程度对结果进行排名的一种能力，这种相关程度的计算方法可以是TF/IDF，地理位置临近，模糊相似，或者其他算法。- 分析 Analysis 它是将文本块拆分为有区别的、规范化的token的一个过程。目的是为了 创建倒排索引 以及 查询倒排索引match查询匹配查询match是个 核心 查询。无论需要查询什么字段，match查询都应该会是首选的查询方式。 它是一个高级全文查询，这表示它既能处理全文字段，又能处理精确字段.这就是说， match 查询主要的应用场景就是进行全文搜索. match的单词匹配12345678GET /my_index/my_type/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;QUICK!&quot; } }}Elasticsearch 执行单词匹配这个 match 查询的步骤是：1. 检查字段类型 。 标题 title 字段是一个 string 类型（ analyzed ）已分析的全文字段，这意味着查询字符串本身也应该被分析。2. 分析查询字符串 。 将查询的字符串 QUICK! 传入标准分析器中，输出的结果是单个项 quick 。因为只有一个单词项，所以 match 查询执行的是单个底层 term 查询。3. 查找匹配文档 。 用 term 查询在倒排索引中查找 quick 然后获取一组包含该项的文档，本例的结果是文档：1、2 和 3 。4. 为每个文档评分 。 用 term 查询计算每个文档相关度评分 score ，这是种将 词频（term frequency，即词 quick 在相关文档的 title 字段中出现的频率）和反向文档频率（inverse document frequency，即词 quick 在所有文档的 title 字段中出现的频率），以及字段的长度（即字段越短相关度越高）相结合的计算方式。参见 相关性的介绍 。match的多次匹配12345678GET /my_index/my_type/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;BROWN DOG!&quot; } }}因为 match 查询必须查找两个词（ [“brown”,”dog”] ），它在内部实际上先执行两次 term 查询，然后将两次查询的结果合并作为最终结果输出。为了做到这点，它将两个 term 查询包入一个 bool 查询中以上示例告诉我们一个重要信息：即任何文档只要 title 字段里包含 指定词项中的至少一个词 就能匹配，被匹配的词项越多，文档就越相关如果想提高精度，让所有词都能匹配12345678910{ &quot;query&quot;: { &quot;match&quot;: { &quot;category_name&quot;: { &quot;query&quot;: &quot;服装，食品&quot;, &quot;operator&quot;: &quot;and&quot; } } }}还可以控制精度，如果有如果用户给定 5 个查询词项，想查找只包含其中 4 个的文档，该如何处理？match 查询支持 minimum_should_match 最小匹配参数， 这让我们可以指定必须匹配的词项数用来表示一个文档是否相关。我们可以将其设置为某个具体数字，更常用的做法是将其设置为一个百分数，因为我们无法控制用户搜索时输入的单词数量：1234567891011GET /my_index/my_type/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: { &quot;query&quot;: &quot;quick brown dog&quot;, &quot;minimum_should_match&quot;: &quot;75%&quot; } } }}结构化查询类似于过滤器的写法，全文检索的bool查询也接受must, must_not, should. 所有 must 语句必须匹配，所有 must_not 语句都必须不匹配，但有多少 should 语句应该匹配呢？ 默认情况下，没有 should 语句是必须匹配的（should条件项可以不满足，但是一旦满足，评分会更高。），只有一个例外：那就是当没有 must 语句的时候，至少有一个 should 语句必须匹配。全文检索的bool query会为每个文档计算相关度评分_score,_queryscore = (all_matched_must_score + all_matched_should_score) / (must + should 语句总数)同样，可以通过 minimum_should_match 参数控制需要匹配的 should 语句的数量， 它既可以是一个绝对的数字，又可以是个百分比### 查询语句权重bool查询不仅限于组合查询单个词match查询，它可以组合任意其他的查询。通过汇总多个独立查询的分数，从而达到为每个文档微调相关度评分_score。调整评分方法：- should 查询语句匹配得越多表示文档的相关度越高。- 可以通过指定 boost来控制任何查询语句的相对的权重， boost 的默认值为 1 ，大于 1 会提升一个语句的相对权重1234567891011121314151617181920212223242526272829GET /_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: { &quot;match&quot;: { &quot;content&quot;: { &quot;query&quot;: &quot;full text search&quot;, &quot;operator&quot;: &quot;and&quot; } } }, &quot;should&quot;: [ { &quot;match&quot;: { &quot;content&quot;: { &quot;query&quot;: &quot;Elasticsearch&quot;, &quot;boost&quot;: 3 } }}, { &quot;match&quot;: { &quot;content&quot;: { &quot;query&quot;: &quot;Lucene&quot;, &quot;boost&quot;: 2 } }} ] } }}多字段查询 最简单的一种，将不同查询字符串映射到不同字段，进而写多个match查询。1234567891011GET /_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;should&quot;: [ { &quot;match&quot;: { &quot;title&quot;: &quot;War and Peace&quot; }}, { &quot;match&quot;: { &quot;author&quot;: &quot;Leo Tolstoy&quot; }} ] } }} bool查询 ： 查询条件匹配的越多越好。 因此匹配多个查询条件的文档评分要高于只匹配一个查询条件的文档。 single query string best fields searching for words that represent a concept, such as “brown fox”, the words mean more together than they do individually. most fields 最常见的bool 查询，查询条件匹配地越多越好。 通过将一个词索引到多个fields, 微调查询相关性。 cross fields 适合实体信息分散在多个field（子field）中的场景，例如Person field由first_name 和 last_name两个子field组成。我们希望在 任何 这些列出的字段中找到尽可能多的词，这有如在一个大字段中进行搜索，这个大字段包括了所有列出的字段。 每一种搜索都有自己的一套策略。 最佳字段查询 dis_max如果查询是“albino elephant”，dis_max确保匹配一个字段的“albino”和匹配另一个字段的“elephant”得到比匹配两个字段的“albino”更高的分数。 12345678910{ &quot;query&quot;: { &quot;dis_max&quot;: { &quot;queries&quot;: [ { &quot;match&quot;: { &quot;title&quot;: &quot;Brown fox&quot; }}, { &quot;match&quot;: { &quot;body&quot;: &quot;Brown fox&quot; }} ] } }} 还可以通过tie_breaker来调优，当两个文档中都不具有同时包含 两个词 的 相同字段时候，剩下的结果还是需要bool查询来排序的。 tie_breaker 参数提供了一种dis_max和bool之间的折中选择，它的评分方式如下： 获得最佳匹配语句的评分 _score 。 将其他匹配语句的评分结果与 tie_breaker 相乘。 对以上评分求和并规范化。 multi_match123456789{ &quot;multi_match&quot;: { &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;type&quot;: &quot;best_fields&quot;, &quot;fields&quot;: [ &quot;title&quot;, &quot;body&quot; ], &quot;tie_breaker&quot;: 0.3, &quot;minimum_should_match&quot;: &quot;30%&quot; }} 多数字段 most_fields 为了对相关度进行微调，常用的一个技术就是将相同的数据索引到不同的字段，它们各自具有独立的分析链。 主字段包括词源，同义词以及变音词用来匹配尽可能多的文档。 相同的文本被索引到其他字段，以提供更精确的匹配。– 其他字段作为匹配每个文档时提高相关度评分的信号，匹配字段越多则越好。 示例：如果我们有两个文档，其中一个包含词 jumped ，另一个包含词 jumping ，用户很可能期望后者能排的更高，因为它正好与输入的搜索条件（jumping rabbits）一致。 title字段多个索引1234567891011121314151617181920PUT /my_index{ &quot;settings&quot;: { &quot;number_of_shards&quot;: 1 }, &quot;mappings&quot;: { &quot;my_type&quot;: { &quot;properties&quot;: { &quot;title&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;, &quot;fields&quot;: { &quot;std&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;standard&quot; } } } } } }} most_field 微调精度 示例12345678910GET /my_index/_search{ &quot;query&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;jumping rabbits&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [ &quot;title&quot;, &quot;title.std&quot; ] } }} 用广度匹配字段 title 包括尽可能多的文档——以提升召回率——同时又使用字段 title.std 作为 信号 将相关度更高的文档置于结果顶部。 cross-fields entity searchperson 字段可能是这么索引的1234{ &quot;firstname&quot;: &quot;Peter&quot;, &quot;lastname&quot;: &quot;Smith&quot;} 与most-fields（多字段-多个查询字符串）相比，cross-fields 查询情景为单个字符串在多个字段中进行搜索。 用户可能想搜索 “Peter Smith” 这个人，这些词出现在不同的字段中，如果使用dis_max 或者best_fields查询是错误的方式。 我们想要的结果是：词 peter 和 smith 都必须出现，但是可以出现在任意字段中 只要词都出现就可以。 而most_fields是只要出现一个就行。 要用 title 和 description 字段搜索图书，可能希望为 title 分配更多的权重。使用“Peter smith”跨字段查询示例：12345678910GET /books/_search{ &quot;query&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;peter smith&quot;, &quot;type&quot;: &quot;cross_fields&quot;, &quot;fields&quot;: [ &quot;title^2&quot;, &quot;description&quot; ] } }} 比较dis_max 和cross_field dis_max是在多字段中查找，但是查询条件需要同时出现在一个字段;字段中心式cross_field在多字段中查找，但是查询条件不需要同时出现在一个字段;词中心式 参考的教程","link":"/2017/04/18/es-note4/"},{"title":"Hive Driver源码执行流程分析","text":"引言接着上一篇来说[执行入口的分析][1]，CliDriver最终将用户指令command提交给了Driver的run方法（针对常用查询语句而言），在这里用户的command将会被编译，优化并生成MapReduce任务进行执行。所以Driver也是Hive的核心，他扮演了一个将用户查询和MapReduce Task转换并执行的角色，下面我们就看看Hive是如何一步一步操作的。 源码分析在说run方法之前，由于CliDriver需要得到一个Driver类的实例，所以首先看一下Driver的构造方法。Driver有三个构造函数，主要功能也就是设置类的实例变量HiveConf。SessionState前文已经有介绍，SessionState返回了当前会话的一些信息，提取配置文件，初始化Driver实例。12345public Driver() { if (SessionState.get() != null) { conf = SessionState.get().getConf(); }} run下面就开始解析Driver内部对用户命令command的处理流程，首先是入口函数run. run函数通过调用runInternal方法处理用户指令，在处理完成runInternal之后，如果执行过程中出现出错，还附加了对错误码和错误信息的处理，此处省略。12345678910public CommandProcessorResponse run(String command) throws CommandNeedRetryException { return run(command, false);}public CommandProcessorResponse run(String command, boolean alreadyCompiled) throws CommandNeedRetryException { CommandProcessorResponse cpr = runInternal(command, alreadyCompiled); ...} runInternalrunInternal方法包含的主要操作有，处理preRunHook（具体功能可以顾名思义哦），compile ， execute， 处理postRunHook以及构造CommandProcessorResponse并返回。下面依次从代码的角度分析这几步的具体操作： PreRunHook处理preRunHook，首先根据配置文件和指令，构造用户Hook执行的上下文hookContext，然后读取用户PreRunHook配置指定的类（字符串）， 此配置项对应于Hive配置文件当中的“hive.exec.driver.run.hooks”一项，利用反射机制Class.forName实例化PreRunHook类实例（getHook函数完成），依次执行各钩子的功能（preDriverRun函数完成）。123456789101112131415161718HiveDriverRunHookContext hookContext = new HiveDriverRunHookContextImpl(conf, command); // Get all the driver run hooks and pre-execute them.List&lt;HiveDriverRunHook&gt; driverRunHooks;try{ driverRunHooks = getHooks(HiveConf.ConfVars.HIVE_DRIVER_RUN_HOOKS, HiveDriverRunHook.class); for (HiveDriverRunHook driverRunHook : driverRunHooks) { driverRunHook.preDriverRun(hookContext); }}catch (Exception e) { errorMessage = &quot;FAILED: Hive Internal Error: &quot; + Utilities.getNameMessage(e); SQLState = ErrorMsg.findSQLState(e.getMessage()); downstreamError = e; console.printError(errorMessage + &quot;\\n&quot; + org.apache.hadoop.util.StringUtils.stringifyException(e)); return createProcessorResponse(12);} compile编译，直接调用complieInternal函数编译用户指令，将指令翻译成MapReduce任务。这一个过程涉及的内容比较多，也很重要，后面将单独用一篇文章说明编译优化的过程。这里借用网上的一幅图，帮助对compile的功能有个整体的理解，参考文献: Hive实现原理.pdf。 execute在运行之前还有获取锁的操作，由于新版本添加了ACID事务的支持，还设置了事务管理器等，目前还没详细的弄懂这块的处理逻辑和功能，先放一下，主要看下execute函数执行了什么操作，也就是如何根据编译结果执行任务的。 首先是从编译得到的查询计划QueryPlan里获取基本的查询ID，查询字串等信息，并在回话状态中把当前查询字串和查询计划插入到历史记录中。12345678String queryId = plan.getQueryId();String queryStr = plan.getQueryStr();if (SessionState.get() != null) { SessionState.get().getHiveHistory().startQuery(queryStr, conf.getVar(HiveConf.ConfVars.HIVEQUERYID)); SessionState.get().getHiveHistory().logPlanProgress(plan);} 与PreRunHook类似，在执行任务之前，检查并执行用户设定的&quot;hive.pre.exec.hooks&quot;，此处不再详述。完成这部操作之后，向控制台简单的打印一些信息之后，就开始正式执行任务了。 DriverContext 创建执行上下文DriverContext，它记录的信息主要包括可执行的任务队列(Queue runnable), 正在运行的任务队列(Queue running), 当前启动的任务数curJobNo, statsTasks（Map, what used for?）以及语义分析Semantic Analyzers依赖的Context对象等。123456789101112131415161718DriverContext driverCxt = new DriverContext(ctx); driverCxt.prepare(plan);public DriverContext(Context ctx) { this.runnable = new ConcurrentLinkedQueue&lt;Task&lt;? extends Serializable&gt;&gt;(); this.running = new LinkedBlockingQueue&lt;TaskRunner&gt;(); this.ctx = ctx; }public void prepare(QueryPlan plan) { // extract stats keys from StatsTask List&lt;Task&lt;?&gt;&gt; rootTasks = plan.getRootTasks(); NodeUtils.iterateTask(rootTasks, StatsTask.class, new Function&lt;StatsTask&gt;() { public void apply(StatsTask statsTask) { statsTasks.put(statsTask.getWork().getAggKey(), statsTask); } }); } 顺便提一下Context对象，在Context的源码注释当中提到， 每一个查询都要对应一个Context对象，不同查询之间Context对象是不可重用的， 执行完一个查询之后需要clear对应的Context对象（主要是语法分析用到的temp文件目录），在Hive的实现中也是这么做的。回顾上一篇文章，从CliDriver循环的读取用户指令，每读取到一条指令都要进行processLine，processCmd，processLocalCmd的处理，然后提交给Driver编译解析。Context对象是在compile函数中实例化的，也就说每一条查询都会创建一个Context对象，当执行完一条查询从Driver返回到processLocalCmd中时，都会调用Driver对象的close函数对Context进行清理（ctx.clear），这样就保证了一条查询对应一个Context对象。对于DriverContext对象也是类似，在execute函数中实例化，Driver的close函数中关闭（driverCtx.shutdown），和Context相比一个用来辅助语义分析，一个用来辅助任务执行。还有，我们发现在processCmd函数中通过CommandProcessorFactory设置了Driver类的实例对象，也就是每一条查询都需要一个Driver对象进行处理，那这些Driver对象之间是否可以共享呢？答案是肯定的，在CommandProcessorFactory中维持了一个HiveConf到Driver的Map，每次获取Driver对象时都是根据conf对象来查找到的，如果不存在才重新创建一个Driver对象，而HiveConf对象又是在CliDriver的run方法中实例化的，与一个CliSessionState对应，所以Driver实例应该是与一个Cli的会话对应，同一个会话内部的查询共享一个Driver实例。 Manage and run all tasks 扯得有点远，继续看Driver对查询任务的执行，在实例化DriverContext对象之后，就将查询计划plan中的任务放入到DriverContext的runnable队列中。1234for (Task&lt;? extends Serializable&gt; tsk : plan.getRootTasks()) { assert tsk.getParentTasks() == null || tsk.getParentTasks().isEmpty(); driverCxt.addToRunnable(tsk);} 下面就开始运行任务Task，整个任务的运行由一个循环控制，只要DriverContext没有被关闭，并且runnable和running队列中还有任务就一直循环。为了方便描述，下文将一次对任务循环过程的每一步进行说明，这里只给出循环判断条件。12345while (!destroyed &amp;&amp; driverCxt.isRunning()) {}public synchronized boolean isRunning() { return !shutdown &amp;&amp; (!running.isEmpty() || !runnable.isEmpty());} 1. Put all the tasks into runnable queue 在循环内部，首先不停的从runnable队列中抽取队首的任务，然后launch该任务。12345678910while (!destroyed &amp;&amp; driverCxt.isRunning()) { // Launch upto maxthreads tasks Task&lt;? extends Serializable&gt; task; while ((task = driverCxt.getRunnable(maxthreads)) != null) { perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TASK + task.getName() + &quot;.&quot; + task.getId()); TaskRunner runner = launchTask(task, queryId, noName, jobname, jobs, driverCxt); if (!runner.isRunning()) { break; }} 2. Launch a task 在launch一个任务的过程中，根据任务类型（是不是MapReduceTask或者ConditialTask），做一些操作(don’t know what used for），将DriverContext当前已启动任务数curJobNo加1，然后根据配置文件conf，查询计划plan，执行上下文cxt（DriverContext），初始化一个任务，接着创建任务结果TaskResult对象和任务执行对象TaskRunner，将TaskRunner放入DriverContext的running队列中，表示该任务正在运行。最后，根据配置文件指定的任务运行模式，即是否支持并行运行，启动任务。1234567891011121314151617181920212223242526272829303132333435363738394041424344private TaskRunner launchTask(Task&lt;? extends Serializable&gt; tsk, String queryId, boolean noName, String jobname, int jobs, DriverContext cxt) throws HiveException { if (SessionState.get() != null) { SessionState.get().getHiveHistory().startTask(queryId, tsk, tsk.getClass().getName()); } if (tsk.isMapRedTask() &amp;&amp; !(tsk instanceof ConditionalTask)) { if (noName) { conf.setVar(HiveConf.ConfVars.HADOOPJOBNAME, jobname + &quot;(&quot; + tsk.getId() + &quot;)&quot;); } conf.set(&quot;mapreduce.workflow.node.name&quot;, tsk.getId()); Utilities.setWorkflowAdjacencies(conf, plan); cxt.incCurJobNo(1); console.printInfo(&quot;Launching Job &quot; + cxt.getCurJobNo() + &quot; out of &quot; + jobs); } tsk.initialize(conf, plan, cxt); TaskResult tskRes = new TaskResult(); TaskRunner tskRun = new TaskRunner(tsk, tskRes); cxt.launching(tskRun); // Launch Task if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.EXECPARALLEL) &amp;&amp; (tsk.isMapRedTask() || (tsk instanceof MoveTask))) { // Launch it in the parallel mode, as a separate thread only for MR tasks //并发执行 if (LOG.isInfoEnabled()){ LOG.info(&quot;Starting task [&quot; + tsk + &quot;] in parallel&quot;); } tskRun.setOperationLog(OperationLog.getCurrentOperationLog()); tskRun.start(); } else { if (LOG.isInfoEnabled()){ LOG.info(&quot;Starting task [&quot; + tsk + &quot;] in serial mode&quot;); } //顺序执行 tskRun.runSequential(); } return tskRun; } 3. Poll a finished task 完成任务的启动之后，将调用DriverContext的pollFinished函数，查看任务是否执行完毕，如果有任务完成，则将该任务出队，并将已完成的任务添加到钩子上下文HookContext中。 123456789101112131415161718192021TaskRunner tskRun = driverCxt.pollFinished(); if (tskRun == null) { continue;}hookContext.addCompleteTask(tskRun);public synchronized TaskRunner pollFinished() throws InterruptedException { while (!shutdown) { Iterator&lt;TaskRunner&gt; it = running.iterator(); while (it.hasNext()) { TaskRunner runner = it.next(); if (runner != null &amp;&amp; !runner.isRunning()) { it.remove(); return runner; } } wait(SLEEP_TIME); } return null; } 4. Handle the finished task 针对一个已完成的任务，首先获取任务的结果对象TaskResult和退出状态, 如果任务非正常退出，则第一步先判断任务是否支持Retry，如果支持，关闭当前DriverContext，设置jobTracker为初始状态，抛出CommandNeedRetry异常，这个异常会在CliDriver的processLocalCmd中捕获，然后尝试重新处理该命令，参见上一篇文章的说明。如果任务不支持Retry，则启动备份任务backupTask（类似于回滚？），并添加到runnable队列，在下次循环过程中执行。如果没有backupTask，则查找用户配置“hive.exec.failure.hooks”,根据用户配置相应出错处理，并关闭DriverContext， 返回退出码。12345678910111213141516171819202122232425262728293031323334353637383940414243444546Task&lt;? extends Serializable&gt; tsk = tskRun.getTask();TaskResult result = tskRun.getTaskResult();int exitVal = result.getExitVal(); if (exitVal != 0) { if (tsk.ifRetryCmdWhenFail()) { driverCxt.shutdown(); // in case we decided to run everything in local mode, restore the // the jobtracker setting to its initial value ctx.restoreOriginalTracker(); throw new CommandNeedRetryException(); } Task&lt;? extends Serializable&gt; backupTask = tsk.getAndInitBackupTask(); if (backupTask != null) { setErrorMsgAndDetail(exitVal, result.getTaskError(), tsk); console.printError(errorMessage); errorMessage = &quot;ATTEMPT: Execute BackupTask: &quot; + backupTask.getClass().getName(); console.printError(errorMessage); // add backup task to runnable if (DriverContext.isLaunchable(backupTask)) { driverCxt.addToRunnable(backupTask); } continue; } else { hookContext.setHookType(HookContext.HookType.ON_FAILURE_HOOK); // Get all the failure execution hooks and execute them. for (Hook ofh : getHooks(HiveConf.ConfVars.ONFAILUREHOOKS)) { perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.FAILURE_HOOK + ofh.getClass().getName()); ((ExecuteWithHookContext) ofh).run(hookContext); perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.FAILURE_HOOK + ofh.getClass().getName()); } setErrorMsgAndDetail(exitVal, result.getTaskError(), tsk); SQLState = &quot;08S01&quot;; console.printError(errorMessage); driverCxt.shutdown(); // in case we decided to run everything in local mode, restore the // the jobtracker setting to its initial value ctx.restoreOriginalTracker(); return exitVal; } } 5. Find children tasks 最后调用DriverContext的finished函数，对完成的任务进行处理（处理逻辑没看懂）， 然后判断当前任务是否包含子任务，如果包含则依次将子任务添加到runnable队列，下次循环中被启动执行。123456789driverCxt.finished(tskRun);if (tsk.getChildTasks() != for (Task&lt;? extends Serializable&gt; child : tsk.getChildTasks()) { if (DriverContext.isLaunchable(child)) { driverCxt.addToRunnable(child); } }} 6. Do something before return 当所有的任务都完成之后，如果发现DriverContext已经被关闭，表明任务取消，打印信息并返回对应的状态码。最后清楚任务执行中不完整的输出，并加载执行用户指定的&quot;hive.exec.post.hooks&quot;，完成对应的钩子功能。对于执行过程中出现的异常，CommandNeedRetryException将会直接向上抛出，其他Exception，直接打印出错信息。无论是否发生异常，只要能够获取到任务执行过程中的MapReduce状态信息，都将在finally语句块中打印。（限于篇幅，此处只给出部分代码，钩子的处理方式前文已经给出不再详述，异常处理的部分，有兴趣的执行查看）12345678910111213141516171819//判断DriverContext是否被关闭if (driverCxt.isShutdown()) { SQLState = &quot;HY008&quot;; errorMessage = &quot;FAILED: Operation cancelled&quot;; console.printError(errorMessage); return 1000;}//删除不完整的输出HashSet&lt;WriteEntity&gt; remOutputs = new HashSet&lt;WriteEntity&gt;(); for (WriteEntity output : plan.getOutputs()) { if (!output.isComplete()) { remOutputs.add(output); } } for (WriteEntity output : remOutputs) { plan.getOutputs().remove(output); } 最后的最后，如果所有的任务都正常执行完毕，此次查询完成，plan.setDone()，打印OK~ PostRunHook and return还没完~当execute函数执行完成后，返回到runInternal函数中，接着释放锁，与之前的PreRunHook相对应，还需要加载相应用户自定义的PostRunHook（代码不再重复），最后才调用creatProcessorResponse，创建响应对象CommandProcessorResponse并返回。123private CommandProcessorResponse createProcessorResponse(int ret) { return new CommandProcessorResponse(ret, errorMessage, SQLState, downstreamError); }","link":"/2016/05/18/hive1/"},{"title":"Netty入门综述","text":"本文是笔者自学Netty过程中总结出来的一个类似专题入门的综述文章，主要阐述以下几点关于Netty的问题: IO模型发展历程 netty基本组件 netty线程模型 希望在探讨清楚这几个问题的同时可以让自己和读者(如果有的话..)入门。本文参照了很多业界人士的深刻见解，在文中都有标注，如读者有空可以直接读原文。 I/O模型发展早期的Socket API实现网络编程Ref Server端创建一个ServerSocket, 绑定一个端口，监听listen()此端口上的连接请求 服务器使用accept（）并阻塞直到一个连接请求建立。随后返回一个新的socket用于客户端和服务端之间的通信 一系列客户端请求这个端口 启动一个新线程处理连接 读socket，得到字节流 解码协议，得到HTTP请求对象 处理HTTP请求，得到一个结果，封装成一个HttpResponse对象 编码协议，将结果序列化字节流 写Socket，将字节流发给客户端 继续循环步骤3 早期的网络编程是一种阻塞的IO模型，如果存在多个客户端的连接socket就需要创建多个线程。这种无疑会造成大量线程空闲，管理和切换上资源的浪费。 NIO - Selector多路复用模型(关于单路复用，使用国际象棋的大师车轮战这个例子更容易理解) 单路的问题是每个请求响应独占一个连接/线程，并独占连接网络读写；这样导致连接在有大量时间被闲置无法更好地利用网络资源。由于是独占读写IO，这样导致并发量高的时候就会出现性能问题著名的C10K问题。为了解决这个一个连接就要开一个线程的浪费，NIO包提供了一种Selector-非阻塞IO模型，可以在一个进程或者线程中处理多个请求。不用再一个请求开多个线程。解决C10K问题。 Selector多路复用模型使用了事件通知API以确定在一组非阻塞socket中有哪些已经就绪能够进行I/O相关操作。因为可以在任意事件检查任意的读操作or写操作的状态，所以一个单一的线程便可以处理多个并发连接。 例如如图，selector负责检查socketA-C哪些需要的数据已经准备好了，并告知thread. 假如此时socketA，socketB数据准备好了，selector把这两个socket准备已准备好的“事件”通知给thread. Thread依次去处理socketA和socketB. 这套处理机制对应着Linux操作系统的epoll和Mac的Kqueue. 这种模型的好处是使用较少的线程可以处理大量连接，减少线程切换和管理的代价。但是一旦系统的并发量特别大，使用单一的线程处理是难以满足客户端要求的。另外Java原生的NIO使用并不是很方便, 因此Netty应运而生。 Netty 简介我们先来看看Netty之前的服务器网络通信框架模型都有哪些:1.最基本的是使用阻塞I/O服务器单线程逐个处理连接请求。2.接着发展为使用多线程处理请求。一旦一个连接建立成功后，创建一个单独的线程处理其I/O操作。显然这种通信框架在高并发下线程数激增，服务器难以支撑。3.接着网络通信框架发展为使用线程池处理请求，将请求放入线程池的任务队列，避免大量的线程造成的切换代价。4.Reactor模式 - Reactor单线程模型-多线程模型-主从模型(更多关于Reactor模式Ref) 现在来讲讲Netty, Netty使用了三种Reactor线程模型。Netty提供了一个异步的事件驱动的网络通信框架，支持快速地开发可维护的高性能面向协议的服务器和客户端。 Netty是建立在NIO基础之上，在NIO之上又提供了更高层次的抽象。在Netty里面，Accept连接可以使用单独的线程池去处理，读写操作又是另外的线程池来处理。当然，Accept连接和读写操作也可以使用同一个线程池来进行处理。而请求处理逻辑既可以使用单独的线程池进行处理，也可以跟放在读写线程一块处理。线程池中的每一个线程都是NIO线程。用户可以根据实际情况进行组装，构造出满足系统需求的并发模型。 从高层次的角度来看，Netty解决了两个关注领域：技术和体系结构。首先，它的基于Java NIO的异步和事件驱动的实现，保证了高负载下应用程序性能的最大化和可伸缩性。其次，Netty也包含了一组设计模式，将应用程序逻辑从网络层解耦，简化了开发过程，同事也最大限度提高了可测试性，模块化以及代码的可重用性。 Netty提供了内置的常用编解码器，包括行编解码器［一行一个请求］，前缀长度编解码器［前N个字节定义请求的字节长度］，可重放解码器［记录半包消息的状态］，HTTP编解码器，WebSocket消息编解码器等等。Netty提供了一些列生命周期回调接口，当一个完整的请求到达时，当一个连接关闭时，当一个连接建立时，用户都会收到回调事件，然后进行逻辑处理。 Netty可以同时管理多个端口，可以使用NIO客户端模型，这些对于RPC服务是很有必要的。Netty除了可以处理TCP Socket之外，还可以处理UDP Socket。 Netty 核心组件Netty网络抽象代表：Channel(socket) + EventLoop + ChannelFutureChannel是Java NIO的一个基本构造。简单的说，可以把Channel看做是传入或者传出数据的载体socket。因此Channel可以被打开，关闭，连接或者断开连接。Channel提供的API大大的降低了直接使用socket类的复杂性。 Channel有四种状态：当Channel的状态发生变化后，就会触发对应的事件。这些事件会被转发给ChannelPipeline中的ChannelHandler，其可以随后对它们做出响应。 EventLoop代表netty控制流，多线程处理，并发。EventLoop定义了Netty核心抽象，一旦Channel注册到了EventLoop, EventLoop就处理连接的生命周期中Channel的所有IO操作。 一个EventLoopGroup包含一个或者多个EventLoop，一个EventLoop在它的生命周期内只和一个Thread绑定。所有由Eventloop处理的IO事件都将在它专有的Thread上被处理。一个Channel在它的生命周期内只注册一个EventLoop，一个EventLoop可能会被分配给多个Channel. Netty内部使用回调处理事件。Future: 因为Netty所有的IO操作都是异步的，因此当一个异步过程调用发出后，调用方不会立刻获得结果。但是调用方还是想获得这个结果怎么办？ Netty提供了ChannelFuture接口，其addListener()方法注册一个ChannelFutureListener,以便在某个操作完成时（无论是否成功）得到通知. 应用业务逻辑组件：由责任链模式组织起来的Event和ChannelHandlerNetty使用不同的事件Event通知我们状态的改变或者操作的状态，这使得我们可以基于已经发生的事件来触发适当的动作。例如IO连接的数据已经准备好，Netty会将这个时间通知工作线程。因为Netty是一个网络编程框架，因此Event是按照它们与IO数据流相关进行分类的。 事件可能包括Inbound event: 连接已经被激活/连接失活 数据读取 用户事件 错误事件 Outbound event: 打开或者关闭远程节点的连接 将数据写到或者flush到socket Netty使用了IO事件驱动模型，与NIO相比Netty允许使用者在不破坏现有处理逻辑的情况下定义自己的事件处理逻辑。这一优势要归功于Netty使用了责任链模式。总的来说：Netty定义了一个管道[ChannelPipeline]，在管道中组织了多个拦截器[ChannelHandler]处理各个事件[ChannelEvent]. 如果用户想要定义自己的事件处理逻辑，他可以自己实现一个ChannelHandler。 类的职责： ChannelPipeline: 责任链模式核心组件，ChannelHandler容器，按顺序组织各个handler,并在它们之间转发事件。 ChannelHandlerContext : 当ChannelHandler被添加到ChannelPipeline时，它将会被分配一个ChannelHandlerContext，ChannelHandlerContext代表了Pipeline和handler之间的绑定。封装一个具体的ChannelHandler,并为ChannelHanndler的执行提供一个线程环境(ChannelHanlderInvoker),可以理解为ChannelPipeline链路上的一个节点，节点里面包含有指向向前后节点的指针，事件在各个ChannelHandler之间传递，靠的就是ChannelHandlerContext ChannelHandler: 真正对IO事件作出响应和处理的地方，也是Netty暴露给业务代码的一个扩展点。一般来说，主要业务逻辑就是自定义ChannelHandler的方式实现的。 ChannelHandlerInvoker: 为ChannelHandler提供一个运行的线程环境，默认的实现DefaultChannelHandlerInvoker有一个EventExecutor类型的成员，就是Netty的EventLoop线程，所以默认ChannelHandler的处理逻辑在EventLoop线程内。当然也可以提供不同的实现，替换默认的线程模型 关于ChannelPipeline，只需要了解DefaultChannelPipeline这个默认的实现类，这个类其实就是一个链表管理类，管理者每一个ChannelHandlerContext类型的节点，从它的addFirst、addLast、remove等成员方法就可以看出来. ChannelHandler是一个顶级接口，有两个子接口ChannelInboundHandler和ChannelOutboundHandler分别处理read和write相关的IO事件，为了便于业务方实现，两个子接口分别有一个简单的Adapter实现类，所有方法的默认实现都是代理给ChannelHandlerContext类（其实是不关心事件，直接转发给pipeline中下一个节点的handler来处理）。业务方实现自己的ChannelHandler时，推荐继承相应的Adapter类，只实现自己关心的事件的处理方法 编码器和解码器网络数据是一系列的字节，当通过Netty发送或者接收一个消息的时候，就会发生一次数据转换。inbound msg会被解码，即从字节码转换为另一种格式，通常是Java对象。如果是outbound msg，则会发生相反方向的转换：从当前格式被编码为字节。使用Netty可以定制编解码协议，实现自己的特定协议的服务器。如果你实现的编解码协议是HTTP协议，那么你实现的就是HTTP服务器；如果你实现的协议是redis协议，那么你实现的就是Redis服务器。 引导类Netty的引导类为应用程序的网络层配置提供了容器，这涉及到两种类型的引导：服务端引导ServerBootstrap 服务端引导就是个引导类，封装了服务端启动过程，包括端口绑定和监听等过程. 客户端引导BootStrap：客户端引导类就是封装了客户端启动的过程，只要是创建socket，并发起connect调用，建立一个到服务端的链接的过程。 BootStrap和ServerBootstrap的区别除了网络编程中的作用之外，还有一个区别也很明显：BootStrap只需要一个EventLoopGroup，引导服务端的ServerBootstrap则需要两个EventLoopGroup。这是因为服务器需要两组不同的Channel,第一组将只包含一个ServerChannel，代表服务器自身的已绑定到某个本地端口的正在监听的套接字。而第二组将包含所有已创建的用来传递客户端的连接的channel. Netty线程模型Ref线程模型指定了操作系统，编程语言，框架或者应用程序的上下文中的线程管理的关键方面，它确定了代码的执行方式。 Java5引入了线程池Executor API,线程池通过利用缓存和重用线程极大地提高了性能。基本的线程池模式可以描述为： 从线程池空闲列表选择一个Thread,并且指派它去运行一个已提交的任务（一个Runnable的实现） 当任务完成时，将Thread返回给列表，使其可重用。虽然线程池化和重用相对于简单地为每个任务创建和销毁线程是一种进步，但是它并不能消除由上下文切换所带来的开销，这种开销将随着线程数量的增加很快变得明显，并且在高负载下愈演愈烈。另外，线程安全性也是开发者必须要解决的问题。 Netty线程模型：通过NioEventLoop类的设计可以看出Netty线程模型设计原理。我们知道Netty使用EventLoop来运行任务，处理所有IO连接。一个EventLoop将由一个永远都不会改变的Thread驱动，同时任务(Runnable/Callable)提交给EventLoop实现。为了优化线程池多线程切换的开销和线程安全问题，Netty采用了串行化设计理念。从消息的读取，编码以及后续handler的执行，始终都由IO线程NioEventLoop负责，这就意味着整个流程不会进行线程上下文的切换，数据也不会面临被并发修改的风险。对用户而言，甚至不需要了解Netty的线程细节，这确实是个非常好的设计理念。一个NioEventLoop聚合了一个多路复用Selector，因此可以处理成百上千的客户端连接，Netty的处理策略是每当有一个新的客户端接入，则从NioEventLoop线程组中顺序获取一个可用的NioEventLoop,当到达顺组上限之后，重新返回到0，通过这种方式，可以基本保证NioEventLoop的负载均衡。一个客户端连接只注册到一个NioEventLoop上，这样就避免多个IO线程去并发操作它。 Netty通过串行化设计理念降低了用户的开发难度，提升了处理性能。这种无锁化设计，避免了多线程成竞争导致的性能下降问题，利用线程组实现了多个串行化线程水平并行执行，线程之间并没有交集，这样既可以充分利用多核提升并行处理能力，同时避免了线程上下文的切换和并发保护带来的额外性能损耗。 Netty线程模型分类Netty的线程模型并不是一成不变的，它实际取决于用户的启动配置参数，通过设置不同的启动参数，Netty可以同时支持Reactor单线程模型，多线程模型和主从Reactor多线程模型。 服务端线程模型 - Reactor多线程模型的实现服务端启动的时候，创建两个NioEventloopGroup，它们实际上是两个独立的Reactor线程池： 用于接收客户端的TCP连接，该线程池职责如下： 接收客户端TCP连接，初始化Channel参数 将链路状态变更时间通知给ChannelPipeline. 另一个用于处理I/O相关的读写操作，或者执行系统Task,定时任务等。 异步读取通信对端的数据报，发送读事件到ChannelPipeline 异步发送消息到通信对端，调用ChannelPipeline的消息发送接口 执行系统调用Task 执行定时任务Task,例如链路空闲状态监测任务123456789EventloopGroup bossGroup = new NioEventLoopGroup(1);EventloopGroup workerGroup = new NioEventLoopGroup(2);EventloopGroup group = new NioEventLoopGroup();try{ServerBootStrap b = new ServerBootStrap();b.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class)...} 客户端线程模型 EventLoop接口EventLoop接口通过运行任务去处理连接生命周期发生的事件。在这个模型中，一个EventLoop将由一个永远都不会改变的Thread驱动，同时任务（Runnable或者Callable）可以直接提交给EventLoop实现，以立刻执行或者调度执行。NioEventLoop是Netty的Reactor线程，它的职责如下： 作为服务端Acceptor线程，负责处理客户端的请求接入; 作为客户端connector线程，负责注册监听连接请求，判断异步连接结果； 作为IO线程，监听网络读操作，负责从socketchannel读取报文； 作为IO线程，负责向SocketChannel写入报文发送给对方 作为定时任务线程，可以执行定时任务，例如链路空闲监测和发送心跳消息等； 作为线程执行器可以执行普通的任务线程 参考 Netty线程模型,李林峰 http://docs.jboss.org/netty/3.1/guide/html/architecture.html#d0e1996 责任链模式 Netty系列之Netty高性能之道,李林峰 [通俗的讲，Netty能做什么？,知乎](https://www.zhihu.com/question/24322387 [Netty4 学习笔记, zxh0](http://blog.csdn.net/zxhoo/article/details/17264263） 建议阅读：为什么建议Netty的I/O线程与业务线程分离 Netty InfoQ 迷你书,李林峰","link":"/2018/02/13/netty-summarize/"},{"title":"PHP哈希表内核实现","text":"毫无疑问，Hash表是php语言最重要的数据结构。话不多说，看一下php7中HashTable的定义： HashTable 结构定义12345678910111213141516171819202122typedef struct _zend_array HashTable;struct _zend_array { zend_refcounted_h gc; ／* gc引用计数 *／ union { struct { ZEND_ENDIAN_LOHI_4( zend_uchar flags, zend_uchar _unused, zend_uchar nIteratorsCount, zend_uchar consistency) } v; uint32_t flags; /* 哈希表的标记信息，包括类型标志，迭代器计数等 */ } u; uint32_t nTableMask; /* 散列表掩码 */ Bucket * arData; /* Bucket数组首指针 */ uint32_t nNumUsed; /* 已用Bucket计数 */ uint32_t nNumOfElements; /* 有效元素计数，nNumOfElements = nNumUsed - UNDEF(Bucket.val) */ uint32_t nTableSize; /* 哈希表总大小 */ uint32_t nInternalPointer; /* 内部指针，提供给迭代器使用，默认用于标注第一个有效的元素在Bucket数组中的位置 */ zend_long nNextFreeElement; /* 下一个可用的数值索引,如:arr[] = 1;arr[\"a\"] = 2;nNextFreeElement = 1; */ dtor_func_t pDestructor; /* 无效Bucket value的销毁函数指针 */}; 其中arData指向了哈希表中元素的实际存储位置，其他字段用于计数，定位hash key，执行迭代等辅助操作。 Bucket的结构也比较简洁，包含三个信息，元素的key， value，以及相应的hash值： 12345typedef struct _Bucket { zval val; /* HashTable value */ zend_ulong h; /* hash value (or numeric index) for key */ zend_string *key; /* string key or NULL for numerics */} Bucket; 如下图所示，与PHP5不同的是，HashTable的数据结构发生的巨大的变化，取消了原有按key映射双向链表的模式，而采用一维数组统一管理。 好处在于： 加速寻值定位，直接利用hash值计算得到索引，在一维数组上通过指针运算定位元素，效率更高。 数组的结构形式能够很好的利用cpu cache的性能，减少cpu cache命中失败的次数。(延伸阅读：cpu cache line ) 简化Bucket的管理，不需要双向链表维持插入顺序，利用数组的天然优势，减少了Bucket结构的冗余信息，指针内存以及分配操作。 既然arData并不是按key映射的散列表，那么映射函数是如何将key与arData中的value建立映射关系的呢？实际上这个散列表也在arData中，比较特别的是散列表在ht-&gt;arData内存之前，分配内存时这个散列表(散列表)与Bucket数组一起分配。arData向后移动到了Bucket数组的起始位置，并不是申请内存的起始位置，这样散列表可以由arData指针向前移动访问到，即arData[-1]、arData[-2]、arData[-3]……散列表的结构是uint32_t，它保存的是value在Bucket数组中的位置。 源码分析先看一段简单的PHP代码：12345$arr[\"a\"] = 1;$arr[\"b\"] = 2;$arr[\"c\"] = 3;$arr[\"d\"] = 4;unset($arr[\"c\"]); 在PHP7的内核实现中，其对应的表示如下： 有了整体的数据结构支持，我们详细的来看一下，常见哈希表操作(包括外部接口与内部调整)的具体实现方式。PHP7的哈希表实现针对不同的操作场景提供了不同的操作接口，比如数值key插入，字符串key插入等，根据语法分析过程生产的opcode和操作数的不同，在zend_vm_execute的入口函数中会调用不同的哈希表接口处理。 本文只选择字符串类型插入的过程来分析，帮助加深对哈希表实现的理解，对于需要特别区分的地方，会单独描述。 初始化：void zend_hash_real_init_ex(HashTable *ht, int packed)1234567891011121314151617181920212223static zend_always_inline void zend_hash_real_init_ex(HashTable *ht, int packed){ HT_ASSERT_RC1(ht); ZEND_ASSERT(!((ht)-&gt;u.flags &amp; HASH_FLAG_INITIALIZED)); if (packed) { /*压缩数组，针对hash key为完美连续自然数的场景优化，参考https://phpinternals.net/docs/hash_flag_packed*/ HT_SET_DATA_ADDR(ht, pemalloc(HT_SIZE(ht), GC_FLAGS(ht) &amp; IS_ARRAY_PERSISTENT)); /*分配Bucket数组及散列表*/ (ht)-&gt;u.flags |= HASH_FLAG_INITIALIZED | HASH_FLAG_PACKED; HT_HASH_RESET_PACKED(ht); ／*压缩数组直接使用整数key作为hash索引，进一步减少了定位开销，所以也就不要前置的散列表*／ } else { (ht)-&gt;nTableMask = -(ht)-&gt;nTableSize; HT_SET_DATA_ADDR(ht, pemalloc(HT_SIZE(ht), GC_FLAGS(ht) &amp; IS_ARRAY_PERSISTENT)); (ht)-&gt;u.flags |= HASH_FLAG_INITIALIZED; if (EXPECTED(ht-&gt;nTableMask == (uint32_t)-8)) { /*初始化散列表*/ Bucket *arData = ht-&gt;arData; HT_HASH_EX(arData, -8) = -1; HT_HASH_EX(arData, -7) = -1; ... HT_HASH_EX(arData, -1) = -1; } else { HT_HASH_RESET(ht); } }} 初始化的操作主要完成了Bucket首地址arData的设置与散列表的初始赋值。取Bucket头指针的宏定义，可见分配内存时，申请了物理上连续，逻辑上分开的两段内存，即散列表与Bucket数组。 123456#define HT_SET_DATA_ADDR(ht, ptr) do { (ht)-&gt;arData = (Bucket*)(((char*)(ptr)) + HT_HASH_SIZE((ht)-&gt;nTableMask));} while (0)#define HT_HASH_SIZE(nTableMask) \\ 取nTableMask的负数 (((size_t)(uint32_t)-(int32_t)(nTableMask)) * sizeof(uint32_t)) 添加元素: zval *_zend_hash_add_or_update_i(HashTable *ht, zend_string *key, zval *pData, uint32_t flag ZEND_FILE_LINE_DC)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980static zend_always_inline zval *_zend_hash_add_or_update_i(HashTable *ht, zend_string *key, zval *pData, uint32_t flag ZEND_FILE_LINE_DC){ zend_ulong h; uint32_t nIndex; uint32_t idx; Bucket *p; IS_CONSISTENT(ht); HT_ASSERT_RC1(ht); if (UNEXPECTED(!(ht-&gt;u.flags &amp; HASH_FLAG_INITIALIZED))) { CHECK_INIT(ht, 0); if (!ZSTR_IS_INTERNED(key)) { zend_string_addref(key); //为key添加引用计数 ht-&gt;u.flags \\&amp;= ~HASH_FLAG_STATIC_KEYS; //非内部key zend_string_hash_val(key); //计算key的hash值 time 33算法 } goto add_to_hash; } else if (ht-&gt;u.flags &amp; HASH_FLAG_PACKED) { zend_hash_packed_to_hash(ht); //转换为非自然序的形式，重构散列表 if (!ZSTR_IS_INTERNED(key)) { zend_string_addref(key); ht-&gt;u.flags \\&amp;= ~HASH_FLAG_STATIC_KEYS; zend_string_hash_val(key); } } else if ((flag &amp; HASH_ADD_NEW) == 0) { //添加新元素 p = zend_hash_find_bucket(ht, key, 0); //按key寻找 if (p) { //如果找到了数据，就detroy掉老数据，然后用传入的data覆盖掉旧数据 zval *data; if (flag &amp; HASH_ADD) { if (!(flag &amp; HASH_UPDATE_INDIRECT)) { return NULL; } ZEND_ASSERT(&amp;p-&gt;val != pData); data = &amp;p-&gt;val; if (Z_TYPE_P(data) == IS_INDIRECT) { data = Z_INDIRECT_P(data); if (Z_TYPE_P(data) != IS_UNDEF) { return NULL; } } else { return NULL; } } else { ZEND_ASSERT(&amp;p-&gt;val != pData); data = &amp;p-&gt;val; if ((flag &amp; HASH_UPDATE_INDIRECT) &amp;&amp; Z_TYPE_P(data) == IS_INDIRECT) { data = Z_INDIRECT_P(data); } } if (ht-&gt;pDestructor) { ht-&gt;pDestructor(data); } ZVAL_COPY_VALUE(data, pData); return data; } if (!ZSTR_IS_INTERNED(key)) { zend_string_addref(key); ht-&gt;u.flags &amp;= ~HASH_FLAG_STATIC_KEYS; } } else if (!ZSTR_IS_INTERNED(key)) { zend_string_addref(key); ht-&gt;u.flags &amp;= ~HASH_FLAG_STATIC_KEYS; zend_string_hash_val(key); } ZEND_HASH_IF_FULL_DO_RESIZE(ht); /* If the Hash table is full, resize it */add_to_hash: idx = ht-&gt;nNumUsed++; //获取存储位置，idx ht-&gt;nNumOfElements++; //有效计数+1 if (ht-&gt;nInternalPointer == HT_INVALID_IDX) { ht-&gt;nInternalPointer = idx; 首次插入，更新nInternalPointer } zend_hash_iterators_update(ht, HT_INVALID_IDX, idx); p = ht-&gt;arData + idx; p-&gt;key = key; p-&gt;h = h = ZSTR_H(key); ZVAL_COPY_VALUE(&amp;p-&gt;val, pData); //value赋值 nIndex = h | ht-&gt;nTableMask; //计算idx在散列表的位置 Z_NEXT(p-&gt;val) = HT_HASH(ht, nIndex); // 拉链，指向当前散列表上标记的元素， #define Z_NEXT(zval) (zval).u2.next HT_HASH(ht, nIndex) = HT_IDX_TO_HASH(idx); //更新散列表元素值 return &amp;p-&gt;val;} 针对插入过程，除去一些初始化检查，压缩数组类型变化及是否需要扩容check操作，比较核心的是goto标记add_to_hash位置开始的内容：首先按照插入顺序，放入Buckdet数组下一个可以位置，哈希表计数更新。以此位置为索引，填充传入的value。当发生碰撞，采用头插法，(zval).u2.next记录下一个相同散列值元素的位置。最后更新散列表中的索引值。 扩容resize: zend_hash_do_resize(HashTable *ht)123456789101112131415161718192021static void ZEND_FASTCALL zend_hash_do_resize(HashTable *ht){ IS_CONSISTENT(ht); HT_ASSERT_RC1(ht); if (ht-&gt;nNumUsed &gt; ht-&gt;nNumOfElements + (ht-&gt;nNumOfElements &gt;&gt; 5)) { /* additional term is there to amortize the cost of compaction */ zend_hash_rehash(ht); } else if (ht-&gt;nTableSize &lt; HT_MAX_SIZE) { /* Let's double the table size */ void *new_data, *old_data = HT_GET_DATA_ADDR(ht); //获取索引数组的头指针 uint32_t nSize = ht-&gt;nTableSize + ht-&gt;nTableSize; //计算新表容量 Bucket *old_buckets = ht-&gt;arData; new_data = pemalloc(HT_SIZE_EX(nSize, -nSize), GC_FLAGS(ht) &amp; IS_ARRAY_PERSISTENT); ht-&gt;nTableSize = nSize; ht-&gt;nTableMask = -ht-&gt;nTableSize; HT_SET_DATA_ADDR(ht, new_data); //重置Bucket首指针 memcpy(ht-&gt;arData, old_buckets, sizeof(Bucket) * ht-&gt;nNumUsed); //由于Bucket数组按照插入顺序放置元素，直接拷贝旧数据到新表 pefree(old_data, GC_FLAGS(ht) &amp; IS_ARRAY_PERSISTENT); //释放旧数据 zend_hash_rehash(ht); //rehash，重新计算索引 } else { zend_error_noreturn(E_ERROR, \"Possible integer overflow in memory allocation (%u * %zu + %zu)\", ht-&gt;nTableSize * 2, sizeof(Bucket) + sizeof(uint32_t), sizeof(Bucket)); }} 比较简单，double size =》alloc memory =》rehash rehash: zend_hash_rehash(HashTable *ht）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778ZEND_API int ZEND_FASTCALL zend_hash_rehash(HashTable *ht){ Bucket *p; uint32_t nIndex, i; IS_CONSISTENT(ht); if (UNEXPECTED(ht-&gt;nNumOfElements == 0)) { if (ht-&gt;u.flags &amp; HASH_FLAG_INITIALIZED) { ht-&gt;nNumUsed = 0; HT_HASH_RESET(ht); } return SUCCESS; } HT_HASH_RESET(ht); //重置所有索引数组值为-1 i = 0; p = ht-&gt;arData; if (HT_IS_WITHOUT_HOLES(ht)) { //没有UNDEF的value，遍历Bucket数组，计算索引值，并填充索引数组 do { nIndex = p-&gt;h | ht-&gt;nTableMask; Z_NEXT(p-&gt;val) = HT_HASH(ht, nIndex); HT_HASH(ht, nIndex) = HT_IDX_TO_HASH(i); p++; } while (++i &lt; ht-&gt;nNumUsed); } else { do { if (UNEXPECTED(Z_TYPE(p-&gt;val) == IS_UNDEF)) { uint32_t j = i; Bucket *q = p; if (EXPECTED(ht-&gt;u.v.nIteratorsCount == 0)) { //没有迭代器在使用 while (++i &lt; ht-&gt;nNumUsed) { p++; if (EXPECTED(Z_TYPE_INFO(p-&gt;val) != IS_UNDEF)) { //有效的数据重新拷贝到前一个位置，并重置索引，无效的跳过，被后面的有效值覆盖掉 ZVAL_COPY_VALUE(&amp;q-&gt;val, &amp;p-&gt;val); q-&gt;h = p-&gt;h; nIndex = q-&gt;h | ht-&gt;nTableMask; q-&gt;key = p-&gt;key; Z_NEXT(q-&gt;val) = HT_HASH(ht, nIndex); HT_HASH(ht, nIndex) = HT_IDX_TO_HASH(j); if (UNEXPECTED(ht-&gt;nInternalPointer == i)) { ht-&gt;nInternalPointer = j; } q++; j++; } } } else { uint32_t iter_pos = zend_hash_iterators_lower_pos(ht, 0); while (++i &lt; ht-&gt;nNumUsed) { p++; if (EXPECTED(Z_TYPE_INFO(p-&gt;val) != IS_UNDEF)) { ZVAL_COPY_VALUE(&amp;q-&gt;val, &amp;p-&gt;val); q-&gt;h = p-&gt;h; nIndex = q-&gt;h | ht-&gt;nTableMask; q-&gt;key = p-&gt;key; Z_NEXT(q-&gt;val) = HT_HASH(ht, nIndex); HT_HASH(ht, nIndex) = HT_IDX_TO_HASH(j); if (UNEXPECTED(ht-&gt;nInternalPointer == i)) { ht-&gt;nInternalPointer = j; } if (UNEXPECTED(i == iter_pos)) { //更新迭代器的信息 zend_hash_iterators_update(ht, i, j); iter_pos = zend_hash_iterators_lower_pos(ht, iter_pos + 1); } q++; j++; } } } ht-&gt;nNumUsed = j; break; } nIndex = p-&gt;h | ht-&gt;nTableMask; Z_NEXT(p-&gt;val) = HT_HASH(ht, nIndex); HT_HASH(ht, nIndex) = HT_IDX_TO_HASH(i); p++; } while (++i &lt; ht-&gt;nNumUsed); } return SUCCESS;} rehash首先重置散列表到初始状态，如果当前哈希表内没有被标记为UNDEF的value，那么从Bucket数组的首地址开始遍历，重新设置散列表，并执行头插。如果哈希表里有标记为UNDEF的value，则在遍历的过程中，忽略掉UNDEF的value，并使用下一个有效的value，覆盖当前UNDEF的value，同时完成散列表的更新和碰撞拉链。如果当前哈希表有关联的迭代器，还需要把迭代器里的position更新为新的索引值。 查找的入口：Bucket *zend_hash_find_bucket(const HashTable *ht, zend_string *key, zend_bool known_hash)123456789101112131415161718192021222324252627static zend_always_inline Bucket *zend_hash_find_bucket(const HashTable *ht, zend_string *key, zend_bool known_hash){ zend_ulong h; uint32_t nIndex; uint32_t idx; Bucket *p, *arData; if (known_hash) { //hash值是否已经保存在string的字段中 h = ZSTR_H(key); } else { h = zend_string_hash_val(key); } arData = ht-&gt;arData; nIndex = h | ht-&gt;nTableMask; idx = HT_HASH_EX(arData, nIndex); //计算Bucket数组中的idx while (EXPECTED(idx != HT_INVALID_IDX)) { p = HT_HASH_TO_BUCKET_EX(arData, idx); if (EXPECTED(p-&gt;key == key)) { /* check for the same interned string */ return p; //key的内存地址一样 } else if (EXPECTED(p-&gt;h == h) &amp;&amp; EXPECTED(p-&gt;key) &amp;&amp; EXPECTED(zend_string_equal_content(p-&gt;key, key))) {//key的内容一样 return p; } idx = Z_NEXT(p-&gt;val); } return NULL;} 哈希表的查找过程也比较简单：locate bucket =》search for collision chain =》compare key and return value 删除的入口： zend_hash_del(HashTable *ht, zend_string *key)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071ZEND_API int ZEND_FASTCALL zend_hash_del(HashTable *ht, zend_string *key){ zend_ulong h; uint32_t nIndex; uint32_t idx; Bucket *p; Bucket *prev = NULL; IS_CONSISTENT(ht); HT_ASSERT_RC1(ht); h = zend_string_hash_val(key); nIndex = h | ht-&gt;nTableMask; idx = HT_HASH(ht, nIndex); while (idx != HT_INVALID_IDX) { p = HT_HASH_TO_BUCKET(ht, idx); if ((p-&gt;key == key) || (p-&gt;h == h &amp;&amp; p-&gt;key &amp;&amp; zend_string_equal_content(p-&gt;key, key))) { //找到当前元素，并标记前驱节点 _zend_hash_del_el_ex(ht, idx, p, prev); return SUCCESS; } prev = p; idx = Z_NEXT(p-&gt;val); } return FAILURE;}static zend_always_inline void _zend_hash_del_el_ex(HashTable *ht, uint32_t idx, Bucket *p, Bucket *prev){ if (!(ht-&gt;u.flags &amp; HASH_FLAG_PACKED)) { //非压缩数组 if (prev) { //有前驱节点，前驱节点的后继指向当前节点的后继 Z_NEXT(prev-&gt;val) = Z_NEXT(p-&gt;val); } else { //否则，更新散列表中的值为后继节点的idx HT_HASH(ht, p-&gt;h | ht-&gt;nTableMask) = Z_NEXT(p-&gt;val); } } if (HT_IDX_TO_HASH(ht-&gt;nNumUsed - 1) == idx) { //如果删除的是Bucket数组中最后一个填充的元素，循环往前忽略掉UNDEF的Bucket do { ht-&gt;nNumUsed--; } while (ht-&gt;nNumUsed &gt; 0 &amp;&amp; (UNEXPECTED(Z_TYPE(ht-&gt;arData[ht-&gt;nNumUsed-1].val) == IS_UNDEF))); } ht-&gt;nNumOfElements--; if (HT_IDX_TO_HASH(ht-&gt;nInternalPointer) == idx || UNEXPECTED(ht-&gt;u.v.nIteratorsCount)) { uint32_t new_idx; //删除的是nInternalPointer位置的元素 new_idx = idx = HT_HASH_TO_IDX(idx); while (1) { new_idx++; if (new_idx &gt;= ht-&gt;nNumUsed) { new_idx = HT_INVALID_IDX; break; } else if (Z_TYPE(ht-&gt;arData[new_idx].val) != IS_UNDEF) { //找到第一个非UNDEF的元素为foreach的起点 break; } } if (ht-&gt;nInternalPointer == idx) { ht-&gt;nInternalPointer = new_idx; } zend_hash_iterators_update(ht, idx, new_idx); //更新迭代器的位置 } if (p-&gt;key) { zend_string_release(p-&gt;key); //释放key } if (ht-&gt;pDestructor) { //当前的value设为undef，如果Destructor不为空，destory掉老的value zval tmp; ZVAL_COPY_VALUE(&amp;tmp, &amp;p-&gt;val); ZVAL_UNDEF(&amp;p-&gt;val); ht-&gt;pDestructor(&amp;tmp); } else { ZVAL_UNDEF(&amp;p-&gt;val); }} 在执行元素的删除时，首先按照hash find的方式，查找的目标key所在的Bucket，因为涉及到collision chain的调整，所以还需要标记一下当前节点的前驱节点。真正在执行元素删除时，如果有前驱节点，则把前驱节点的后继指向下一个节点，否则直接更新散列表的值为后继节点的位置。如果发现删除的元素是Bucket数组中的最后一个元素，此时会在Bucket数组中回溯，忽略掉UNDEF的value。相当于执行一次已删除元素的清理。如果删除的是nInterPointer位置的元素，还需要更新一下这个值，指向第一个非UNDEF的元素位置，为foreach的起点，保证foreach的正确性。最后如果设置的destructor则执行对于的销毁操作，并把当前的value置为UNDEF。","link":"/2018/03/21/php-hashtable/"},{"title":"Redis AOF 持久化- Redis源码分析","text":"在《Redis RDB持久化》一文中，我们对RDB持久化的流程，格式以及实现方式进行了阐述。本文重点关注下另外一种持久化方式：AOF持久化。 初始AOF与RDB将整个内存快照写入文件的方式不同，AOF以协议文本的方式，将所有对数据库进行过写入的命令（及其参数）记录到 AOF 文件，以此达到记录数据库状态的目的。 12345reids&gt; RPUSH list 1 2 3 4redis&gt; LRANGE list 0 -1redis&gt; RPOP listredis&gt; LPUSH list 1redis&gt; LRANGE list 0 -1 执行上述命令后生成的快照文件如下： 123456789101112131415161718192021222324252627282930*2$6SELECT$10*6$5RPUSH$4list$11$12$13$14*2$4RPOP$4list*3$5LPUSH$4list$11 AOF命令协议由于在我们执行的指令中，有4条对数据库执行内容变更的操作，最终写入aof文件的也只有4条。 AOF存储一条命令的格式如下： 123456*&lt;count&gt; /*接下来的命令有count个参数*/$&lt;len&gt; /*第一个参数的长度*/&lt;content&gt; /*第一个参数的内容*/$&lt;len&gt; /*第二个参数的长度*/&lt;content&gt; /*第二个参数的长度*/... 完全符合示例中生成的aof文件内容。需要指出的就是第一条SELECT命令是Redis自己加上的，为了保证两条相邻的指令操作了不同的内部数据库时，能够正确的区分。 AOF实现原理简单的来说，AOF的实现原理就是基于配置项”appendonly yes/no”来控制是否将更新命令写入appendonly文件，如果设置为yes，则每次数据更新的命令都会被追加到内存缓存server.aof_buf里，然后根据配置项appendfsync，决定何时将aof_buf刷盘。下面就看看此流程中的实现细节。 Append aof_buf追加aof_buf的入口函数在feedAppendOnlyFile中，具体的执行流程如下图： 如果当前更新操作和上一次aof记录操作的数据库不一致，则自动生成一个SELECT命令，控制选择正确的数据库 如果当前操作指令中包含expire信息，如setex，expire等，需要特殊处理把设置到期时间的功能统一使用PEXPIRE命令记录 按照AOF命令协议的格式，拼装当前操作的核心指令 如果系统开启了写AOF的配置，则将拼装的命令写入buf；如果有在运行的rewrite子进程（参见下一小节的AOF重写），为了记录rewrite期间的数据差异，还需要将指令追加到aof_rewrite_buf中。 Flush aof_buf刷盘流程 刷盘的流程整体上分为write()将aof_buf写入系统缓存以及fsync写磁盘两个步骤。首先，在设置了每秒flush一次aof_buf配置情况下，如果后台有正在运行的fsync任务，并且距离上次write等待没超过2秒，直接返回，等待上一次延迟的任务执行完成。否则，调用aofWrite循环write()。由于无法保证write()一定成功，所以当写入不完全时，会执行特定的出错处理机制。如果完全写入，表示aof_buf的内容已写入系统缓存，此时增加aof_current_size的计数。到此为止，就完成了通过write调用将aof_buf写入系统缓存的工作。一旦系统缓存写入成功，即使Redis程序崩溃或者退出，只要系统正常运行，那么aof_buf也一定能刷入磁盘中。 如果aof文件fsync的策略配置为AOF_FSYNC_ALWAYS，则直接在当前主进程中执行一次fsync，如果fsync的策略配置为AOF_FSYNC_EVERYSEC，并且后台没有正在执行的fsync任务，则为aof文件的fd创建一个新的fsync任务在后台异步执行。 AOF_FSYNC_ALWAYS和AOF_FSYNC_EVERYSEC两种刷盘策略是在安全性和效率之前的不同取舍方案。 AOF_FSYNC_ALWAYS能保证每次执行的命令都会同步的写入内核缓存和同步磁盘，但每次操作都需要在主进程中写缓存写磁盘，势必会影响Redis处理后续指令的速度。 AOF_FSYNC_EVERYSEC的策略下，异步刷盘和延迟等待的功能，减小了写AOF对Redis主进程执行命令的影响，同时保证最多丢失2s内的操作数据。但由于fsync和write调用都是阻塞的，比如出现写磁盘被阻塞时，那么后续write调用因为都操作相同的文件描述符也会相应的等待，进而影响整个Redis主流程的执行。 异步刷盘由于fsync操作可能阻塞主流程的执行，所以Redis使用了一个独立的线程来处理异步fsync文件到磁盘的工作。Redis对所有需要异步线程操作的任务做了一个统一的封装，代码在bio.c文件中。实现的逻辑也比较简单，针对目前支持的close_file, aof_fsync, lazy_free三种后台任务，各启动一个线程并分配一个任务队列。线程循环等待任务并执行，最后修改任务的等待状态。 刷盘时机Redis源码中调用flushAppendOnlyFile执行flush aof_buf的地方一共有4个： 服务器主进程处理完本次IO和时间事件后，等待下一次事件(epoll_wait)到来之前 服务器定时器serverCron中，默认每秒执行一次 服务器退出之前的准备工作prepareForShutdown()，执行一次强制刷盘 通过配置指令关闭AOF功能时，执行一次强制刷盘 在第一种情况下，服务器在处理客户端请求的操作指令时，如果涉及到数据库内容的更新，并且配置开启了AOF功能，那么上一小节的feedAppendOnlyFile将会被调用，把操作命名以协议格式追加到buf中。在服务器完成进入下一次时间循环之前，flushAppendOnlyFile会被执行一次。 从刷盘流程的流程图中，不难看出，写aof_buf到内核缓存在一次flushAppendOnlyFile的调用中不一定会执行（阻塞或者2s内的延迟等待）。如果后续一直没有新的事件到来，那么本次写入aof_buf的操作就有丢失的可能。所以在服务器每秒一次的定时任务中，会根据当前是否有被延迟执行的刷盘操作以及写aof_buf出错1秒后重试等条件，触发一次对flushAppendOnlyFile的调用。 后面两种case比较类似，都是在正常退出之前，强制执行一次刷盘。强制执行的时候会忽略前面等待的任务而直接写内核缓存并创建新的刷盘任务。 AOF Rewrite为什么要rewriteAOF文件只是简单的存储了写操作相关的命令，而并没有进行合并。随着Redis服务器在运行过程中不断接受命令，如果Redis只是将客户端修改数据库的命令存储在AOF文件中，AOF文件会急剧膨胀而导致效率低下（AOF文件越大，占用存储空间越大，数据还原过程耗时越多）。所以Redis提供了一种rewrite的机制，以当前数据库的数据空间为终态，压缩重写AOF文件。Rewrite可以理解对同一个key先后modify的指令合并为一条指令的过程，具体的实现流程就是遍历当前数据库的键空间，将每个key对应的对象用一条命令来表达并保存到AOF文件中。 Rewrite的具体流程由于rewrite操作需要访问整个内存数据库，与RDB持久化类似，为了防止数据访问的冲突，Redis也fork了一个新的子进程来独立的完成rewrite的过程。 因为子进程可能会对父进程做一份完整的内存拷贝，为了减少大规模内存拷贝的次数并防止内存被占满，如果有正在运行的RDB子进程，直接返回。否则执行步骤2 父进程创建管道，并监听子进程给父进程同步数据管道上的可读事件。 fork子进程。子进程执行步骤4，父进程执行步骤5。 子进程开始扫描从父进程copy来得内存数据，并生成相应的set指令，写入rewrite文件，最后通知父进程执行完成，子进程退出。 父进程做一些统计和设置状态的工作，由于父进程已经将管道加入了事件监听器中，所以父进程可以直接返回，无需再等待子进程执行完成。 在aof_rewrite的父子进程模型中，存在两类管道：第一种是与rdb持久化执行过程中提到的父进程用于监听子进程正常退出的通信管道。由于子进程只能获取到fork()之前的内存数据，为了保持最终录入结果的正确性，在rewrite期间父进程新产生的操作指令也需要通过给子进程写入rewrite文件。这就是第二类管道，专门用于同步差异数据的。回顾Append aof_buf小节aof_rewrite_buf就是父进程用于缓冲差异数据，并最终通过管道同步给子进程的。前述流程中提到的管道都是第二类管道。第二类管道包含3对双向管道，分别用于：父进程给子进程同步数据的读写管道，子进程给父进程同步状态用的读写管道，父进程确认收到子进程数据的读写管道。 我们注意到在父进程返回之前，把当前aof_select_db重置为-1，这是为了保证下一次调用feedAppendOnlyFile()记录操作命令的时候就会强制生成一条SELECT指令，保证父进程同步给子进程的数据能够安全地合并到rewrite文件中。 下面就来看一些rewrite文件生成的具体流程，即上流程图中蓝色标记的具体实现方式： 创建新的临时文件 Redis 4.0新加入了混合模式的持久化文件，综合了RDB文件内容更紧凑恢复更快，AOF机制更安全耐久的优点。如果开启了混合模式，则先用RDB格式将内存快照写入文件（恢复的时候，识别RDB的协议头可以判断是否是混合模式）。否则，遍历整个内存数据空间，根据相应的key-value类型，生成对应的set命令，写入文件。 由于父进程可能还在持续的发送差异数据，所以先执行一次刷盘，完成大部分数据的写入，使下一次差异指令的刷盘过程更快。 持续等待并读取父进程从管道写入的差异数据，直到20ms内没有新的数据到来或者已经等待了1s后退出循环。 利用管道通知父进程停止发送差异数据，并开始同步等待父进程确认收到停止发送的指令。如果5秒内父进程没有响应则执行步骤6，否则执行步骤7。 关闭创建的临时文件描述符，删除临时文件退出，本次rewrite失败。 再从管道读取一次数据，确保收到父进程停止发送差异前的所有数据。刷盘，关闭文件描述符，并通过rename原子性的完成临时文件的重命名到指定的目标文件中。 Rewrite时机Rewrite的触发机制主要有一下三个： 手动调用bgrewriteaof命令，如果当前有正在运行的rewrite子进程，则本次rewrite会推迟执行，否则，直接触发一次rewrite； 通过配置指令手动开启AOF功能，如果没有rdb子进程的情况下，会触发一次rewrite，将当前数据库中的数据写入rewrite文件； 在Redis定时器中，如果有需要退出执行的rewrite并且没有正在运行的rdb或者rewrite子进程时，触发一次或者aof文件大小已经到达配置的rewrite条件也会自动触发一次。 这里重点看一下，Redis基于aof文件大小，自动触发rewrite的策略： 123456789101112131415 /* Trigger an AOF rewrite if needed. */if (server.aof_state == AOF_ON &amp;&amp; server.rdb_child_pid == -1 &amp;&amp; server.aof_child_pid == -1 &amp;&amp; server.aof_rewrite_perc &amp;&amp; server.aof_current_size &gt; server.aof_rewrite_min_size){ long long base = server.aof_rewrite_base_size ? server.aof_rewrite_base_size : 1; long long growth = (server.aof_current_size*100/base) - 100; if (growth &gt;= server.aof_rewrite_perc) { serverLog(LL_NOTICE,\"Starting automatic rewriting of AOF on %lld%% growth\",growth); rewriteAppendOnlyFileBackground(); }} 从代码中不难看出，在开启了aof功能并且没有正在运行的rdb或者rewrite子进程时，触发rewrite需要满足两个条件： 当前aof文件的大小超过了配置的aof_rewrite文件大小的最小基准值； aof当前文件大小相对于上一次rewrite后aof文件大小的增长率，超过了配置的比例； AOF Rewrite父子进程通讯模型在上一节中，我们从rewrite子进程的视角，介绍了具体的流程和出发时机，这一节，我们将从更高层的视角，来看一下整个rewrite过程中父子进程通讯的模型以及父进程在一些问题上处理细节。 整个rewrite过程中，父子进程的通讯模型如上图。前面章节我们主要从右边子进程的维度，分析了整个执行流程。那么从父进程的角度，我们仍需要解决以下问题： 如何有效的同步rewrite过程中的差异数据给子进程； 如何有效的监听子进程同步过来的数据，实时的响应处理； 子进程退出后，如何需要执行哪些善后处理 同步指令在rewrite子进程运行过程中，随着客户端请求命令的不断到来，父进程在feedAppendOnlyFile调用的最后，会将组装过了协议指令，先追加到缓存中，然后通过管道同步给子进程。由于主进程不能等待写管道完成才继续执行，所以需要缓存先存储这些需要同步给子进程的数据。为了避免realloc调用，触发的大规模内存拷贝，Redis对这部分数据，通过一个内存块aofrwblock的链表来维护，默认大小10M。 追加数据到缓存的时候，首先在链表尾部的内存写入数据（不够就再创建新的内存块），同时在管道（上图的diff_data）上注册写事件，通过事件循环触发管道数据的写入。 核心的写管道代码如下： 123456789101112131415161718while(1) { ln = listFirst(server.aof_rewrite_buf_blocks); block = ln ? ln-&gt;value : NULL; if (server.aof_stop_sending_diff || !block) { aeDeleteFileEvent(server.el,server.aof_pipe_write_data_to_child, AE_WRITABLE); return; } if (block-&gt;used &gt; 0) { nwritten = write(server.aof_pipe_write_data_to_child, block-&gt;buf,block-&gt;used); if (nwritten &lt;= 0) return; memmove(block-&gt;buf,block-&gt;buf+nwritten,block-&gt;used-nwritten); block-&gt;used -= nwritten; block-&gt;free += nwritten; } if (block-&gt;used == 0) listDelNode(server.aof_rewrite_buf_blocks,ln); } 监听管道父进程对子进程响应管道child_ack的监听在管道创建的时候就注册了可读事件的处理函数。子进程写入数据后基于Redis的事件触发机制，回调该处理函数即可。 善后处理与RDB的父子进程执行模式一致，在子进程执行结束后，会通过管道（存储在server的结构体中）向父进程发送一些统计数据。在接收到子进程结束的通知后，除了清理子进程的状态和统计信息，最重要的工作是将子进程生成的aof临时文件，替换到配置的aof持久化文件中。为了保证替换的原子性，rename将会被调用。当我们用新的临时文件替换旧的aof文件时，存在以下两种场景会导致主进程的阻塞： aof功能被关闭，主进程不再持有旧aof文件的文件描述符（server.aof_fd=-1）, 当使用临时文件rename旧的aof文件后，由于没有进程再占用它，所以会触发操作系统unlink，即删除该文件的操作，这会阻塞主进程； aof功能仍旧开启，主进程虽然继续持有旧aof文件的描述符，rename不会触发unlink, 但当执行完文件替换后，需要将server.aof_fd重置为新打开的临时文件的描述符，并手动关闭旧的aof文件描述符。同样由于除了主进程，可能没有别的文件再占用它，所以也会触发系统的unlink操作，从而阻塞主进程 为了解决上述两种场景，主进程在aof功能被关闭时，再次打开旧的文件，持有它的文件描述符。将1和2等价到一起。最后通过异步线程池的方式来主动关闭旧的aof文件。","link":"/2018/07/30/redis-01/"},{"title":"RocketMQ源码分析1--Remoting","text":"本文试图以一种简易的语言让你了解到RocketMQ的通信协议模块是如何设计的（毕竟太多程序员自己都很难看懂的技术文章了）。另外如果想要深入了解通信模块，你需要具备Netty的知识。推荐Netty入门综述。 本文是RocketMQ源码分析系列之一，如有疑问或者技术探讨，可以email me,欢迎探讨. 在分布式应用中，不可避免的一个问题就是跨进程的通信，此问题基本都通过RPC调用来解决。RocketMQ的通信模块无疑也是通过RPC实现的Producer、Consumer与Broker之间的通信。在讲解RocketMQ的通信模块之前，先来说关于高性能RPC调用三个重要主题。 传输因为RPC的本质是进程间通信，采用什么样的IO通信模型在很大程度上决定了通信的性能。 协议在网络通信传输过程中，因为发送端和接收端通常不能保证两边使用的是相同的编程语言，即使都是相同语言，也有可能CPU位数不一样，也会造成数据出错。另外因为数据包都是连在一起发送的，没法做消息区分。所以通常需要约定一个通信协议，协议规定好数据流中每个字节的含义。发送端要保证按照协议组装好数据流，接收端按照协议规定读出里面的数据。网络通信传输的是字节码，需要对消息进行编解码。 协议指RPC调用在网络传输中约定的数据封装方式，通常包括三部分：编解码，消息头，消息体。常见的消息体编解码方式是序列化。消息头负责存储方便编码以及方便扩展的元信息，例如语言版本，响应码等等。 线程当通信消息传输完毕之后，通过什么样的线程模型处理线程请求也很重要。常见的模型有Reactor单线程模型，Reactor多线程模型以及Reactor主从模型。虽然单路复用的思想是一个线程handle多个连接，尽量减少线程切换的开销。但是如果客户端请求量大，很可能单线程处理不过来请求导致请求积压响应变慢。因此Reactor多线程模型就是IO操作和非IO操作分开。非IO的线程称为工作线程，客户端的请求直接被丢到线程池中，客户端发送请求不会堵塞。 接着我们来讲讲RocketMQ是怎么解决通信协议的“传输”、“协议”以及“线程”的。 传输简单来说，RocketMQ的remoting模块通过Netty实现了IO单路复用的Reactor通信模型。在RocketMQ启动NameServer的时候，会执行NameServer初始化以及服务端通信对象RemotingServer的启动操作this.remotingServer.start();. start()方法会启动NettyRemotingServer的Netty服务端并初始化一个channel。12345678910111213141516171819202122232425ServerBootstrap childHandler = this.serverBootstrap.group(this.eventLoopGroupBoss, this.eventLoopGroupSelector) .channel(useEpoll() ? EpollServerSocketChannel.class : NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 1024) .option(ChannelOption.SO_REUSEADDR, true) .option(ChannelOption.SO_KEEPALIVE, false) .childOption(ChannelOption.TCP_NODELAY, true) .childOption(ChannelOption.SO_SNDBUF, nettyServerConfig.getServerSocketSndBufSize()) .childOption(ChannelOption.SO_RCVBUF, nettyServerConfig.getServerSocketRcvBufSize()) .localAddress(new InetSocketAddress(this.nettyServerConfig.getListenPort())) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override public void initChannel(SocketChannel ch) throws Exception { ch.pipeline() .addLast(defaultEventExecutorGroup, HANDSHAKE_HANDLER_NAME, new HandshakeHandler(TlsSystemConfig.tlsMode)) .addLast(defaultEventExecutorGroup, new NettyEncoder(), // 指定消息编码器 new NettyDecoder(), // 指定消息解码器 new IdleStateHandler(0, 0, nettyServerConfig.getServerChannelMaxIdleTimeSeconds()), new NettyConnectManageHandler(), new NettyServerHandler() ); } }); 这里启动了Netty的服务端，可以看出是Netty的Reactor模型的体现。EventLoopGroupBoss负责IO的连接，EventLoopGroupSelector负责连接的处理操作。在启动服务端的同时，会初始化一个channel, 并赋予channel注册到一个默认的defaultEventExecutorGroup. 当Producer发送消息（例如“Hello world”）时候，首先rocketmq使用netty进行IO通信，Netty的pipeline在各个handler传递消息进行处理。1pipeline.fireChannelRead(readBuf.get(i)); 1234public final ChannelPipeline fireChannelRead(Object msg) { AbstractChannelHandlerContext.invokeChannelRead(head, msg); return this; } 协议RocketMQ自定义的私有协议栈都是基于TCP/IP协议，使用Netty的NIO TCP协议栈方便的进行私有协议栈的定制和开发。使用Netty定义私有协议栈的步骤： 1. 自定义协议规则RocketMQ协议分为以下四个部分: header data: 协议的头，数据是序列化【fastjson】后的json。 json的每个key字段都是固定的，不同的通讯请求字段不一样。 body data： 请求的二进制实际数据。例如发送消息的网络请求中，body中传输实际的消息内容。 length: 消息总长度。 header length: 序列化类型&amp;消息头长度。第一个字节表示序列化类型，后面三个自己表示消息头长度。 在RocketMQ源码中，通信的消息封装在RemotingCommand这个bean中，其属性可以看出RocketMQ通信协议: 1234567891011121314151617181920212223//RequestCode定义:当表示请求操作代码时候，请求接收方根据代码执行相应操作；//当表示应答结果代码时候，0表示成功，非0表示错误代码。private int code;private LanguageCode language = LanguageCode.JAVA; // 请求和应答方语言private int version = 0; //请求和应答方程序版本private int opaque = RequestId.getAndIncrement();//请求发起方做tcp连接上的线程复用。private int flag = 0; //通信层标志位private String remark;//把下面customHeader中的信息填充到这里，见 makeCustomHeaderToNet//\"extFields\":{\"topic\":\"yyztest2\",\"queueId\":\"3\",\"consumerGroup\":\"yyzGroup2\",\"commitOffset\":\"28\"}private HashMap&lt;String, String&gt; extFields;//请求自定义字段/** * header data * 例如CONSUMER_SEND_MSG_BACK消息， * customHeader 为ConsumerSendMsgBackRequestHeader 填充见 MQClientAPIImpl.consumerSendMessageBack */private transient CommandCustomHeader customHeader;/** * body部分 */private transient byte[] body; 2. 自己编写encoder和decoder通过remoting模块可以发现通信过程中，RocketMQ服务器与客户端通过传递 RemotingCommand对象来进行交互。通过NettyDecoder/NettyEncoder对Remoting Command进行协议的编解码。 消息编解码 使用NettyEncoder.encode进行消息序列化 通信方式: RocketMQ支持三种方式的通信 同步/异步/单向oneway 通信流程解码 RemotingCommand类中的decode方法 - 根据自定义的协议进行解析获取消息头部和消息体：123456789101112131415161718192021public static RemotingCommand decode(final ByteBuffer byteBuffer) { int length = byteBuffer.limit(); int oriHeaderLen = byteBuffer.getInt();//获取报文头部的长度 int headerLength = getHeaderLength(oriHeaderLen); byte[] headerData = new byte[headerLength]; byteBuffer.get(headerData); // 获得报文头部数据 //反序列化解析header data和RemotingCommand类 RemotingCommand cmd = headerDecode(headerData, getProtocolType(oriHeaderLen)); int bodyLength = length - 4 - headerLength; // 获取body长度 byte[] bodyData = null; if (bodyLength &gt; 0) { bodyData = new byte[bodyLength]; byteBuffer.get(bodyData); // 获取报文提数据 } cmd.body = bodyData; //把body部分还原出来，也就是把消息内容 return cmd; } 为了形象起见，我自己写了一个简单的producer发送消息的demo123456Message msg = new Message(\"Topic2Test\",// topic \"TagA\",// tag (\"Hello RocketMQ \").getBytes()// body );SendResult sendResult = producer.send(msg); 通过在decode方法中最后获取到的cmd设置断点会发现：此时RemotingCommand的code为310，通过查阅RequestCode发现：1public static final int SEND_MESSAGE_V2 = 310; 解码消息得知消息状态码为发送消息。并且发送方语言为JAVA,消息所在群组为gsm_group,消息的topic为Topic2Test。并且消息体Hello RocketMQ的字节码存储在了body里面。 3. 编写自己的应用Client&amp;Serverremoting模块通过定义RemotingClient和RemotingServer实现了基于Netty通信的应用客户端和服务器。无论是客户端还是服务器都支持三种通信方式： invokeSync 同步通信 invokeAsync 异步通信 invoikeOneway 单向通信（不需要知道响应）通信对象是上文提到的RemotingCommand，服务端对RemotingCommand进行解码，然后处理。 服务端（实现类NettyRemotingServer）和客户端（实现类NettyRemotingClient）继承了抽象类NettyRemotingAbstract并且实现了RemotingServer/RemotingClient. 抽象类NettyRemotingAbstract中定义了处理请求的方法processRequestCommand,处理响应调用的方法processResponseCommand. 当客户端与服务端进行通信时候 客户端发送请求给服务端，通过Netty的channel进行IO通信给服务端 服务端处理客户端请求。NettyServerHandler接收消息类型REQUEST_COMMAND，调用 processRequestCommand(ctx, cmd); 服务端处理完毕后，通过Netty的channel进行IO通信客户端 客户端处理服务端发送的响应，NettyClientHandler接收消息类型为RESPONSE_COMMAND，调用processResponseCommand(ctx, cmd)123456789101112131415public void processMessageReceived(ChannelHandlerContext ctx, RemotingCommand msg) throws Exception { final RemotingCommand cmd = msg; if (cmd != null) { switch (cmd.getType()) { case REQUEST_COMMAND: processRequestCommand(ctx, cmd); break; case RESPONSE_COMMAND: processResponseCommand(ctx, cmd); break; default: break; } }} 服务端服务端会针对每个访问请求查询其请求编码，根据请求编码使用相应的处理器(processor)进行处理。processorTable记录请求编码、处理器、线程池的关系。 123456/** * This container holds all processors per request code, aka, for each incoming request, we may look up the * responding processor in this map to handle the request. */protected final HashMap&lt;Integer/* request code */, Pair&lt;NettyRequestProcessor, ExecutorService&gt;&gt; processorTable = new HashMap&lt;Integer, Pair&lt;NettyRequestProcessor, ExecutorService&gt;&gt;(64); 客户端客户端使用responseTable记录所有响应。当客户端发送消息时候，会创建ResponseFuture异步响应结果。将每个响应的opaque与ResponseFuture组成的ConcurrentMap存储到responseTable. 1234/** * This map caches all on-going requests. */protected final ConcurrentMap&lt;Integer /* opaque */, ResponseFuture&gt; responseTable = new ConcurrentHashMap&lt;Integer, ResponseFuture&gt;(256); 默认使用同步通信.因为服务端和客户度逻辑相似，这里以客户端为例，：NettyRemotingClient的同步通信实现如下：123456789101112131415161718192021222324 /** * addr:为服务端地址，RPC调用需要制定服务端地址这样客户端才可以发送请求 * request: RemotingCommand封装的消息 */ public RemotingCommand invokeSync(String addr, final RemotingCommand request, long timeoutMillis) throws InterruptedException, RemotingConnectException, RemotingSendRequestException, RemotingTimeoutException { final Channel channel = this.getAndCreateChannel(addr); if (channel != null &amp;&amp; channel.isActive()) { try { if (this.rpcHook != null) { this.rpcHook.doBeforeRequest(addr, request); } System.out.println(\"invoke sync! request: \"+request.toString()); // 同步通信，通信回应存储在reponse里 RemotingCommand response = this.invokeSyncImpl(channel, request, timeoutMillis); if (this.rpcHook != null) { this.rpcHook.doAfterResponse(RemotingHelper.parseChannelRemoteAddr(channel), request, response); } return response; } ... } }} 可以看出RemotingCommand当表示应答结果时候如果处理成功则code = 0。 核心处理为invokeSyncImpl方法123456789101112131415161718192021222324252627282930313233343536373839404142public RemotingCommand invokeSyncImpl(final Channel channel, final RemotingCommand request, final long timeoutMillis) throws InterruptedException, RemotingSendRequestException, RemotingTimeoutException { final int opaque = request.getOpaque(); try { final ResponseFuture responseFuture = new ResponseFuture(opaque, timeoutMillis, null, null); // 客户端使用resonseTable,服务端使用processortable this.responseTable.put(opaque, responseFuture); final SocketAddress addr = channel.remoteAddress(); channel.writeAndFlush(request).addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture f) throws Exception { if (f.isSuccess()) { responseFuture.setSendRequestOK(true); return; } else { responseFuture.setSendRequestOK(false); } responseTable.remove(opaque); responseFuture.setCause(f.cause()); responseFuture.putResponse(null); log.warn(\"send a request command to channel &lt;\" + addr + \"&gt; failed.\"); } }); RemotingCommand responseCommand = responseFuture.waitResponse(timeoutMillis); if (null == responseCommand) { if (responseFuture.isSendRequestOK()) { throw new RemotingTimeoutException(RemotingHelper.parseSocketAddressAddr(addr), timeoutMillis, responseFuture.getCause()); } else { throw new RemotingSendRequestException(RemotingHelper.parseSocketAddressAddr(addr), responseFuture.getCause()); } } return responseCommand; } finally { this.responseTable.remove(opaque); } } 首先构造一个ResonseFuture对象，Netty发送请求(writeAndFlush)并设置监听回调响应.代码流程图参照下图 参考 http://zqhxuyuan.github.io/2017/10/18/Midd-RocketMQ/#","link":"/2018/02/19/remoting/"},{"title":"RocketMQ源码分析3--Store数据存储","text":"本文是RocketMQ源码分析系列之三，如有疑问或者技术探讨，可以email me,欢迎探讨. 设计思路关键词： CommitLog, ConsumeQueue, 内存映射文件 一个产品的技术选型一般都要基于其使用场景，RocketMQ的数据存储也不例外。在数据存储方面，RocketMQ采用了将数据(所有topic的消息)只写在了一个唯一的文件(CommitLog)里面. 之所以这么设计，是有其历史原因的:RocketMQ的最初使用场景是为了支持淘宝双十一实时订单场景(大数据量，topic-partition多)，另外Rocket在设计上最初借鉴了Kafka并试图克服其在topic-partition增多时候的不足。 先来看看Kafka是怎么做数据存储的。 在Kafka的架构设计上，消息以topic分类，每个topic存储在多个partition上，一个partition对应一个单独的log文件上。消息不断追加到log的末尾（顺序写）。partition可以分布在不同的机器上，因此一个topic的消息可以水平扩展到多机器上。当Producer想要写入消息时候，消息会根据其key的哈希值分派到对应的partition上，然后追加写到partition的log文件。但是从IO层面看，这种多topic-多partition方式，对于每个文件来说虽然是顺序IO，但是当并发读写多个partition，实际上是随机IO写到多个partition的。由于磁盘是随机读写慢，顺序读写快。因此，实验证明：topic数量增加到一定程度，Kafka性能急剧下降。 为了解决这个问题，RocketMQ的消息是存储在一个单一的CommitLog文件里，从而避免是磁盘的随机IO。 另外Kafka针对Producer和Consumer使用了同1份存储结构，而RocketMQ却为Producer和Consumer分别设计了不同的存储结构，Producer对应CommitLog, Consumer对应ConsumeQueue。RocketMQ把所有消息都存储在一个CommitLog里，然后再根据topic - queueId异步分发给ConsumeQueue。 ConsumeQueue是逻辑队列，并不真正存储消息的内容，仅仅存储消息在CommitLog的位移(offset)。之所以分成多个queue，是因为这样可以让多个消费者同时消费，而不需要上锁。 对于消费者来说，ConsumeQueue其实是CommitLog的一个索引文件。Consumer消费消息的时候，要读2次： 先读ConsumeQueue得到offset 再根据offset从读CommitLog得到消息。 有关数据存储，可以看出通过将消息都追加写到一个CommitLog文件，实现了顺序写提高了磁盘IO性能。但是这里还有一个问题，Consumer在读消息的时候，实际上是一个随机读磁盘的过程。那么RocketMQ是怎么优化这一点的呢？ 对于CommitLog和ConsumeQueue源代码中都有一个成员MapedFileQueue – Consumer消费消息过程中使用了零拷贝（mmap+write），mmap是一种内存映射文件的方法，即将一个文件或者其他对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中的一段虚拟地址的一一映射关系。实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存。而系统会自动回写脏页面到对应的文件磁盘，即完成了对文件的操作而不比调用read,write等系统调用函数。相反，内核空间对区域的修改也直接反应到用户空间，从而实现不同进程间的文件共享。有关零拷贝推荐此文。 通过使用零拷贝，解决了文件存储IO的瓶颈，从而实现了Broker的高吞吐量。 Store模块数据核心文件RocketMQ的数据存储功能在源码中为Store模块，其结构如下图所示:业务层均通过DefaultMessageStore类提供的方法作为统一入口访问底层文件。RocketMQ存储核心围绕以下六类文件： IndexFile: 由IndexService类提供服务 Consumequeue: 文件由Consumequeue类提供服务 Commitlog: 文件由CommitLog类提供服务 Checkpoint文件: 由StoreCheckPoint类提供访问服务。Checkpoint文件存储Commitlog，CQ 和Index file的刷盘时间。 Config文件: 由ConfigMananger类提供访问服务，json格式存储相关配置。 Lock文件 对于IndexFile/Consumequeue/Commitlog这三类大文件，为了提供读写性能，底层采用java.nio.MappedByteBuffer类，该类是文件内存映射方案(零拷贝)，支持随机读/顺序写操作。大文件在存储时候会被分成固定大小的小文件，每个小文件均由MapedFile类提供操作服务；MapedFile类提供了顺序写、随机读、内存数据刷盘、内存清理等与文件相关的服务。MappedFileQueue中用集合list把这些MappedFile文件组成了一个逻辑上连续的队列，从而实现了整个大文件的串联。 MappedFileMappedFileQueue + Maped File 保存存放在物理机器上的文件信息。Mapped File: 存储具体的文件信息: 包括文件路径，文件名(文件起始偏移)，写位移，读位移等等信息，同时使用了虚拟内存提高IO效率。 MappedFile提供API:12345public appendMessage(final byte[] data) // 顺序写msg到MapedFilepublic commit(final int commitLeastPages)//将内存消息刷盘public SelectMappedBufferResult selectMappedBuffer(int pos, int size) // 随机读消息 CommitLogCommitLog是保存消息元数据的物理存储文件,所有到达Broker的消息都会保存到Commitlog文件。这里需要强调的是所有topic的消息都会统一保存在commitLog中，举个例子：当前集群有TopicA, TopicB，这两个Toipc的消息会按照消息到达的先后顺序保存到同一个commitLog中，而不是每个Topic有自己独立的cCommitLog。 每个CommitLog大小上限为1G，满1G之后会自动新建CommitLog文件做保存数据用。 CommitLog的清理机制： 按时间清理，默认清理三天前的commitlog文件 按磁盘水位清理。当磁盘使用量到达磁盘容量的75%,开始清理最老的commitlog文件。 ConsumeQueueConsumeQueue是消息的位置文件，主要存储消息在CommitLog的位置(offset)，多个文件构成一个队列。内部采用MappedFileQueue实现了消息位置文件队列功能。1一个CQ存储单元 = 8字节commitlog offset + 4字节commit log item size + 8字节message tag hashcode. IndexFile索引文件存储的消息的索引 - 方便根据topic+消息的key快速查询消息。 具体说来：索引可以理解为一个类似hashtable的结构。建立索引时候根据每条消息的topic + &quot;#&quot; + key的hash值取模计算出实际文件位置absSlotPos，将消息在CommitLog的位移位置存储到slotPos对应的slot里面。123456789int absIndexPos = IndexHeader.INDEX_HEADER_SIZE + this.hashSlotNum * hashSlotSize + this.indexHeader.getIndexCount() * indexSize;// 存入该消息的索引this.mappedByteBuffer.putInt(absIndexPos, keyHash);this.mappedByteBuffer.putLong(absIndexPos + 4, phyOffset);this.mappedByteBuffer.putInt(absIndexPos + 4 + 8, (int) timeDiff);this.mappedByteBuffer.putInt(absIndexPos + 4 + 8 + 4, slotValue);this.mappedByteBuffer.putInt(absSlotPos, this.indexHeader.getIndexCount()); ConsumeQueue和IndexFile什么时候建立的呢？ – 在Broker启动的时候，会启动一个ReputMessageService线程服务:1this.reputMessageService.start(); 该线程每隔一秒就会执行，根据CommitLog最新追加到的消息不断生成： 消息的offset到CommitQueue 消息索引到IndexFile 具体流程为：其中：reputOffset: broker启动时候CommitLog的最大offset. 构建ConsumeQueue:12345678910111213141516class CommitLogDispatcherBuildConsumeQueue implements CommitLogDispatcher { @Override public void dispatch(DispatchRequest request) { final int tranType = MessageSysFlag.getTransactionValue(request.getSysFlag()); switch (tranType) { case MessageSysFlag.TRANSACTION_NOT_TYPE: case MessageSysFlag.TRANSACTION_COMMIT_TYPE: DefaultMessageStore.this.putMessagePositionInfo(request); break; case MessageSysFlag.TRANSACTION_PREPARED_TYPE: case MessageSysFlag.TRANSACTION_ROLLBACK_TYPE: break; } } } 1234567 /** * 根据commitlog中该条消息的topic和queueID找到该条消息对应的ConsumeQueue */public void putMessagePositionInfo(DispatchRequest dispatchRequest) { ConsumeQueue cq = this.findConsumeQueue(dispatchRequest.getTopic(), dispatchRequest.getQueueId()); cq.putMessagePositionInfoWrapper(dispatchRequest); } 构建IndexFile:123456789class CommitLogDispatcherBuildIndex implements CommitLogDispatcher { @Override public void dispatch(DispatchRequest request) { if (DefaultMessageStore.this.messageStoreConfig.isMessageIndexEnable()) { DefaultMessageStore.this.indexService.buildIndex(request); } }} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public void buildIndex(DispatchRequest req) { IndexFile indexFile = retryGetAndCreateIndexFile(); if (indexFile != null) { long endPhyOffset = indexFile.getEndPhyOffset(); DispatchRequest msg = req; String topic = msg.getTopic(); String keys = msg.getKeys(); if (msg.getCommitLogOffset() &lt; endPhyOffset) { return; } final int tranType = MessageSysFlag.getTransactionValue(msg.getSysFlag()); switch (tranType) { case MessageSysFlag.TRANSACTION_NOT_TYPE: case MessageSysFlag.TRANSACTION_PREPARED_TYPE: case MessageSysFlag.TRANSACTION_COMMIT_TYPE: break; case MessageSysFlag.TRANSACTION_ROLLBACK_TYPE: return; } if (req.getUniqKey() != null) { indexFile = putKey(indexFile, msg, buildKey(topic, req.getUniqKey())); if (indexFile == null) { log.error(\"putKey error commitlog {} uniqkey {}\", req.getCommitLogOffset(), req.getUniqKey()); return; } } if (keys != null &amp;&amp; keys.length() &gt; 0) { String[] keyset = keys.split(MessageConst.KEY_SEPARATOR); for (int i = 0; i &lt; keyset.length; i++) { String key = keyset[i]; if (key.length() &gt; 0) { indexFile = putKey(indexFile, msg, buildKey(topic, key)); if (indexFile == null) { log.error(\"putKey error commitlog {} uniqkey {}\", req.getCommitLogOffset(), req.getUniqKey()); return; } } } } } else { log.error(\"build index error, stop building index\"); }} 数据存储功能消息队列在使用过程中都会面临着如何承载消息堆积并在合适的时机投递的问题。处理堆积的最佳方式就是数据存储。 下图是RocketMQ数据的存储管理方式。所有Producer生产的消息，按照插入顺序写入一个CommitLog文件。CommitLog按照固定的大小被切割为多个小文件，每个文件MappedFile都通过内存映射的方式加载入内存。MappedFile随着消息的写入，不断生成，并放在MappedFileQueue队列的最后，每次消息写入都在最后一个MappedFile后追加，直到达到最大的文件大小，重新生成一个新的MappedFile。每个MappedFile名字表示文件中第一条消息在CommitLog中的物理偏移量。每条消息有固定的格式，在写入CommitLog的过程中，RocketMQ会启动一个异步线程，读取MappedFile，根据生产消息时指定的topic和ConsumeQueue id将每条消息在整个CommitLog中的物理偏移量，以及消息的大小和标签码，组织为一条CQUnit，写入对应ConsumeQueue。ConsumeQueue的组织方式与CommitLog类似，每个topic的每个ConsumeQueue独占一个文件夹，文件夹里同样按照MappedFileQueue的方式，组织所有CQStoreUnit。 从功能上讲，数据存储用于存储： Producer生产的消息 Consumer的逻辑队列: 记录消息在CommitLog中的offset 索引：用于查询消息，当用户想要查询topic下的某个key消息时，能够快速响应. 主从复制：用于实现Master - Slave之间的信息同步。 核心类：123456789101112131415161718192021222324252627282930DefaultMessageStore { private final MessageStoreConfig messageStoreConfig; //存储相关配置，例如存储路径，CommitLog路径及大小 private final CommitLog commitLog; private final ConcurrentMap&lt;String/* topic */, ConcurrentMap&lt;Integer/* queueId */, ConsumeQueue&gt;&gt; consumeQueueTable; //topic的消费队列表 private final FlushConsumeQueueService flushConsumeQueueService; //CQ刷盘服务线程 private final CleanCommitLogService cleanCommitLogService;//磁盘超过水位进行清理 private final CleanConsumeQueueService cleanConsumeQueueService; private final IndexService indexService; //索引服务 - 用于创建索引文件集合，当用户想要查询某个topic下某个key的消息时，能够快速响应 主要用于查询消息用的，根据topic+key获取消息, private final AllocateMappedFileService allocateMappedFileService; private final ReputMessageService reputMessageService; //CommitLog异步构建CQ和索引文件的服务。 private final HAService haService; //主备同步服务 private final ScheduleMessageService scheduleMessageService;//延迟消息：先把消息投递到delay topic暂存，然后通过定时器把delay topic暂存的消息投递到真实的topic. private final StoreStatsService storeStatsService; //存储统计服务 private final TransientStorePool transientStorePool; private final RunningFlags runningFlags = new RunningFlags(); private final SystemClock systemClock = new SystemClock(); private final ScheduledExecutorService scheduledExecutorService = Executors.newSingleThreadScheduledExecutor(new ThreadFactoryImpl(\"StoreScheduledThread\")); private final BrokerStatsManager brokerStatsManager; //Broker统计服务 private final MessageArrivingListener messageArrivingListener; //消息到达监听器 private final BrokerConfig brokerConfig; private volatile boolean shutdown = true; private StoreCheckpoint storeCheckpoint; //检查点 private AtomicLong printTimes = new AtomicLong(0); private final LinkedList&lt;CommitLogDispatcher&gt; dispatcherList; //转发CommitLog到CQ private RandomAccessFile lockFile; private FileLock lock; boolean shutDownNormal = false;} //存储模块核心类,提供数据功能API Producer生产消息1PutMessageResult result = this.commitLog.putMessage(msg); // 向CommitLog追加消息 broker在接收到Producer发送的消息之后，会同步或者异步地刷盘。一般来说，数据存储为了追求高吞吐，会先将数据写入到内存，然后再刷盘到磁盘中进行持久化。1private final FlushCommitLogService flushCommitLogService; 同步刷盘：当数据写到内存之后立刻刷盘，在保证刷盘成功的前提下响应client 异步刷盘：数据写入内存后，直接响应client, 异步将内存中的数据持久化到磁盘上。 Consumer消费消息通过PullMessageProcessor处理消费消息。123public GetMessageResult getMessage(final String group, final String topic, final int queueId, final long offset, final int maxMsgNums, final SubscriptionData subscriptionData); 在根据topic和queueId在指定consumeQueue中第offset个消息开始，拉取maxMsgNums条消息时，首先根据offset找到consumeQueue中的目标MappedFile，然后计算offset在MappedFile中真实的物理偏移量，开始依次读取maxMsgNums条consumeQueue记录CQStoreUnit，回顾之前的数据存储图，CQStoreUnit中存储了此条消息在commitlog中的真实物理偏移和大小，以此为依据在commitlog的消息记录（过程与读取consumeQueue的CQStoreUnit相似，都是先找MappedFile，再取数据）。 为了防止一次性拉去的消息太多，阻塞其他任务，会从以拉取消息的个数和内存大小两个角度限制一次消息拉去的量。 索引索引组件用于创建索引文件集合，RocketMQ允许我们在消息体的property字段中设置一些属性信息记为keys，当消费者想要获取某个topic下的某个key的消息时候能够快速响应。索引组件的具体实现是通过IndexFile来操作的，逻辑结构图如下： 其实，IndexFile就是一个Hash表的具体实现。头字段（蓝色区域）用于记录当前IndexFile文件中内存的使用情况及最新修改的时间戳，哈希桶（紫色区域）用于记录索引信息的哈希值槽位，真实的索引信息（绿色区域）存储在最后，按照索引信息的录入顺序，依次存储。 当录入索引信息的时候，以topic#key为键，计算其哈希值，对哈希桶的总数取余，定位当前索引信息的哈希槽位。由于索引信息按序录入，所以当索引信息一到的时候，其存储位置就固定了，只需要在哈希槽位中记录其位置信息即可，即上图的indexCount。在发生哈希碰撞的时候，采用头插，以最新索引信息的indexCount覆盖哈希槽位的旧值，并把旧值记录在索引信息中。 当以某个topic和key取对应消息的物理点位时，首先定位槽位，然后遍历碰撞的“链表”，取到所需要的信息即可。 IndexFile文件内部的查找就是Hash查找的过程，那如何查找IndexFile呢？这就需要用到IndexFile头中的beginTimeStamp字段和endTimeStamp字段了，它们代表了当前索引信息中第一条和最后一条索引的更新时间。消费者查找时，出了topic和关键字信息，还需要指定时间段。索引组件根据请求的时间段，从后往前匹配。 主从复制参考 travi’s blog很精彩-1 travi’s blog很精彩-2 Dengshenyu - Kafka系列 meliong blog 零拷贝","link":"/2018/03/11/rocketmq-store/"},{"title":"Redis RDB持久化 - Redis源码分析","text":"Redis是内存数据库，持久化的功能可以将Redis在内存中的数据保存到磁盘里，避免数据在进程退出或者意外宕机等情况下意外丢失。Redis提供了两种持久化的方式，RDB和AOF。本文重点关注RDB相关的知识点。 初识RDBRDB的持久化方式是将内存数据以快照的形式写入磁盘文件，并在Redis启动的时候，通过此文件恢复内存数据的状态。 1234redis&gt; set 1 2redis&gt; set 123456789 2redis&gt; set msg helloredis&gt; save 上述命令执行完成后，生成的rdb文件如下：(Redis version 4.0.8) RDB文件结构从上面的二进制文件，不容易看出RDB文件的组织方式。实际上RDB文件的整体结构如下图所示: Magic+Version开头的幻数，用来标识文件为RDB文件。Magic为固定的REDIS这五个字符，Version指定了当前RDB文件的版本号。结合上图的二进制dump文件，知道当前RDB文件的版本号为8。 AuxFields一些记录redis状态的附属信息，包括redis-ver(版本), redis-bits(机器位数), ctime(创建时间), user-mem(内存占比)以及slave节点编号(可选)等。每个附属域字段的前置标识码都是250，即上图的372。 DB SNAPSHOT每一个内存库的快照，以标识码SELECT_DB=254(即上图的376)开始，后面紧跟当前内存库的编号。标识码RESIZE_DB=251(即上图的376)开始的三段内存，记录了数据库中键值对个数和设置了到期时间的键值对个数。回顾上面的操作指令和二进制dump文件，我们设置了3条数据，没有设置过期时间，二者相稳合。 接下来就是每一条键值对的持久化信息。如果某一条记录设置了过期时间，并且执行SAVE持久化的时候，还没过期，那么会首先记录下到期的时间戳，以标识码252开始。由于Redis内置了多种数据类型，为了高效的存储和还原，其持久化时候的编码方式各异，但整体上都遵循type+key+value的格式。以dump文件倒数第二行中比较明显的 msg=》hello为例，做个简要说明：类型码0表示value在Redis内部是以字符串格式进行的编码，003表示key的长度为3，紧跟着key的字符串msg，value也一样。详细的每一种数据结构的编码方式会在后文阐述。 EOF文件结束标识码EOF=255，即rdb dump文件中的377。 CHECK_SUMRDB文件校验和，校验RDB文件是否损坏，8字节长，紧随EOF。 重点问题对RDB文件有了基础的了解后，继续深入对RDB持久化的学习。在此之前，先思考下设计实现一个持久化机制前，需要重点解决那些问题？ 由于Redis内部有多种数据结构的实现，如何为每一种数据结构，设计一种编码方式，达到高效存储和还原的目的。 Redis需要快速的响应用户请求，如何在尽量小地影响用户访问的情况下，完成持久化的操作。 对问题2的一个延伸，在哪些情况下触发持久化的流程，以及如何配置。 持久化的具体实现流程和恢复流程。 带着这些问题，一一分析。 如何设计存储协议，支持多种数据结构的存储长度编码在介绍各种数据结构的持久化编码方式之前，先介绍一下RDB中对长度信息的编码。在RDB文件中有很多地方需要存储长度信息，如字符串长度、list长度等等。如果使用固定的int或long类型来存储该信息，在长度值比较小的时候会造成较大的空间浪费。Redis设计了一套针对长度的编码方式，主要通过读取第一字节的最高 2 位来决定接下来如何解析长度信息： 编码方式 占用字节 说明 00 \\ 000000 1 byte 低6位表示长度，最大63 01 \\ 00··000 2 byte 低14位表示长度 10 \\ 000000 + 4byte 5 byte 后面4 byte表示长度，网络字节序 10 \\ 000001 + 8byte 9 byte 后面8 byte表示长度，网络字节序 11 \\ OBKIND 1 byte 低6位指定了对象的类型 OBKIND的几种定义如下 1234#define RDB_ENC_INT8 0 /* 8 bit signed integer */#define RDB_ENC_INT16 1 /* 16 bit signed integer */#define RDB_ENC_INT32 2 /* 32 bit signed integer */#define RDB_ENC_LZF 3 /* string compressed with FASTLZ */ 举个例子，执行如下几条Redis指令: 123redis&gt;set -1 2redis&gt;set msg helloredis&gt;save 分析RDB的dump文件如图： 第一条指令的编码中300(对应十进制192)，二进制高两位都是1，表示采用了特殊的自定义编码，低6位都是0，说明使用了RDB_ENC_INT8的方式编码，即紧随着的内容377(对应二进制11111111)是一个有符号的整数-1，这与key是-1稳合，同理value是2；第二条指令dump比较直观，看下value hello的编码，005高二位都是0，表示采用1字节的长度编码，低6位的值就是5，说明value的长度为5， 与hello稳合。 String由于Redis中key总是以字符串的形式组织，所以将其纳入字符串类型的value中一起分析，不再单独描述。字符串的编码方式有两种：整数编码(OBJ_ENCODING_INT )和普通字符串序列(REDIS_ENCODING_RAW)。 整数编码根据数字的大小有两种不同的实现方式：能在32bit中表示的有符号数，按照32位整数形式存储；否则按照字符数组的形式存储。核心代码如下： 12345678910111213141516171819/* Save a long long value as either an encoded string or a string. */ssize_t rdbSaveLongLongAsStringObject(rio *rdb, long long value) { unsigned char buf[32]; ssize_t n, nwritten = 0; /*Encode as signed integer*/ int enclen = rdbEncodeInteger(value,buf); if (enclen &gt; 0) { return rdbWriteRaw(rdb,buf,enclen); } else { /* Encode as string */ enclen = ll2string((char*)buf,32,value); serverAssert(enclen &lt; 32); if ((n = rdbSaveLen(rdb,enclen)) == -1) return -1; nwritten += n; if ((n = rdbWriteRaw(rdb,buf,enclen)) == -1) return -1; nwritten += n; } return nwritten;} 其中，整数编码的实现比较简单，根据字面值值能否使用1byte, 2byte, 4byte来表示，将数字按位存储到一个字节数组里。整体结构如下图： 对于太长的数字值，Redis使用ll2string方法进行转换。一般我们把一个数字转换为字符表示时，比较直观的做法就是除10取余，然后再加上对应的数字字符。但是，对于long long这种很长的数字，效率就比较低下了。看下ll2string的实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253int ll2string(char *dst, size_t dstlen, long long svalue) { static const char digits[201] = \"0001020304050607080910111213141516171819\" \"2021222324252627282930313233343536373839\" \"4041424344454647484950515253545556575859\" \"6061626364656667686970717273747576777879\" \"8081828384858687888990919293949596979899\"; int negative; unsigned long long value; /* The main loop works with 64bit unsigned integers for simplicity, so * we convert the number here and remember if it is negative. */ if (svalue &lt; 0) { if (svalue != LLONG_MIN) { value = -svalue; } else { value = ((unsigned long long) LLONG_MAX)+1; } negative = 1; } else { value = svalue; negative = 0; } /* Check length. */ uint32_t const length = digits10(value)+negative; if (length &gt;= dstlen) return 0; /* Null term. */ uint32_t next = length; dst[next] = '\\0'; next--; while (value &gt;= 100) { int const i = (value % 100) * 2; value /= 100; dst[next] = digits[i + 1]; dst[next - 1] = digits[i]; next -= 2; } /* Handle last 1-2 digits. */ if (value &lt; 10) { dst[next] = '0' + (uint32_t) value; } else { int i = (uint32_t) value * 2; dst[next] = digits[i + 1]; dst[next - 1] = digits[i]; } /* Add sign. */ if (negative) dst[0] = '-'; return length;} 其核心实现的思想，比较直接，除以10比较慢，那就一次除以100，余数在表digits中查。由于每个余数可以占据digits中两个位置，可以通过余数乘以2来定位。 原生字符串编码的处理，如果字符串长度小于等于11，首先会尝试按照整数方式进行编码，否则如果配置了启动rdb_compression功能，并且字符串内容大于等于20个字节，则使用lzf算法压缩后存储。通过lzf算法压缩后，编码得到的结构如下图： 如果前面两个条件都不满足，则先按照前述长度编码的方式，记录字符串的长度，然后逐个字节的写入字符串内容。 string浮点数的存储按照原生字符串的形式编码，验证如下图： HASHhash对象的编码方式有两种：ziplist和dict。 当哈希对象可以同时满足以下两个条件时列表对象使用ziplist编码： 哈希对象保存的所有字符串元素的长度都小于64字节 哈希对象保存的元素数量小于512个 否则哈希对象需要使用dict编码。 ziplist的内存结构可以描述为： 1&lt;zlbytes&gt;&lt;zltail&gt;&lt;zllen&gt;&lt;entry&gt;…&lt;entry&gt;&lt;zlend&gt; 其中zlbytes占4字节，表示ziplist占用的总字节数；zltail占4字节，表示最后一项在ziplist中的偏移量，方便在尾端直接进行PUSH和POP等操作，zllen占2字节，表示entry的个数；zlend结束符，和前述的EOF一致，值为255。entry是真正存放数据的数据项，有自己独立的内部结构如下： 1&lt;prevrawlen&gt;&lt;len&gt;&lt;data&gt; prevrawlen表示前一项数据的总字节数，len表示当前数据项data的长度。这两个长度都是根据实际情况变长编码的，这里不展开描述。 我们使用一个简单的例子来观察RDB对ziplist类型的hash如何存储，并验证这种内存结构。 123redis&gt;hset me name niexiaoredis&gt;hset me age 18redis&gt;save 执行这几条命令后，生成的ziplist结构如下图： 如果hash对象以ziplist类型编码，首先按照当前机器的字节序，取前四个字节zlbytes，计算zset对象整体占据的字节数，然后按照原生字符串的形式，即将当前hash对象在内存中的表示，按字节顺序写入文件。整体结构如下图： 上图中按照方框分隔，依次表示的内容为：hash类型以ziplist编码时对应的持久化类型码13；对key的持久化内容长度2+内容’m’,’e’；ziplist总共占用的字节 数18；剩下的所有内容按照颜色与内存结构图中完全一致。 Dict的结构比较常见，在Redis中的实现结构如下图： 针对Dict的持久化过程比较简单，先写入dict的元素个数，然后再迭代entry，以字符串的方式，将entry.field和entry.value写入文件，不再验证。 ZSETzset对象的编码方式有两种：ziplist和skiplist。那么什么情况下会使用ziplist，什么情况下会使用skiplist呢？ 当zset对象同时满足以下两个条件时，对象使用ziplist编码 有序集合保存的元素数量小于128个 有序集合保存的所有元素成员的长度都小于64字节 否则，将使用skiplist编码。 ziplist编码的持久化在hash类型中已经有详细的说明。这里重点看一下对skiplist的持久化。 同样，我们先用个例子回顾下skiplist的结构： 123456789redis&gt;zadd algebra 87.5 Aliceredis&gt;zadd algebra 89.0 Bobredis&gt;zadd algebra 65.5 Charlesredis&gt;zadd algebra 78.0 Davidredis&gt;zadd algebra 93.5 Emilyredis&gt;zadd algebra 87.5 Fredredis&gt;zadd algebra 95.5 \"a long string bigger than 64 bytes\"redis&gt;zrenrangebyscore algebra 94 96 redis&gt;save 执行上述命令后，Redis内部生成的一个跳表可能的结构如下图： 而对应的RDB dump文件如下： 针对skiplist类型value的持久化，整体流程为首先存储整个skiplist的长度，也就是节点个数，然后遍历节点，再依次写入节点内容和得分。需要注意的地方有两点：对skiplist的持久化，value数据是按照跳表节点从后往前存储的。这么做的好处是，有助于提升还原skiplist时候的效率，因为每次插入的都是当前最小值，减少了查找的过程，Redis源码中给出的注释是： We save the skiplist elements from the greatest to the smallest (that’s trivial since the elements are already ordered in the skiplist): this improves the load process, since the next loaded element will always be the smaller, so adding to the skiplist will always immediately stop at the head, making the insertion O(1) instead of O(log(N)). 第二，对于节点内容，按照原生字符串方式进行持久化，而对于double类型的score，并非按照整数类型的编码，而是全部转换为小端后，按照内存布局写入。 LISTRedis中list类型遍量的底层数据结构是quicklist，它是一个双向链表，源码注释中是这样描述的： A doubly linked list of ziplists 它确实是一个双向链表，而且是一个ziplist的双向链表。这里不对具体的结构以及涉及到的ziplist节点大小调参等细节做过多描述。还有一个影响持久化的特性是：当列表很长的时候，最容易被访问的很可能是两端的数据，中间的数据被访问的频率比较低（访问起来性能也很低）。如果应用场景符合这个特点，那么list还提供了一个选项，能够把中间的数据节点进行压缩，从而进一步节省内存空间。 有了数据结构的支持，持久化的过程也就很直观了：首先写入quicklist节点中的长度信息，也就是quicklistNode的节点个数。然后从头结点开始遍历，如果不是被压缩过的节点，则直接把上文ziplist的内存结构按原生字符串的方式，写入文件。否则，按照如下lzf压缩格式编码写入。 SETset对象的编码方式也有两种：intset和dict。当set中添加的元素都是整型且元素数目较少时(512)，set使用intset作为底层数据结构，否则，set使用dict作为底层数据结构。dict格式的持久化在前面已经介绍过了。 这里重点看下intset，它整体结构有header和content两部分组成。Header中encoding指定了content里整数占据的字节数，length指定了整数 的个数，而content的整体长度就是encoding*content。 intset还有一个比较重要的特性是：它所包含的整数元素是按照有序的方式组织，从而便于在上面进行二分查找，用于快速地判断一个元素是否属于这个集合。 针对intset的持久化，由于其在内存上的连续性，所以持久化的时候可以直接按照内存布局，也就是原生字符串的形式写入文件。拿个例子验证下： 12345redis&gt;sadd score 10redis&gt;sadd score 5redis&gt;sadd score 7redis&gt;sadd score 9redis&gt;save 执行上诉命令后生成的dump文件如下图： 图中被彩色区域框起来的部分，依次表示：intset的总大小： 14byte(encoding)+4byte(length)+2*4 = 16bytes encoding：2，表示每个整数元素使用2字节编码 length: 4，表示总共有4个元素 content: 按照数据元素的大小，从小到大排序依次存储。 如何解决数据存储时的阻塞问题前面对持久化过程中，不同对象和编码的持久化写入方式和RDB文件结构进行了介绍。那么如何生成RDB文件呢？Redis提供了两个命令来生成RDB文件，SAVE和BGSAVE。 SAVE命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在服务器进程阻塞期间，不能处理任何命令请求。显然，在线上的应用环境中，这种操作是不被允许的。SAVE只适合在确认影响范围的情况下，手动执行。 BGSAVE派生一个子进程，负责处理RDB文件的创建工作，父进程继续处理命令请求。 这里可能会有一个疑问，为什么不在主进程中创建新的线程，而是创建新的子进程来执行RDB的持久化呢？主要是出于Redis性能的考虑，我们知道Redis对客户端响应请求的工作模型是单进程和单线程的，如果在主进程内启动一个线程，这样会造成对数据的竞争条件。所以为了避免使用锁降低性能，Redis选择启动新的子进程，独立拥有一份父进程的内存拷贝，以此为基础执行RDB持久化。 从代码的角度来说，SAVE命令的执行流程如下图： SAVE的实现过程比较直白，没有太多的逻辑分支，要么全部成功，要么报错。这里隐含了一种”原子性”的概念，因为，rdb的dump文件无论执行多少次SAVE，都只会有一个文件，所以要求要么SAVE成功后，新的dump文件，被更新，要么任何一环出问题，旧的dump文件保持不变。Redis的实现方式就是，利用rename调用，先生成一个临时文件，并完成刷盘 ，然后使用rename调用对旧的dump文件进行替换。关于rename的原子性可以参考rename的man page 和 讨论。 BGSAVE的执行流程如下： 如果有正在执行的AOF子进程或者RDB子进程，直接返回失败，防止两个子进程，同时执行大量的磁盘写入，影响效率。 保存执行bgsave时，已修改未持久化的脏数据项个数。 打开一个双向管道，用于后续父子进程的通信。 fork出一个子进程。 子进程中首先关闭从父进程继承来的套接字，然后设置进程名称。 子进程调用save执行持久化保存。 如果子进程保存成功，则统计持久化过程中，因copy-on-write机制，生成的脏页内存信息，写日志并通过管道，同步给父进程，用于展示Redis的状态信息时使用(info command)。 如果子进程持久化出错，直接退出(使用错误码1)。 父进程首先统计执行fork操作的时间以及内存分配的速率。 如果子进程创建成功，则记录子进程pid和持久化类型；否则，关闭创建的双向管道，设置bgsave的状态为出错。 从bgsave的实现流程不难看出，解决父进程阻塞的问题主要依赖于fork出子进程，独立运行。但是，在实际线上应用场景中，Redis分配的内存都比较大，并且更新很频繁，那么当Copy-On-Write机制出发的内存拷贝过程，会非常消耗资源。参考bgsave的fork问题 所以，为了尽可能的避免内存脏页的问题，在父进程的最后，选择关闭dict的resize功能，避免出发大量的内存拷贝。 持久化的配置与触发时机显然，线上应用场景中，持久化功能需要自动的执行，并且是可配置的。Redis允许用户通过设置服务器的save选项，让服务器每隔一段时间自动执行一次BGSAVE命令。save选项的设置可以包含多个条件，只要其中任意一个被满足，服务器就会执行BGSAVE命令。 123save 900 1save 300 10save 60 10000 以上诉配置为例，只要服务器900秒之内进行了至少1次修改，或者300秒之内进行了至少10次修改，或者60秒之内进行了至少10000次修改，都会触发bgsave。 Redis服务器保存此配置的结构体为： 1234struct saveparam { time_t seconds; //秒数 int changes; //被修改次数}; Redis的服务器会在内部启动一个定时操作，维护Redis的状态，其中一项工作就是检查saveparam，判断是否需要执行bgsave命令。具体的检查逻辑如下： 123456789101112131415161718192021222324252627282930313233343536 /* Check if a background saving or AOF rewrite in progress terminated. */ if (server.rdb_child_pid != -1 || server.aof_child_pid != -1 || ldbPendingChildren()) { if ((pid = wait3(&amp;statloc,WNOHANG,NULL)) != 0) { ... if (pid == server.rdb_child_pid) { backgroundSaveDoneHandler(exitcode,bysignal); if (!bysignal &amp;&amp; exitcode == 0) receiveChildInfo(); } ... } }else{ for (j = 0; j &lt; server.saveparamslen; j++) { struct saveparam *sp = server.saveparams+j;/* Save if we reached the given amount of changes,* the given amount of seconds, and if the latest bgsave was* successful or if, in case of an error, at least* CONFIG_BGSAVE_RETRY_DELAY seconds already elapsed. */ if (server.dirty &gt;= sp-&gt;changes &amp;&amp; server.unixtime-server.lastsave &gt; sp-&gt;seconds &amp;&amp; (server.unixtime-server.lastbgsave_try &gt; CONFIG_BGSAVE_RETRY_DELAY || server.lastbgsave_status == C_OK)) { serverLog(LL_NOTICE,\"%d changes in %d seconds. Saving...\", sp-&gt;changes, (int)sp-&gt;seconds); rdbSaveInfo rsi, *rsiptr; rsiptr = rdbPopulateSaveInfo(&amp;rsi); rdbSaveBackground(server.rdb_filename,rsiptr); break; } } } 首先检查是否有持久化的子进程未关闭，如果有则对子进程执行后，服务器dirty项，上一次保存时间和是否执行成功等状态进行更新，同时恢复dict的resize功能，并关闭父进程打开的pipe。否则，遍历配置项，检查当前的dirty项和上一次保存到现在的时间差是否满足配置的条件，如果是，则再执行一次bgsave。由于，子进程的持久化可能会失败（比如前面说的fork()拷贝内存过大的问题导致），所以检查条件的时候，如果发现上一次失败了，那么会延迟一段时间，再执行bgsave来消弱这种出错的问题。 其他实现细节Redis对IO操作做了统一的封装，提供了一个抽象的访问接口。根据IO操作媒介的不同，有不同的实现，主要分为Buffer IO，Stdio file pointer IO和File descriptors set IO三种。 核心IO结构体定义如下, 它对不同IO实现所依赖的底层读写接口进行了定义，以union的方式，把IO操作的IO变量组织起来。 12345678910111213141516171819202122232425262728293031323334353637383940414243struct _rio {/* Backend functions.* Since this functions do not tolerate short writes or reads the return* value is simplified to: zero on error, non zero on complete success. */ size_t (*read)(struct _rio *, void *buf, size_t len); size_t (*write)(struct _rio *, const void *buf, size_t len); off_t (*tell)(struct _rio *); int (*flush)(struct _rio *); void (*update_cksum)(struct _rio *, const void *buf, size_t len); /* The current checksum */ uint64_t cksum; /* number of bytes read or written */ size_t processed_bytes; /* maximum single read or write chunk size */ size_t max_processing_chunk; /* Backend-specific vars. */ union { /* In-memory buffer target. */ struct { sds ptr; off_t pos; } buffer; /* Stdio file pointer target. */ struct { FILE *fp; off_t buffered; /* Bytes written since last fsync. */ off_t autosync; /* fsync after 'autosync' bytes written. */ } file; /* Multiple FDs target (used to write to N sockets). */ struct { int *fds; /* File descriptors. */ int *state; /* Error state of each fd. 0 (if ok) or errno. */ int numfds; off_t pos; sds buf; } fdset; } io;}; RDB持久化的文件写入，就是基于文件的IO实现: 1234567891011static const rio rioFileIO = { rioFileRead, /*基于文件读写接口的读写实现*/ rioFileWrite, rioFileTell, rioFileFlush, NULL, /* update_checksum */ 0, /* current checksum */ 0, /* bytes read or written */ 0, /* read/write chunk size */ { { NULL, 0 } } /* union for io-specific vars */}; 这种高度抽象的代码实现方式，不仅通用而且精简了代码，值得学习~ 参考文献 redis-ziplist redis-intset redis-skiplist redis-dict redis-object 《Redis设计与实现》 黄健宏著","link":"/2018/07/30/redis-rdb/"}],"tags":[{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"侣行","slug":"侣行","link":"/tags/侣行/"},{"name":"架构","slug":"架构","link":"/tags/架构/"},{"name":"模型学习","slug":"模型学习","link":"/tags/模型学习/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"Design Pattern","slug":"Design-Pattern","link":"/tags/Design-Pattern/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"optimize","slug":"optimize","link":"/tags/optimize/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"Kafka","slug":"Kafka","link":"/tags/Kafka/"},{"name":"并发编程","slug":"并发编程","link":"/tags/并发编程/"},{"name":"Netty","slug":"Netty","link":"/tags/Netty/"},{"name":"My SQL","slug":"My-SQL","link":"/tags/My-SQL/"},{"name":"算法","slug":"算法","link":"/tags/算法/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"RocketMQ","slug":"RocketMQ","link":"/tags/RocketMQ/"},{"name":"ElasticSearch","slug":"ElasticSearch","link":"/tags/ElasticSearch/"},{"name":"Flink","slug":"Flink","link":"/tags/Flink/"},{"name":"GO","slug":"GO","link":"/tags/GO/"},{"name":"Hive","slug":"Hive","link":"/tags/Hive/"},{"name":"高可用","slug":"高可用","link":"/tags/高可用/"},{"name":"Nginx","slug":"Nginx","link":"/tags/Nginx/"},{"name":"CPU","slug":"CPU","link":"/tags/CPU/"},{"name":"架构设计","slug":"架构设计","link":"/tags/架构设计/"},{"name":"PHP","slug":"PHP","link":"/tags/PHP/"}],"categories":[{"name":"学习积累","slug":"学习积累","link":"/categories/学习积累/"},{"name":"业务实践","slug":"业务实践","link":"/categories/业务实践/"},{"name":"游世界","slug":"游世界","link":"/categories/游世界/"},{"name":"读书笔记","slug":"读书笔记","link":"/categories/读书笔记/"},{"name":"游中国","slug":"游中国","link":"/categories/游中国/"}]}